# Transformer Implementation in PyTorch

This repository provides an implementation of the classic Transformer architecture based on the seminal paper "Attention is All You Need." The codebase is developed using the PyTorch framework.

## Notebooks

### 1. Main Notebook (transformer_model.ipynb)

- Comprehensive guide covering training, inference, and evaluation of the Transformer model.
- Detailed notes on the Transformer architecture, concepts of Neural Networks, and nuances of the PyTorch framework.

**[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1W12kswh2dB3Ec3TQ-klMjOrQW0AJFIFQ?usp=sharing)**

### 2. Exploratory Dataset Analysis (transformer_data_processing.ipynb)

- Initial investigation and analysis of the dataset.
- Provides insights into data distribution, characteristics, and potential preprocessing steps.

### 3. Starter Notebook (transformer_model.ipynb)

- A simplified introduction to the Transformer model.
- Contains supplementary notes.

## Dataset

The model is trained on the [Opus Books dataset](https://huggingface.co/datasets/opus_books) from the HuggingFace repository.

## Acknowledgements

Special thanks to the detailed tutorial: ["Coding a Transformer from scratch on PyTorch, with full explanation, training, and inference"](https://www.youtube.com/watch?v=ISNdQcPhsts). This resource played an instrumental role in shaping this implementation.
