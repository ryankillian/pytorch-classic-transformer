{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4b2e0e9b13e547d1b6089a31983f024a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6bcfe19ae29e4420a94359fb1f867485",
              "IPY_MODEL_9a675ce80f1f4de0a024c5e6b5c8695c",
              "IPY_MODEL_1a01fd7cd57043bb8b2c1025df1eec74"
            ],
            "layout": "IPY_MODEL_e7d40e0c00a24c81adb2116579af79e1"
          }
        },
        "6bcfe19ae29e4420a94359fb1f867485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73d4a951b2fa4f469b5360b892072c5c",
            "placeholder": "​",
            "style": "IPY_MODEL_9d16e3dd1d0243f2ad08f7c0d2b9b0c6",
            "value": "Downloading builder script: 100%"
          }
        },
        "9a675ce80f1f4de0a024c5e6b5c8695c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf41d44b0f0e4e4d8c63404c285225c1",
            "max": 6081,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af33c9b3d8cd460f902194ee5416cc88",
            "value": 6081
          }
        },
        "1a01fd7cd57043bb8b2c1025df1eec74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dcdc09c9ea843c8b327e70fab27095f",
            "placeholder": "​",
            "style": "IPY_MODEL_54880d1b03cb4572a50c5917041cb081",
            "value": " 6.08k/6.08k [00:00&lt;00:00, 362kB/s]"
          }
        },
        "e7d40e0c00a24c81adb2116579af79e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73d4a951b2fa4f469b5360b892072c5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d16e3dd1d0243f2ad08f7c0d2b9b0c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf41d44b0f0e4e4d8c63404c285225c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af33c9b3d8cd460f902194ee5416cc88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0dcdc09c9ea843c8b327e70fab27095f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54880d1b03cb4572a50c5917041cb081": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af9e6478fb0341cfa7c9002fa49b3b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4387fef5686b4d31b092905d3ad472e9",
              "IPY_MODEL_e600a2039eca4ca8a3605dbdf116482c",
              "IPY_MODEL_bbd1400606eb469c8b39c578089be3dc"
            ],
            "layout": "IPY_MODEL_4103dd26baaf468b8774db6cff2aba4a"
          }
        },
        "4387fef5686b4d31b092905d3ad472e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d224d7eba2d84d799a4a658fe72a86bd",
            "placeholder": "​",
            "style": "IPY_MODEL_8d01f01ee6d946cb8842fc65453c2a53",
            "value": "Downloading metadata: 100%"
          }
        },
        "e600a2039eca4ca8a3605dbdf116482c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdd8e0edaacb48bfbd031e2aca9cc036",
            "max": 161154,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41d369f90da64727aa42c9c0f950d1d9",
            "value": 161154
          }
        },
        "bbd1400606eb469c8b39c578089be3dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fade2be8539a4a78b728f1673b99c890",
            "placeholder": "​",
            "style": "IPY_MODEL_ad157c0a60fb451bbdef41136617a4dd",
            "value": " 161k/161k [00:00&lt;00:00, 3.01MB/s]"
          }
        },
        "4103dd26baaf468b8774db6cff2aba4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d224d7eba2d84d799a4a658fe72a86bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d01f01ee6d946cb8842fc65453c2a53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdd8e0edaacb48bfbd031e2aca9cc036": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41d369f90da64727aa42c9c0f950d1d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fade2be8539a4a78b728f1673b99c890": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad157c0a60fb451bbdef41136617a4dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78f1fb27b7154e44ac3d1c0ae50e0f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7336d4f1c47a45b6a36c2a4f1eca018f",
              "IPY_MODEL_871238414f8b4358b4b25a2a220115b4",
              "IPY_MODEL_9f0c9ad9bc874dacb4dde4d0ab49ccc2"
            ],
            "layout": "IPY_MODEL_12ef4cffe3894db8b473c760dd6b9991"
          }
        },
        "7336d4f1c47a45b6a36c2a4f1eca018f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_811dc188a2fb49e690de1c91eb2867b2",
            "placeholder": "​",
            "style": "IPY_MODEL_ae058c4dd69a4c75a2edab383717fbc8",
            "value": "Downloading readme: 100%"
          }
        },
        "871238414f8b4358b4b25a2a220115b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cff180916a7948559094df8e5b50fa3f",
            "max": 20464,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9848a98d1bcb47d09e75c126e0d79f25",
            "value": 20464
          }
        },
        "9f0c9ad9bc874dacb4dde4d0ab49ccc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20a877293eaa414ba79c679f0be7b7e9",
            "placeholder": "​",
            "style": "IPY_MODEL_d33a63375cdf46a98306c60a6fb76077",
            "value": " 20.5k/20.5k [00:00&lt;00:00, 1.04MB/s]"
          }
        },
        "12ef4cffe3894db8b473c760dd6b9991": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "811dc188a2fb49e690de1c91eb2867b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae058c4dd69a4c75a2edab383717fbc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cff180916a7948559094df8e5b50fa3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9848a98d1bcb47d09e75c126e0d79f25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20a877293eaa414ba79c679f0be7b7e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d33a63375cdf46a98306c60a6fb76077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2373b16f89df43729c03ded27530a39f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8118afa888b64d539c6ef2f6ef67bfce",
              "IPY_MODEL_456566e4341246818d8ffc17bb5b1530",
              "IPY_MODEL_6a3d64bbd3df42d6b56eebb98bad380d"
            ],
            "layout": "IPY_MODEL_7c0e29c78f6f40f09a273cfa20fa6139"
          }
        },
        "8118afa888b64d539c6ef2f6ef67bfce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e407d445cae64d26a0aec823fb3066ed",
            "placeholder": "​",
            "style": "IPY_MODEL_f933eebb5d254a51a8cbac259d760dce",
            "value": "Downloading data: 100%"
          }
        },
        "456566e4341246818d8ffc17bb5b1530": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cb5c74f55744260b0a4f7df9f2fbb0c",
            "max": 12009501,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3aca7274436148c98dafee6270595086",
            "value": 12009501
          }
        },
        "6a3d64bbd3df42d6b56eebb98bad380d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38840bf2404147cfbd9dda6cb339c976",
            "placeholder": "​",
            "style": "IPY_MODEL_7abc3dc3984f49b8841743ad44ffa8e8",
            "value": " 12.0M/12.0M [00:01&lt;00:00, 15.9MB/s]"
          }
        },
        "7c0e29c78f6f40f09a273cfa20fa6139": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e407d445cae64d26a0aec823fb3066ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f933eebb5d254a51a8cbac259d760dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0cb5c74f55744260b0a4f7df9f2fbb0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3aca7274436148c98dafee6270595086": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38840bf2404147cfbd9dda6cb339c976": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7abc3dc3984f49b8841743ad44ffa8e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a1554dfd26f4007b2582006e21a5e48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95adf9b606b246d5a8470a095c236888",
              "IPY_MODEL_85c92ffe7e134586a4e133587b4fb72c",
              "IPY_MODEL_f70a468e4cea402eaee1c9098ba86870"
            ],
            "layout": "IPY_MODEL_98186d7c514c44b79245c2ef6d5eab2e"
          }
        },
        "95adf9b606b246d5a8470a095c236888": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ec10492b72542dfbe1ff31b4f79b8ac",
            "placeholder": "​",
            "style": "IPY_MODEL_7d49fb00ae7f4ea687dd99426e7bec3b",
            "value": "Generating train split: 100%"
          }
        },
        "85c92ffe7e134586a4e133587b4fb72c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_373104fb58bd4e279af32934df0b6a6c",
            "max": 127085,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_458c813e6ea647e08827af49905ff779",
            "value": 127085
          }
        },
        "f70a468e4cea402eaee1c9098ba86870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae67600b520045d2b8110348fdeb4dbc",
            "placeholder": "​",
            "style": "IPY_MODEL_4251f94de8794795ae8cf8039de5746b",
            "value": " 127085/127085 [00:13&lt;00:00, 20136.90 examples/s]"
          }
        },
        "98186d7c514c44b79245c2ef6d5eab2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ec10492b72542dfbe1ff31b4f79b8ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d49fb00ae7f4ea687dd99426e7bec3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "373104fb58bd4e279af32934df0b6a6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "458c813e6ea647e08827af49905ff779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae67600b520045d2b8110348fdeb4dbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4251f94de8794795ae8cf8039de5746b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc5a9a7a9a4840dda0f8178b6f3e922d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c778529a8f5942c4a017a03e96d2f043",
              "IPY_MODEL_a6a719441673488c8ff1893db8c51c0c",
              "IPY_MODEL_e768eb65f9434d97a0212908afa2745a"
            ],
            "layout": "IPY_MODEL_e275122cc01343f6b92cd9d8d4eb6231"
          }
        },
        "c778529a8f5942c4a017a03e96d2f043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d5e3096bcbf46e1ac5dae94e924e8c4",
            "placeholder": "​",
            "style": "IPY_MODEL_904691aaff4841bbbb2be97be4a552c3",
            "value": "100%"
          }
        },
        "a6a719441673488c8ff1893db8c51c0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4df9e5294fc942839b5dfe17a292b470",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b1a88e600d043de8f3c0af49c594c07",
            "value": 10
          }
        },
        "e768eb65f9434d97a0212908afa2745a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e24d789f52d4fd89f96d0ead6d48029",
            "placeholder": "​",
            "style": "IPY_MODEL_5ae1cba063544721a07672d8d649eb46",
            "value": " 10/10 [00:05&lt;00:00,  1.99it/s]"
          }
        },
        "e275122cc01343f6b92cd9d8d4eb6231": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d5e3096bcbf46e1ac5dae94e924e8c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "904691aaff4841bbbb2be97be4a552c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4df9e5294fc942839b5dfe17a292b470": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b1a88e600d043de8f3c0af49c594c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e24d789f52d4fd89f96d0ead6d48029": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ae1cba063544721a07672d8d649eb46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Model: PyTorch Implementation\n",
        "\n",
        "---\n",
        "\n",
        "This Colab notebook presents a detailed implementation of the Transformer model using PyTorch.\n",
        "\n",
        "## Key Features:\n",
        "\n",
        "1. **Dataset**: The model is trained on 100,000 sentences from the opus books dataset (English to French translations).\n",
        "   \n",
        "2. **Implementation Details**: The Transformer is built from scratch, delving deep into multi-head attention, positional encodings, and normalization techniques.\n",
        "\n",
        "3. **Neural Network Insights**: This notebook elaborates on various neural network concepts, optimization techniques, and evaluation metrics including BLEU, METEOR, and ROUGE.\n",
        "\n",
        "4. **Evaluation**: We rigorously benchmark the model's performance using standard NLP metrics.\n",
        "\n",
        "## Note:\n",
        "While the notebook contains experimentations that might be non-critical to the main flow, it is primarily designed for personal use, offering a comprehensive understanding of the Transformer's workings. It's shared publicly in the hope that other enthusiasts and researchers might find it beneficial.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "NiPjbnHdaNDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "VlsM-jJ-BWjt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "miEgmDjEsDVl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Embedding layer used in the Transformer model.\n",
        "    Scales the embeddings by the square root of the embedding dimension.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_size: int, vocab_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.scale = math.sqrt(embed_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass for the embedding layer.\n",
        "\n",
        "        Args:\n",
        "        - x (torch.Tensor): Input tensor of shape (batch, seq_len).\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Embedded and scaled tensor of shape (batch, seq_len, d_model).\n",
        "        \"\"\"\n",
        "        return self.embedding(x) * self.scale\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the sinusoidal positional encoding for non-recurrent neural networks.\n",
        "\n",
        "    The positional encoding is necessary to give the model some information about\n",
        "    the relative position of the words in the sentence. This class generates a\n",
        "    matrix of sinusoids where each row corresponds to a position in the sentence\n",
        "    and can be added to word embeddings.\n",
        "\n",
        "    Args:\n",
        "        embed_size (int): Size of the word embeddings.\n",
        "        max_len (int): Maximum length of sequences that will be passed through the model.\n",
        "        dropout (float): Dropout rate for the dropout layer applied to the sum of embeddings and positional encodings.\n",
        "        base (float, optional): Base of the logarithm used in the positional encoding. Default is 10000.\n",
        "\n",
        "    Attributes:\n",
        "        dropout (nn.Module): Dropout layer.\n",
        "        pe (torch.Tensor): Positional encoding matrix.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_size: int, max_len: int, dropout: float, base: float = 10000.0) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        assert embed_size % 2 == 0, \"Embedding dimension must be even to use this positional encoding method.\"\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, embed_size)\n",
        "\n",
        "        # Create a position vector\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # Create a term for the division inside the sine and cosine functions\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(base) / embed_size))\n",
        "\n",
        "        # Apply the positional encoding formula\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # sine applied to even indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # cosine applied to odd indices\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # Add a batch dimension for broadcasting\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Adds positional encodings to token embeddings.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Token embeddings. Shape: (batch_size, sequence_length, embed_size)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Token embeddings with added positional encodings, with dropout applied. Shape: (batch_size, sequence_length, embed_size)\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        # x = x + (self.pe[:, :x.shape[1], :])\n",
        "        return self.dropout(x)\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements Layer Normalization, a type of normalization technique.\n",
        "    The normalization is performed over the last dimension of the input tensor.\n",
        "\n",
        "    Attributes:\n",
        "        eps (float): A small value to ensure numerical stability (prevent division by zero).\n",
        "        gamma (nn.Parameter): A learnable scale factor. Initialized with ones.\n",
        "        beta (nn.Parameter): A learnable shift (bias). Initialized with zeros.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_size: int, eps: float = 1e-6) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embed_size (int): The size of the last dimension of the input tensor.\n",
        "            eps (float, optional): A small value for numerical stability. Defaults to 1e-6.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.gamma = nn.Parameter(torch.ones(embed_size))  # gamma is a learnable parameter\n",
        "        self.beta = nn.Parameter(torch.zeros(embed_size))  # beta is a learnable parameter\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply layer normalization over the last dimension of the input tensor.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch, seq_len, embed_size).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Normalized tensor of the same shape as the input.\n",
        "        \"\"\"\n",
        "\n",
        "        # Calculate mean and standard deviation\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "\n",
        "\n",
        "        # print(\"LayerNorm mean:\", mean)\n",
        "        # print(\"LayerNorm std:\", std)\n",
        "        # print(\"LayerNorm output:\", self.gamma * (x - mean) / (std + self.eps) + self.beta)\n",
        "\n",
        "\n",
        "        # Apply layer normalization\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self, embed_size: int, d_ff: int, dropout: float, activation_fn: nn.Module = nn.ReLU()) -> None:\n",
        "        \"\"\"\n",
        "        Initializes the FeedForwardBlock.\n",
        "\n",
        "        Args:\n",
        "        - embed_size (int): Dimensionality of the input.\n",
        "        - d_ff (int): Dimensionality of the hidden layer.\n",
        "        - dropout (float): Dropout rate.\n",
        "        - activation_fn (nn.Module): Activation function to use. Default is ReLU.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_layer = nn.Linear(embed_size, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.output_layer = nn.Linear(d_ff, embed_size)\n",
        "        self.activation_fn = activation_fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for the FeedForwardBlock.\n",
        "\n",
        "        Args:\n",
        "        - x (torch.Tensor): Input tensor of shape (batch, seq_len, embed_size)\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: Output tensor of shape (batch, seq_len, embed_size)\n",
        "        \"\"\"\n",
        "        # return self.output_layer(self.dropout(self.activation_fn(self.input_layer(x))))\n",
        "        # Print input and output values\n",
        "        # print(\"FeedForward input:\", x)\n",
        "        output = self.output_layer(self.dropout(self.activation_fn(self.input_layer(x))))\n",
        "        # print(\"FeedForward output:\", output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a Multi-Head Attention mechanism, as described in the Transformer architecture.\n",
        "\n",
        "    The multi-head attention mechanism allows the model to focus on different parts\n",
        "    of the input for different tasks simultaneously, which can lead to more\n",
        "    expressive representations.\n",
        "\n",
        "    Attributes:\n",
        "    - embed_size: The size of the embeddings (or the model dimension).\n",
        "    - h: The number of attention heads.\n",
        "    - head_dim: The dimension of each attention head.\n",
        "    - w_q, w_k, w_v, w_o: Linear layers for query, key, value, and output respectively.\n",
        "    - dropout: Dropout layer used after the attention mechanism.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_size: int, h: int, dropout: float) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the MultiHeadAttentionBlock.\n",
        "\n",
        "        Parameters:\n",
        "        - embed_size: The size of the embeddings (or the model dimension).\n",
        "        - h: Number of attention heads.\n",
        "        - dropout: Dropout rate for the attention weights.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.h = h  # Number of heads\n",
        "\n",
        "        # Ensure that the embedding size is divisible by the number of heads\n",
        "        assert embed_size % h == 0, \"embed_size must be divisible by the number of heads\"\n",
        "\n",
        "        self.head_dim = embed_size // h  # Dimension of each head\n",
        "\n",
        "        # Define linear layers for query, key, value, and output projection\n",
        "        self.w_q = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.w_k = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.w_v = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.w_o = nn.Linear(embed_size, embed_size, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        \"\"\"\n",
        "        Compute the attention weights and apply them to the values.\n",
        "\n",
        "        Parameters:\n",
        "        - query: The query tensor.\n",
        "        - key: The key tensor.\n",
        "        - value: The value tensor.\n",
        "        - mask: A mask tensor to mask out certain values (e.g., padding).\n",
        "        - dropout: Dropout layer to apply to the attention weights.\n",
        "\n",
        "        Returns:\n",
        "        - Tensor after applying attention mechanism.\n",
        "        \"\"\"\n",
        "        d_k = query.shape[-1]\n",
        "\n",
        "        # Compute attention scores\n",
        "        # scores size: (batch_size, num_heads, seq_len, seq_len)\n",
        "        scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "        # print(\"Attention scores:\", scores)\n",
        "\n",
        "        batch_size, num_heads, seq_len, _ = scores.shape\n",
        "\n",
        "\n",
        "        # print(\"scores shape\", scores.shape)\n",
        "        # print(\"Initial mask shape:\", mask.shape)\n",
        "        if mask is not None:\n",
        "            # Add heads dimension if it doesn't exist\n",
        "            if len(mask.shape) == 3:\n",
        "                mask = mask.unsqueeze(1)\n",
        "\n",
        "            # Expand mask to match scores shape\n",
        "            mask = mask.expand_as(scores)\n",
        "\n",
        "            scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "\n",
        "        # print(\"scores.masked_fill_: \", scores)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Normalize scores to attention weights\n",
        "        # attention_weights size: (batch_size, num_heads, seq_len, seq_len)\n",
        "        attention_weights = scores.softmax(dim=-1)\n",
        "\n",
        "        # print(\"attention_weights:\", attention_weights)\n",
        "\n",
        "        if dropout is not None:\n",
        "            attention_weights = dropout(attention_weights)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        # output size: (batch_size, num_heads, seq_len, d_k)\n",
        "        return attention_weights @ value\n",
        "\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        \"\"\"\n",
        "        Forward pass for the MultiHeadAttentionBlock.\n",
        "\n",
        "        Parameters:\n",
        "        - q: The query tensor.\n",
        "        - k: The key tensor.\n",
        "        - v: The value tensor.\n",
        "        - mask: A mask tensor to mask out certain values.\n",
        "\n",
        "        Returns:\n",
        "        - Output tensor after multi-head attention mechanism.\n",
        "        \"\"\"\n",
        "        batch_size = q.shape[0]\n",
        "\n",
        "        q = self.w_q(q)\n",
        "        k = self.w_k(k)\n",
        "        v = self.w_v(v)\n",
        "\n",
        "        # Split the last dimension into (self.h, self.head_dim)\n",
        "        # Then transpose it to shape (batch_size, self.h, sequence_length, self.head_dim)\n",
        "        q = q.view(batch_size, -1, self.h, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(batch_size, -1, self.h, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(batch_size, -1, self.h, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Calculate attention\n",
        "        x = self.attention(q, k, v, mask, self.dropout)\n",
        "\n",
        "        # Concatenate heads and pass through the final linear layer\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_size)\n",
        "        return self.w_o(x)\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a residual connection followed by a layer normalization.\n",
        "\n",
        "    This module is used in Transformer blocks to ensure the gradients can flow through the network.\n",
        "    The input is added to the result of the sub-layer function (which is first normalized and dropout is applied).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_size: int, dropout: float) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the ResidualConnection module.\n",
        "\n",
        "        Parameters:\n",
        "        - embed_size: The size of the embeddings (or the model dimension).\n",
        "        - dropout (float): The dropout rate.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = LayerNormalization(embed_size=embed_size)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"\"\"\n",
        "        Forward pass for the residual connection.\n",
        "\n",
        "        Parameters:\n",
        "        - x (torch.Tensor): The input tensor.\n",
        "        - sublayer (callable): The sub-layer function (e.g., self-attention, feed-forward neural network).\n",
        "\n",
        "        Returns:\n",
        "        - torch.Tensor: The output tensor after applying the residual connection.\n",
        "        \"\"\"\n",
        "        # Normalize the input and apply the sub-layer transformation\n",
        "        sublayer_output = sublayer(self.norm(x))\n",
        "        # Add the input to the sub-layer output (Residual Connection)\n",
        "        return x + self.dropout(sublayer_output)\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Defines a single block of the encoder containing a multi-head self-attention mechanism and a feed-forward neural network.\n",
        "    Each of these sub-layers has a residual connection around it followed by layer normalization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_size: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embed_size: The size of the embeddings (or the model dimension).\n",
        "            self_attention_block (MultiHeadAttentionBlock): The multi-head self-attention mechanism.\n",
        "            feed_forward_block (FeedForwardBlock): Point-wise feed-forward network.\n",
        "            dropout (float): Dropout rate for residual connections.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(embed_size, dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass for the encoder block.\n",
        "\n",
        "        Args:\n",
        "            src (torch.Tensor): Source input tensor.\n",
        "            src_mask (torch.Tensor): Source mask tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after passing through the encoder block.\n",
        "        \"\"\"\n",
        "        src = self.residual_connections[0](src, lambda src: self.self_attention_block(src, src, src, src_mask))\n",
        "        src = self.residual_connections[1](src, self.feed_forward_block)\n",
        "        return src\n",
        "\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Encoder consists of a stack of several encoder blocks.\n",
        "    After passing through all the encoder blocks, the output is normalized.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_size: int, layers: nn.ModuleList) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embed_size (int): The size of the embedding vectors.\n",
        "            layers (nn.ModuleList): A list of encoder blocks.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(embed_size)\n",
        "\n",
        "    def forward(self, src: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass for the encoder.\n",
        "\n",
        "        Args:\n",
        "            src (torch.Tensor): Source input tensor.\n",
        "            mask (torch.Tensor): Source mask tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after passing through all encoder blocks and normalization.\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, mask)\n",
        "        return self.norm(src)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Defines a single block of the decoder containing two multi-head attention mechanisms\n",
        "    (self-attention and cross-attention) followed by a feed-forward neural network.\n",
        "    Each of these sub-layers has a residual connection around it followed by layer normalization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_size: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embed_size: The size of the embeddings (or the model dimension).\n",
        "            self_attention_block (MultiHeadAttentionBlock): The self-attention mechanism.\n",
        "            cross_attention_block (MultiHeadAttentionBlock): The cross-attention mechanism.\n",
        "            feed_forward_block (FeedForwardBlock): Point-wise feed-forward network.\n",
        "            dropout (float): Dropout rate for residual connections.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(embed_size, dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass for the decoder block.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Decoder input tensor.\n",
        "            encoder_output (torch.Tensor): Encoder's output tensor.\n",
        "            src_mask (torch.Tensor): Source mask tensor.\n",
        "            tgt_mask (torch.Tensor): Target mask tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after passing through the decoder block.\n",
        "        \"\"\"\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Decoder consists of a stack of several decoder blocks.\n",
        "    After passing through all the decoder blocks, the output is normalized.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_size: int, layers: nn.ModuleList) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embed_size: The size of the embeddings (or the model dimension).\n",
        "            layers (nn.ModuleList): A list of decoder blocks.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(embed_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass for the decoder.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Decoder input tensor.\n",
        "            encoder_output (torch.Tensor): Encoder's output tensor.\n",
        "            src_mask (torch.Tensor): Source mask tensor.\n",
        "            tgt_mask (torch.Tensor): Target mask tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after passing through all decoder blocks and normalization.\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class OutputLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the final linear layer that projects the transformer's output\n",
        "    to a probability distribution over the vocabulary.\n",
        "\n",
        "    Attributes:\n",
        "        proj (nn.Linear): Linear layer that projects from d_model to vocab_size.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_size: int, vocab_size: int) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embed_size (int): The size of the embeddings (or the model dimension).\n",
        "            vocab_size (int): Size of the vocabulary.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass for the output layer.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch, seq_len, d_model).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch, seq_len, vocab_size)\n",
        "                          with log probabilities over the vocabulary.\n",
        "        \"\"\"\n",
        "        # (batch, seq_len, embed_size) --> (batch, seq_len, vocab_size)\n",
        "        return torch.log_softmax(self.proj(x), dim=-1)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the Transformer architecture combining encoder, decoder, embeddings, and a final projection layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings,\n",
        "                 src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, output_layer: OutputLayer) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            encoder (Encoder): The encoder module.\n",
        "            decoder (Decoder): The decoder module.\n",
        "            src_embed (InputEmbeddings): Source embeddings layer.\n",
        "            tgt_embed (InputEmbeddings): Target embeddings layer.\n",
        "            src_pos (PositionalEncoding): Source positional encoding layer.\n",
        "            tgt_pos (PositionalEncoding): Target positional encoding layer.\n",
        "            output_layer (OutputLayer): Final output layer mapping to vocabulary size.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.output_layer = output_layer\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        \"\"\"\n",
        "        Applies embeddings and positional encoding to the source, then passes it through the encoder.\n",
        "\n",
        "        Args:\n",
        "            src (torch.Tensor): Source sequence tensor.\n",
        "            src_mask (torch.Tensor): Source sequence mask tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Encoder output tensor.\n",
        "        \"\"\"\n",
        "        embedded_src = self.src_embed(src)  # Get the embeddings first\n",
        "        src = embedded_src + self.src_pos(embedded_src)  # Then, add positional encodings to the embeddings\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Applies embeddings and positional encoding to the target, then passes it and the encoder output through the decoder.\n",
        "\n",
        "        Args:\n",
        "            encoder_output (torch.Tensor): Output tensor from the encoder.\n",
        "            src_mask (torch.Tensor): Source sequence mask tensor.\n",
        "            tgt (torch.Tensor): Target sequence tensor.\n",
        "            tgt_mask (torch.Tensor): Target sequence mask tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Decoder output tensor.\n",
        "        \"\"\"\n",
        "        embedded_tgt = self.tgt_embed(tgt)  # Get the embeddings first\n",
        "        tgt = embedded_tgt + self.tgt_pos(embedded_tgt)  # Then, add positional encodings to the embeddings\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "\n",
        "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor):\n",
        "        \"\"\"\n",
        "        End-to-end forward pass for the Transformer.\n",
        "\n",
        "        Args:\n",
        "            src (torch.Tensor): Source sequence tensor.\n",
        "            tgt (torch.Tensor): Target sequence tensor.\n",
        "            src_mask (torch.Tensor): Source sequence mask tensor.\n",
        "            tgt_mask (torch.Tensor): Target sequence mask tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Projected output tensor.\n",
        "        \"\"\"\n",
        "        encoder_output = self.encode(src, src_mask)\n",
        "        decoder_output = self.decode(encoder_output, src_mask, tgt, tgt_mask)\n",
        "        return self.output_layer(decoder_output)\n",
        "\n",
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, max_seq_len: int, embed_size: int=512, N: int=1, h: int=4, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
        "    \"\"\"\n",
        "    Constructs and initializes a Transformer model.\n",
        "\n",
        "    Args:\n",
        "        src_vocab_size (int): Vocabulary size of the source language.\n",
        "        tgt_vocab_size (int): Vocabulary size of the target language.\n",
        "        max_seq_len (int): Maximum sequence length for the source language.\n",
        "        embed_size (int): Dimension of embeddings and model. Defaults to 512.\n",
        "        N (int): Number of blocks in both the encoder and decoder. Defaults to 1.\n",
        "        h (int): Number of attention heads. Defaults to 4.\n",
        "        dropout (float): Dropout rate for residual connections. Defaults to 0.1.\n",
        "        d_ff (int): Dimension of the feed-forward network inside blocks. Defaults to 2048.\n",
        "\n",
        "    Returns:\n",
        "        Transformer: An initialized Transformer model.\n",
        "    \"\"\"\n",
        "    # Create the embedding layers\n",
        "    src_embed = InputEmbeddings(embed_size, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(embed_size, tgt_vocab_size)\n",
        "\n",
        "    # Create the positional encoding layers\n",
        "    src_pos = PositionalEncoding(embed_size, max_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(embed_size, max_seq_len, dropout)\n",
        "\n",
        "    # Create the encoder blocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(embed_size, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(embed_size, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(embed_size, encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # Create the decoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(embed_size, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(embed_size, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(embed_size, d_ff, dropout)\n",
        "        decoder_block = DecoderBlock(embed_size, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    # Create the encoder and decoder\n",
        "    encoder = Encoder(embed_size, nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(embed_size, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # Create the projection layer\n",
        "    output_layer = OutputLayer(embed_size, tgt_vocab_size)\n",
        "\n",
        "    # Create the transformer\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, output_layer)\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reproducibility"
      ],
      "metadata": {
        "id": "1qv-6vDik9ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "MkBHdcxalA4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing (old flow - ignore)"
      ],
      "metadata": {
        "id": "i6UfS06JGnv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIM7iPQcGsn_",
        "outputId": "4cdc3240-3686-48b6-e500-00f1dc7e20b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: tokenizers, xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "Successfully installed datasets-2.14.5 dill-0.3.7 huggingface-hub-0.16.4 multiprocess-0.70.15 tokenizers-0.13.3 xxhash-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"opus_books\", \"en-fr\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "4b2e0e9b13e547d1b6089a31983f024a",
            "6bcfe19ae29e4420a94359fb1f867485",
            "9a675ce80f1f4de0a024c5e6b5c8695c",
            "1a01fd7cd57043bb8b2c1025df1eec74",
            "e7d40e0c00a24c81adb2116579af79e1",
            "73d4a951b2fa4f469b5360b892072c5c",
            "9d16e3dd1d0243f2ad08f7c0d2b9b0c6",
            "bf41d44b0f0e4e4d8c63404c285225c1",
            "af33c9b3d8cd460f902194ee5416cc88",
            "0dcdc09c9ea843c8b327e70fab27095f",
            "54880d1b03cb4572a50c5917041cb081",
            "af9e6478fb0341cfa7c9002fa49b3b53",
            "4387fef5686b4d31b092905d3ad472e9",
            "e600a2039eca4ca8a3605dbdf116482c",
            "bbd1400606eb469c8b39c578089be3dc",
            "4103dd26baaf468b8774db6cff2aba4a",
            "d224d7eba2d84d799a4a658fe72a86bd",
            "8d01f01ee6d946cb8842fc65453c2a53",
            "bdd8e0edaacb48bfbd031e2aca9cc036",
            "41d369f90da64727aa42c9c0f950d1d9",
            "fade2be8539a4a78b728f1673b99c890",
            "ad157c0a60fb451bbdef41136617a4dd",
            "78f1fb27b7154e44ac3d1c0ae50e0f7a",
            "7336d4f1c47a45b6a36c2a4f1eca018f",
            "871238414f8b4358b4b25a2a220115b4",
            "9f0c9ad9bc874dacb4dde4d0ab49ccc2",
            "12ef4cffe3894db8b473c760dd6b9991",
            "811dc188a2fb49e690de1c91eb2867b2",
            "ae058c4dd69a4c75a2edab383717fbc8",
            "cff180916a7948559094df8e5b50fa3f",
            "9848a98d1bcb47d09e75c126e0d79f25",
            "20a877293eaa414ba79c679f0be7b7e9",
            "d33a63375cdf46a98306c60a6fb76077",
            "2373b16f89df43729c03ded27530a39f",
            "8118afa888b64d539c6ef2f6ef67bfce",
            "456566e4341246818d8ffc17bb5b1530",
            "6a3d64bbd3df42d6b56eebb98bad380d",
            "7c0e29c78f6f40f09a273cfa20fa6139",
            "e407d445cae64d26a0aec823fb3066ed",
            "f933eebb5d254a51a8cbac259d760dce",
            "0cb5c74f55744260b0a4f7df9f2fbb0c",
            "3aca7274436148c98dafee6270595086",
            "38840bf2404147cfbd9dda6cb339c976",
            "7abc3dc3984f49b8841743ad44ffa8e8",
            "8a1554dfd26f4007b2582006e21a5e48",
            "95adf9b606b246d5a8470a095c236888",
            "85c92ffe7e134586a4e133587b4fb72c",
            "f70a468e4cea402eaee1c9098ba86870",
            "98186d7c514c44b79245c2ef6d5eab2e",
            "6ec10492b72542dfbe1ff31b4f79b8ac",
            "7d49fb00ae7f4ea687dd99426e7bec3b",
            "373104fb58bd4e279af32934df0b6a6c",
            "458c813e6ea647e08827af49905ff779",
            "ae67600b520045d2b8110348fdeb4dbc",
            "4251f94de8794795ae8cf8039de5746b"
          ]
        },
        "id": "dh5M6ApyG1Bs",
        "outputId": "0f75b6ad-ed68-47d9-fd5b-70b227595eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.08k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b2e0e9b13e547d1b6089a31983f024a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading metadata:   0%|          | 0.00/161k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af9e6478fb0341cfa7c9002fa49b3b53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/20.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78f1fb27b7154e44ac3d1c0ae50e0f7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/12.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2373b16f89df43729c03ded27530a39f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/127085 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a1554dfd26f4007b2582006e21a5e48"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset['train']\n",
        "\n",
        "print(train_dataset.shape)\n",
        "print(train_dataset[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jRc8oXMHGCA",
        "outputId": "795af143-d8ea-4029-c756-4742128a72d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(127085, 2)\n",
            "{'id': ['0', '1', '2', '3', '4', '5'], 'translation': [{'en': 'The Wanderer', 'fr': 'Le grand Meaulnes'}, {'en': 'Alain-Fournier', 'fr': 'Alain-Fournier'}, {'en': 'First Part', 'fr': 'PREMIÈRE PARTIE'}, {'en': 'I', 'fr': 'CHAPITRE PREMIER'}, {'en': 'THE BOARDER', 'fr': 'LE PENSIONNAIRE'}, {'en': 'He arrived at our home on a Sunday of November, 189-.', 'fr': 'Il arriva chez nous un dimanche de novembre 189-…'}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def max_len(lang):\n",
        "    max_length_item = max(train_dataset['translation'], key=lambda x: len(x[lang].split()))\n",
        "    print(f'Max length {lang}: ', len(max_length_item[lang].split()))\n",
        "    print(max_length_item[lang])\n",
        "\n",
        "max_len('en')\n",
        "max_len('fr')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNU8vKzFHXQ-",
        "outputId": "466d63bc-c46d-4d9f-9bf1-b4c136d99ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length en:  372\n",
            "Upon the whole, I was by this time so fixed upon my design of going over with him to the continent that I told him we would go and make one as big as that, and he should go home in it. He answered not one word, but looked very grave and sad. I asked him what was the matter with him. He asked me again, “Why you angry mad with Friday?—what me done?” I asked him what he meant. I told him I was not angry with him at all. “No angry!” says he, repeating the words several times; “why send Friday home away to my nation?” “Why,” says I, “Friday, did not you say you wished you were there?” “Yes, yes,” says he, “wish we both there; no wish Friday there, no master there.” In a word, he would not think of going there without me. “I go there, Friday?” says I; “what shall I do there?” He turned very quick upon me at this. “You do great deal much good,” says he; “you teach wild mans be good, sober, tame mans; you tell them know God, pray God, and live new life.” “Alas, Friday!” says I, “thou knowest not what thou sayest; I am but an ignorant man myself.” “Yes, yes,” says he, “you teachee me good, you teachee them good.” “No, no, Friday,” says I, “you shall go without me; leave me here to live by myself, as I did before.” He looked confused again at that word; and running to one of the hatchets which he used to wear, he takes it up hastily, and gives it to me. “What must I do with this?” says I to him. “You take kill Friday,” says he. “What must kill you for?” said I again. He returns very quick—“What you send Friday away for? Take kill Friday, no send Friday away.” This he spoke so earnestly that I saw tears stand in his eyes. In a word, I so plainly discovered the utmost affection in him to me, and a firm resolution in him, that I told him then and often after, that I would never send him away from me if he was willing to stay with me.\n",
            "Max length fr:  324\n",
            "En somme, je fus alors si affermi dans ma résolution de gagner avec lui le continent, que je lui dis qu'il fallait nous mettre à en faire une de cette grandeur-là pour qu'il pût s'en retourner chez lui. Il ne répliqua pas un mot, mais il devint sérieux et triste. Je lui demandai ce qu'il avait. Il me répondit ainsi:--«Pourquoi vous colère avec Vendredi? Quoi moi fait?»--Je le priai de s'expliquer et lui protestai que je n'étais point du tout en colère.--«Pas colère! pas colère! reprit-il en répétant ces mots plusieurs fois; pourquoi envoyer Vendredi loin chez ma nation?»--«Pourquoi!... Mais ne m'as-tu pas dit que tu souhaitais y retourner?»--«Oui, oui, s'écria-t-il, souhaiter être touts deux là: Vendredi là et pas maître là.»--En un mot il ne pouvait se faire à l'idée de partir sans moi.--«Moi aller avec toi, Vendredi! m'écriai-je; mais que ferais-je là?»--Il me répliqua très-vivement là-dessus:--«Vous faire grande quantité beaucoup bien, vous apprendre Sauvages hommes être hommes bons, hommes sages, hommes apprivoisés; vous leur enseigner connaître Dieu, prier Dieu et vivre nouvelle vie.»--«Hélas! Vendredi, répondis-je, tu ne sais ce que tu dis, je ne suis moi-même qu'un ignorant.»--«Oui, oui, reprit-il, vous enseigna moi bien, vous enseigner eux bien.»--«Non, non, Vendredi, te dis-je, tu partiras sans moi; laisse-moi vivre ici tout seul comme autrefois.»--À ces paroles il retomba dans le trouble, et, courant à une des hachettes qu'il avait coutume de porter, il s'en saisit à la hâte et me la donna.--«Que faut-il que j'en fasse, lui dis-je?»--«Vous prendre, vous tuer Vendredi.»--«Moi te tuer! Et pourquoi?»--«Pourquoi, répliqua-t-il prestement, vous envoyer Vendredi loin?... Prendre, tuer Vendredi, pas renvoyer Vendredi loin.»--Il prononça ces paroles avec tant de componction, que je vis ses yeux se mouiller de larmes. En un mot, je découvris clairement en lui une si profonde affection pour moi et une si ferme résolution, que je lui dis alors, et souvent depuis, que je ne l'éloignerais jamais tant qu'il voudrait rester avec moi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique Tokens (basic count for now)\n",
        "english_tokens = set()\n",
        "french_tokens = set()\n",
        "\n",
        "for example in train_dataset:\n",
        "    english_tokens.update(example['translation']['en'].split())\n",
        "    french_tokens.update(example['translation']['fr'].split())\n",
        "\n",
        "print(f\"Unique English tokens: {len(english_tokens)}\")\n",
        "print(f\"Unique French tokens: {len(french_tokens)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DROVb6O1TrX5",
        "outputId": "19bada70-11fe-4c55-b3e4-56ef927227af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique English tokens: 146031\n",
            "Unique French tokens: 169205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_dataset = train_dataset[:200]"
      ],
      "metadata": {
        "id": "ge7mpRlUUKWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_dataset['translation'][99]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWrrWVldURNs",
        "outputId": "c6c968bf-bafb-493b-868b-599c1c1a833a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'en': 'There were still a few games, some galloping races in the playground, then night came; the two pupils who had swept the classroom fetched their hoods and cloaks, and with their baskets under their arms went away quickly, leaving the big gate open ...',\n",
              " 'fr': 'Il y avait encore quelques jeux, des galopades dans la cour ; puis la nuit venait ; les deux élèves qui avaient balayé la classe cherchaient sous le hangar leurs capuchons et leurs pèlerines, et ils partaient bien vite, leur panier au bras, en laissant le grand portail ouvert…'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset= dummy_dataset"
      ],
      "metadata": {
        "id": "AR371nKRUsjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "def get_all_sentences(ds, lang):\n",
        "    for item in ds['translation']:\n",
        "        yield item[lang]\n",
        "\n",
        "tokenizer_en = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "tokenizer_en.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer_en = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"])\n",
        "tokenizer_en.train_from_iterator(get_all_sentences(dataset, 'en'), trainer=trainer_en)\n",
        "\n",
        "tokenizer_fr = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "tokenizer_fr.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer_fr = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"])\n",
        "tokenizer_fr.train_from_iterator(get_all_sentences(dataset, 'fr'), trainer=trainer_fr)"
      ],
      "metadata": {
        "id": "uoXyTpZaT1oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(dataset, tokenizer_en, tokenizer_fr):\n",
        "    tokenized_data_en = []\n",
        "    tokenized_data_fr = []\n",
        "    for example in dataset['translation']:\n",
        "        tokenized_data_en.append(tokenizer_en.encode(example['en']).ids)\n",
        "        tokenized_data_fr.append(tokenizer_fr.encode(example['fr']).ids)\n",
        "    return tokenized_data_en, tokenized_data_fr\n",
        "\n",
        "\n",
        "tokenized_data_en,  tokenized_data_fr = tokenize_data(dataset, tokenizer_en, tokenizer_fr)"
      ],
      "metadata": {
        "id": "yjsirnvfUnb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_data_en[0])\n",
        "print(tokenized_data_en[9])\n",
        "print(tokenized_data_fr[9])\n",
        "print(len(tokenized_data_en[9]))\n",
        "print(len(tokenized_data_fr[9]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f85Z8Ni8U5HD",
        "outputId": "f3b17b23-e196-4c38-8cec-87440cd28bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[45, 617]\n",
            "[189, 855, 4, 183, 12, 141, 9, 194, 99, 6, 100, 22, 122, 65, 282, 4, 16, 91, 7, 5, 591, 238, 8, 152, 7, 5, 187, 233, 730, 81, 282, 1349, 33, 5, 1099, 1270, 11, 36, 394, 6]\n",
            "[307, 83, 4, 21, 43, 5, 619, 77, 7, 96, 4, 45, 12, 97, 162, 4, 63, 809, 11, 8, 182, 9, 116, 168, 4, 53, 14, 5, 59, 1168, 9, 691, 16, 5, 978, 4, 10, 9, 116, 550, 7]\n",
            "40\n",
            "41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ID for [UNK]:\", tokenizer_en.token_to_id(\"[UNK]\"))\n",
        "print(\"ID for [PAD]:\", tokenizer_en.token_to_id(\"[PAD]\"))\n",
        "print(\"ID for [SOS]:\", tokenizer_en.token_to_id(\"[SOS]\"))\n",
        "print(\"ID for [EOS]:\", tokenizer_en.token_to_id(\"[EOS]\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXI3ZXmiU9sp",
        "outputId": "880ea112-f742-4bd1-dfe5-87198f970cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID for [UNK]: 0\n",
            "ID for [PAD]: 1\n",
            "ID for [SOS]: 2\n",
            "ID for [EOS]: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_special_tokens(tokenized_data, tokenizer):\n",
        "    SOS_token = tokenizer.token_to_id(\"[SOS]\")\n",
        "    EOS_token = tokenizer.token_to_id(\"[EOS]\")\n",
        "\n",
        "    # Add the SOS and EOS tokens to each sequence\n",
        "    return [[SOS_token] + seq + [EOS_token] for seq in tokenized_data]\n",
        "\n",
        "tokenized_data_en_with_special_tokens = add_special_tokens(tokenized_data_en, tokenizer_en)\n",
        "tokenized_data_fr_with_special_tokens = add_special_tokens(tokenized_data_fr, tokenizer_fr)"
      ],
      "metadata": {
        "id": "3Il2eIv8VBJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_data_en_with_special_tokens[0])\n",
        "print(tokenized_data_en_with_special_tokens[9])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKprhOsmVFka",
        "outputId": "14cc5854-9de2-4001-99ae-a46449830244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 45, 617, 3]\n",
            "[2, 189, 855, 4, 183, 12, 141, 9, 194, 99, 6, 100, 22, 122, 65, 282, 4, 16, 91, 7, 5, 591, 238, 8, 152, 7, 5, 187, 233, 730, 81, 282, 1349, 33, 5, 1099, 1270, 11, 36, 394, 6, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the maximum sequence length after adding SOS and EOS tokens for English and French respectively\n",
        "MAX_LENGTH_EN = max(len(seq) for seq in tokenized_data_en_with_special_tokens)\n",
        "MAX_LENGTH_FR = max(len(seq) for seq in tokenized_data_fr_with_special_tokens)\n",
        "\n",
        "print(\"Max Length English:\", MAX_LENGTH_EN)\n",
        "print(\"Max Length French:\", MAX_LENGTH_FR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFIFhcp_VKw_",
        "outputId": "28daae43-8fce-4571-a24b-6cafe7f46307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Length English: 134\n",
            "Max Length French: 133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad(sequence, max_length, pad_token_id=1):\n",
        "    \"\"\"Function to pad a sequence to the desired max_length.\"\"\"\n",
        "    if len(sequence) < max_length:\n",
        "        sequence += [pad_token_id] * (max_length - len(sequence))\n",
        "    return sequence\n",
        "\n",
        "# Apply padding to the tokenized data\n",
        "padded_data_en = [pad(seq, MAX_LENGTH_EN) for seq in tokenized_data_en_with_special_tokens]\n",
        "padded_data_fr = [pad(seq, MAX_LENGTH_FR) for seq in tokenized_data_fr_with_special_tokens]"
      ],
      "metadata": {
        "id": "B6NNviDfVOX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(padded_data_en[0])\n",
        "print(padded_data_en[9])\n",
        "print(padded_data_fr[9])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASIY4ZmeVRvU",
        "outputId": "61ee834d-89c2-4e96-fbd8-a131e24524aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 45, 617, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[2, 189, 855, 4, 183, 12, 141, 9, 194, 99, 6, 100, 22, 122, 65, 282, 4, 16, 91, 7, 5, 591, 238, 8, 152, 7, 5, 187, 233, 730, 81, 282, 1349, 33, 5, 1099, 1270, 11, 36, 394, 6, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[2, 307, 83, 4, 21, 43, 5, 619, 77, 7, 96, 4, 45, 12, 97, 162, 4, 63, 809, 11, 8, 182, 9, 116, 168, 4, 53, 14, 5, 59, 1168, 9, 691, 16, 5, 978, 4, 10, 9, 116, 550, 7, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_attention_masks(padded_data):\n",
        "    return [[1 if token != 1 else 0 for token in sequence] for sequence in padded_data]\n",
        "\n",
        "attention_masks_en = create_attention_masks(padded_data_en)\n",
        "attention_masks_fr = create_attention_masks(padded_data_fr)"
      ],
      "metadata": {
        "id": "rCGYcpqUVWf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(attention_masks_en[0])\n",
        "print(attention_masks_en[9])\n",
        "print(attention_masks_fr[9])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbcf8ngYVZPW",
        "outputId": "b87dda04-1c6f-4bf7-cbe9-8a8c445bd606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def create_target_mask_torch(sequence):\n",
        "    # Convert sequence to tensor\n",
        "    sequence_tensor = torch.tensor(sequence)\n",
        "\n",
        "    # Create an upper triangular matrix of size len(sequence) x len(sequence)\n",
        "    look_ahead_mask = torch.triu(torch.ones((len(sequence), len(sequence))), diagonal=1)\n",
        "\n",
        "    # Create a padding mask (1 for padding tokens, 0 otherwise)\n",
        "    padding_mask = (sequence_tensor == 1).unsqueeze(-1).float()\n",
        "\n",
        "    # Combine the two masks\n",
        "    combined_mask = torch.maximum(look_ahead_mask, padding_mask)\n",
        "\n",
        "    return combined_mask\n",
        "\n",
        "target_masks_fr_torch = [create_target_mask_torch(sequence) for sequence in padded_data_fr]\n"
      ],
      "metadata": {
        "id": "KdoaDsIuVczm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(padded_data_fr[0])\n",
        "print(padded_data_fr[7])\n",
        "print(padded_data_fr[9])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiCliPAjVgAO",
        "outputId": "f1511219-1a2e-4c59-ae5f-df5a5400767e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 76, 36, 47, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[2, 119, 657, 1188, 9, 433, 176, 231, 274, 136, 10, 27, 65, 5, 63, 1243, 723, 152, 7, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[2, 307, 83, 4, 21, 43, 5, 619, 77, 7, 96, 4, 45, 12, 97, 162, 4, 63, 809, 11, 8, 182, 9, 116, 168, 4, 53, 14, 5, 59, 1168, 9, 691, 16, 5, 978, 4, 10, 9, 116, 550, 7, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 5))\n",
        "\n",
        "cax1 = ax1.matshow(target_masks_fr_torch[0], cmap='viridis')\n",
        "fig.colorbar(cax1, ax=ax1)\n",
        "ax1.set_title(\"Heatmap 1\")\n",
        "\n",
        "cax2 = ax2.matshow(target_masks_fr_torch[7], cmap='viridis')\n",
        "fig.colorbar(cax2, ax=ax2)\n",
        "ax2.set_title(\"Heatmap 2\")\n",
        "\n",
        "cax3 = ax3.matshow(target_masks_fr_torch[9], cmap='viridis')\n",
        "fig.colorbar(cax3, ax=ax3)\n",
        "ax2.set_title(\"Heatmap 3\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "_WW7920RVjF6",
        "outputId": "725e561f-0451-4e69-d529-aa2c4214ec66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loader (old flow - ignore)"
      ],
      "metadata": {
        "id": "Qk8XpahgbPxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "data_en = torch.tensor(padded_data_en, dtype=torch.long)\n",
        "data_fr = torch.tensor(padded_data_fr, dtype=torch.long)\n",
        "mask_en = torch.tensor(attention_masks_en, dtype=torch.bool)\n",
        "mask_fr = torch.tensor(attention_masks_fr, dtype=torch.bool)\n",
        "tgt_mask = torch.stack(target_masks_fr_torch)\n",
        "\n",
        "# Create a dataset and dataloader\n",
        "dataset = TensorDataset(data_en, data_fr, mask_en, mask_fr, tgt_mask)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
      ],
      "metadata": {
        "id": "Jq9HT-7GbTMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over batches from the dataloader\n",
        "for batch_idx, (src, tgt, src_mask, tgt_mask, look_ahead_mask) in enumerate(dataloader):\n",
        "    print(f\"Batch {batch_idx + 1}:\")\n",
        "    print(\"========================================\")\n",
        "    print(f\"Source shape: {src.shape}\")\n",
        "    print(f\"Target shape: {tgt.shape}\")\n",
        "    print(f\"Source mask shape: {src_mask.shape}\")\n",
        "    print(f\"Target mask shape: {tgt_mask.shape}\")\n",
        "    print(f\"Look-ahead mask shape: {look_ahead_mask.shape}\")\n",
        "    print()\n",
        "\n",
        "    # For demonstration purposes, let's just check the first two batches\n",
        "    if batch_idx == 1:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lLv3s8JqFqM",
        "outputId": "d09ab480-a37f-49f7-b3d2-368befe6251b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1:\n",
            "========================================\n",
            "Source shape: torch.Size([8, 134])\n",
            "Target shape: torch.Size([8, 133])\n",
            "Source mask shape: torch.Size([8, 134])\n",
            "Target mask shape: torch.Size([8, 133])\n",
            "Look-ahead mask shape: torch.Size([8, 133, 133])\n",
            "\n",
            "Batch 2:\n",
            "========================================\n",
            "Source shape: torch.Size([8, 134])\n",
            "Target shape: torch.Size([8, 133])\n",
            "Source mask shape: torch.Size([8, 134])\n",
            "Target mask shape: torch.Size([8, 133])\n",
            "Look-ahead mask shape: torch.Size([8, 133, 133])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "-dE0lWtTKrWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset processing"
      ],
      "metadata": {
        "id": "zFj3OVVOKsF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets tokenizers\n"
      ],
      "metadata": {
        "id": "4LMGNeSbKwR_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reproducibility\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "NrmshvWv8Cdv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"opus_books\", \"en-fr\", split='train[:100000]')"
      ],
      "metadata": {
        "id": "XQNLhunCLj-d"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "SPECIAL_TOKENS = [\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"]\n",
        "UNK_TOKEN = \"[UNK]\"\n",
        "\n",
        "def train_tokenizer(dataset, language):\n",
        "    def get_all_sentences(ds, lang):\n",
        "        return (item['translation'][lang] for item in ds)\n",
        "\n",
        "    tokenizer = Tokenizer(WordLevel(unk_token=UNK_TOKEN))\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "    trainer = WordLevelTrainer(special_tokens=SPECIAL_TOKENS)\n",
        "    tokenizer.train_from_iterator(get_all_sentences(dataset, language), trainer=trainer)\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "tokenizer_en = train_tokenizer(dataset, 'en')\n",
        "tokenizer_fr = train_tokenizer(dataset, 'fr')"
      ],
      "metadata": {
        "id": "SjzDn9IUMR6Z"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(data, tokenizer):\n",
        "    return [tokenizer.encode(text).ids for text in data]\n",
        "\n",
        "def get_max_seq_length(tokenized_data):\n",
        "    return max(len(seq) for seq in tokenized_data)\n",
        "\n",
        "src_texts = [item['translation']['en'] for item in dataset]\n",
        "tgt_texts = [item['translation']['fr'] for item in dataset]\n",
        "\n",
        "tokenized_src = tokenize_data(src_texts, tokenizer_en)\n",
        "tokenized_tgt = tokenize_data(tgt_texts, tokenizer_fr)\n",
        "\n",
        "SRC_SEQ_LEN = get_max_seq_length(tokenized_src) + 2  # +2 for [SOS] and [EOS] tokens\n",
        "TGT_SEQ_LEN = get_max_seq_length(tokenized_tgt) + 2  # +2 for [SOS] and [EOS] tokens\n",
        "MAX_SEQ_LEN = max(SRC_SEQ_LEN, TGT_SEQ_LEN)\n",
        "\n",
        "print(\"SRC_SEQ_LEN: \", SRC_SEQ_LEN)\n",
        "print(\"TGT_SEQ_LEN: \", TGT_SEQ_LEN)\n",
        "print(\"MAX_SEQ_LEN: \", MAX_SEQ_LEN)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DJZM166P2s8",
        "outputId": "5479d810-3487-4200-e3bd-7c5770408a97"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SRC_SEQ_LEN:  473\n",
            "TGT_SEQ_LEN:  484\n",
            "MAX_SEQ_LEN:  484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Determine sizes for each split\n",
        "train_dataset_size = int(0.8 * len(dataset))\n",
        "val_dataset_size = int(0.1 * len(dataset))\n",
        "test_dataset_size = len(dataset) - train_dataset_size - val_dataset_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset_raw, val_dataset_raw, test_dataset_raw = random_split(dataset, [train_dataset_size, val_dataset_size, test_dataset_size])\n"
      ],
      "metadata": {
        "id": "vAA2koBsM3p5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset_raw))\n",
        "print(len(val_dataset_raw))\n",
        "print(len(test_dataset_raw))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3NY50OlNkw_",
        "outputId": "26ae6012-ba46-4fb4-ab73-44fe10500f4a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80000\n",
            "10000\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
        "        super().__init__()\n",
        "        self.ds = ds\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "        self.seq_len = seq_len\n",
        "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def _process_text(self, text, tokenizer):\n",
        "        \"\"\"Tokenize text and handle SOS, EOS, and padding.\"\"\"\n",
        "        tokens = tokenizer.encode(text).ids\n",
        "        num_padding_tokens = self.seq_len - len(tokens) - 2  # Account for SOS and EOS tokens\n",
        "        if num_padding_tokens < 0:\n",
        "            raise ValueError(\"Sentence exceeds maximum sequence length\")\n",
        "\n",
        "        # Concatenate tokens with special tokens and padding\n",
        "        return torch.cat([\n",
        "            self.sos_token,\n",
        "            torch.tensor(tokens, dtype=torch.int64),\n",
        "            self.eos_token,\n",
        "            torch.tensor([self.pad_token] * num_padding_tokens, dtype=torch.int64)\n",
        "        ], dim=0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_target_pair = self.ds[idx]\n",
        "        src_text = src_target_pair['translation'][self.src_lang]\n",
        "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
        "\n",
        "        encoder_input = self._process_text(src_text, self.tokenizer_src)\n",
        "        decoder_input = self._process_text(tgt_text, self.tokenizer_tgt)\n",
        "\n",
        "        # Create label for the target text, without the SOS token but with the EOS token\n",
        "        label = torch.cat([\n",
        "            torch.tensor(self.tokenizer_tgt.encode(tgt_text).ids, dtype=torch.int64),\n",
        "            self.eos_token\n",
        "        ], dim=0)\n",
        "        label = torch.cat([label, torch.tensor([self.pad_token] * (self.seq_len - len(label)), dtype=torch.int64)], dim=0)\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\": encoder_input,\n",
        "            \"decoder_input\": decoder_input,\n",
        "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(1).int(),\n",
        "            \"decoder_mask\": ((decoder_input != self.pad_token).unsqueeze(1).int() & causal_mask(decoder_input.size(0))).int(),\n",
        "            \"label\": label,\n",
        "            \"src_text\": src_text,\n",
        "            \"tgt_text\": tgt_text\n",
        "        }\n",
        "\n",
        "def causal_mask(size):\n",
        "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
        "    return mask == 0\n"
      ],
      "metadata": {
        "id": "SQBFcyAF4gcp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Create instances of the dataset for training, validation, and test data\n",
        "train_dataset = TranslationDataset(train_dataset_raw, tokenizer_en, tokenizer_fr, 'en', 'fr', MAX_SEQ_LEN)\n",
        "val_dataset = TranslationDataset(val_dataset_raw, tokenizer_en, tokenizer_fr, 'en', 'fr', MAX_SEQ_LEN)\n",
        "test_dataset = TranslationDataset(test_dataset_raw, tokenizer_en, tokenizer_fr, 'en', 'fr', MAX_SEQ_LEN)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "q8KePDmN2tAq"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Drive"
      ],
      "metadata": {
        "id": "ISdEOdvY7tgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mvDhjdQ7vUe",
        "outputId": "35e96dce-62c0-4e1e-9c09-5ed5b11fdd79"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define paths\n",
        "base_path = '/content/drive/MyDrive/Colab Notebooks/transformer-model-01'\n",
        "model_save_path = os.path.join(base_path, 'model_weights')\n",
        "tokenizer_en_save_path = os.path.join(base_path, 'tokenizer_en')\n",
        "tokenizer_fr_save_path = os.path.join(base_path, 'tokenizer_fr')\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "os.makedirs(tokenizer_en_save_path, exist_ok=True)\n",
        "os.makedirs(tokenizer_fr_save_path, exist_ok=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "LeyQctXy75TI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Initialization"
      ],
      "metadata": {
        "id": "czwyFWY1sfkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Hyperparameters\n",
        "\n",
        "SRC_VOCAB_SIZE = tokenizer_en.get_vocab_size()\n",
        "TGT_VOCAB_SIZE = tokenizer_fr.get_vocab_size()\n",
        "EMBED_SIZE = 128\n",
        "N = 1 # Number of encoder and decoder blocks\n",
        "H = 4 # Number of attention heads\n",
        "DROPOUT = 0.1\n",
        "D_FF = 2048\n",
        "\n",
        "# Initialize the transformer model\n",
        "model = build_transformer(SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, MAX_SEQ_LEN, EMBED_SIZE, N, H, DROPOUT, D_FF)\n",
        "\n",
        "# Check if CUDA is available and move the model to GPU if it is\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(f\"SRC_VOCAB_SIZE:\", SRC_VOCAB_SIZE)\n",
        "print(f\"TGT_VOCAB_SIZE:\", TGT_VOCAB_SIZE)\n",
        "print(f\"MAX_SEQ_LEN:\", MAX_SEQ_LEN)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLAlTlLusiqn",
        "outputId": "9f99df39-b4ad-4031-f4a2-9f1e6c2d7e72"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SRC_VOCAB_SIZE: 30000\n",
            "TGT_VOCAB_SIZE: 30000\n",
            "MAX_SEQ_LEN: 484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "7E5BUa2NZWbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, val_dataloader, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            src = batch[\"encoder_input\"].to(device)\n",
        "            tgt = batch[\"decoder_input\"].to(device)\n",
        "            src_mask = batch[\"encoder_mask\"].to(device)\n",
        "            tgt_mask = batch[\"decoder_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            logits = model(src, tgt, src_mask, tgt_mask)\n",
        "            loss = loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "    model.train()\n",
        "    return avg_val_loss"
      ],
      "metadata": {
        "id": "qhm_GAHW1lrC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def calculate_bleu(data_loader, model, tokenizer_en, tokenizer_fr, device):\n",
        "    model.eval()\n",
        "\n",
        "    references = []  # Actual target sentences\n",
        "    hypotheses = []  # Model's translations\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            # Extract source and target sentences from the batch\n",
        "            src = batch[\"encoder_input\"].to(device)\n",
        "            tgt = batch[\"label\"].to(device)\n",
        "\n",
        "            # Translate the source sentences\n",
        "            translated = [translate_sentence(sentence_tensor, model, tokenizer_en, tokenizer_fr, MAX_SEQ_LEN, device)\n",
        "                          for sentence_tensor in src]\n",
        "\n",
        "            hypotheses.extend(translated)\n",
        "            references.extend([tgt_str.split() for tgt_str in tokenizer_fr.decode_batch(tgt.cpu().tolist())])\n",
        "\n",
        "    # Compute BLEU score\n",
        "    bleu_score = corpus_bleu(references, hypotheses)\n",
        "\n",
        "    return bleu_score\n",
        "\n",
        "def translate_sentence(sentence_tensor, model, tokenizer_en, tokenizer_fr, max_seq_len, device):\n",
        "    # Convert tensor back to string\n",
        "    sentence = tokenizer_en.decode(sentence_tensor.cpu().tolist(), skip_special_tokens=True)\n",
        "\n",
        "    # Tokenize the source text\n",
        "    encoding = tokenizer_en.encode(sentence, add_special_tokens=True)\n",
        "    tokens = encoding.ids\n",
        "\n",
        "    # Handle truncation manually if sequence length exceeds max_seq_len\n",
        "    if len(tokens) > max_seq_len:\n",
        "        tokens = tokens[:max_seq_len-1]  # Leave space for [EOS]\n",
        "    tokens = tokens + [tokenizer_en.token_to_id(\"[EOS]\")]\n",
        "\n",
        "    tokens_tensor = torch.tensor(tokens).unsqueeze(0).to(device)\n",
        "\n",
        "    # Create a source mask\n",
        "    src_mask = (tokens_tensor != tokenizer_en.token_to_id(\"[PAD]\")).unsqueeze(-2).to(device)\n",
        "\n",
        "    # Start decoding with the [SOS] token\n",
        "    decoder_input = torch.tensor([tokenizer_fr.token_to_id(\"[SOS]\")]).unsqueeze(0).to(device)\n",
        "    output_tokens = []\n",
        "\n",
        "    for _ in range(max_seq_len):\n",
        "        with torch.no_grad():\n",
        "            logits = model(tokens_tensor, decoder_input, src_mask, None)  # No target mask in inference\n",
        "        next_token = logits.argmax(2)[:, -1].unsqueeze(1)\n",
        "        output_tokens.append(next_token.item())\n",
        "        decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
        "\n",
        "        # Stop decoding once the [EOS] token is generated\n",
        "        if next_token.item() == tokenizer_fr.token_to_id(\"[EOS]\"):\n",
        "            break\n",
        "\n",
        "    # Decode the outputs to get the translated sentence\n",
        "    decoded = tokenizer_fr.decode(output_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return decoded.split()  # Return the sentence as a list of words\n",
        "\n"
      ],
      "metadata": {
        "id": "KdAVGSjKZQpY"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tensorboard"
      ],
      "metadata": {
        "id": "7WKe-KPgtv10"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Create a summary writer object. The logs will be saved in the 'runs' directory.\n",
        "writer = SummaryWriter()\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 10\n",
        "LR = 0.001\n",
        "PRINT_EVERY = len(train_dataloader) // 16  # Adjust as needed\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
        "\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 3  # Number of epochs to wait for improvement before stopping\n",
        "best_val_loss = float('inf')  # Initialize with a high value\n",
        "early_stop_counter = 0  # Counter to track non-improvement epochs\n",
        "\n",
        "best_bleu = 0.0\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Extract data from the batch\n",
        "        src = batch[\"encoder_input\"].to(device)\n",
        "        tgt = batch[\"decoder_input\"].to(device)\n",
        "        src_mask = batch[\"encoder_mask\"].to(device)\n",
        "        tgt_mask = batch[\"decoder_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(src, tgt, src_mask, tgt_mask)\n",
        "\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print loss every few batches\n",
        "        if (idx + 1) % PRINT_EVERY == 0:\n",
        "            print(f\"Epoch {epoch + 1}/{EPOCHS} | Batch {idx + 1}/{len(train_dataloader)} | Current Batch Loss: {loss.item():.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = validate(model, val_dataloader, loss_fn, device)\n",
        "    print(f\"Validation Loss after Epoch {epoch + 1}/{EPOCHS}: {val_loss:.4f}\")\n",
        "    # bleu = calculate_bleu(val_dataloader, model, tokenizer_en, tokenizer_fr, device)\n",
        "    # print(f\"Epoch {epoch + 1}/{EPOCHS} | BLEU Score: {bleu*100:.2f}\")\n",
        "\n",
        "\n",
        "    # Checkpoint model based on BLEU score\n",
        "    # if bleu > best_bleu:\n",
        "    #     best_bleu = bleu\n",
        "    #     torch.save(model.state_dict(), f'{model_save_path}/best_bleu_model_weights.pth')\n",
        "    #     print(f\"New best BLEU! Model saved to {model_save_path}/best_bleu_model_weights.pth\")\n",
        "\n",
        "    # Log training loss\n",
        "    writer.add_scalar('Loss/train', loss.item(), epoch)\n",
        "\n",
        "    # Log validation loss\n",
        "    writer.add_scalar('Loss/validation', val_loss, epoch)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        early_stop_counter = 0\n",
        "        torch.save(model.state_dict(), f'{model_save_path}/model_weights_epoch_{epoch + 1}.pth')\n",
        "        tokenizer_en.save(f'{tokenizer_en_save_path}/tokenizer_en_directory_epoch_{epoch + 1}')\n",
        "        tokenizer_fr.save(f'{tokenizer_fr_save_path}/tokenizer_fr_directory_epoch_{epoch + 1}')\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        if early_stop_counter >= patience:\n",
        "            print(\"Early stopping due to no improvement in validation loss!\")\n",
        "            break\n",
        "\n",
        "    end_time = time.time()\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS} | Avg Loss: {avg_loss:.4f} | Time Taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    # for monitoring training and validation curves\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    avg_val_loss = validate(model, val_dataloader, loss_fn, device)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), f'{model_save_path}/final_model_weights.pth')\n",
        "tokenizer_en.save(f'{tokenizer_en_save_path}/final_tokenizer_en_directory')\n",
        "tokenizer_fr.save(f'{tokenizer_fr_save_path}/final_tokenizer_fr_directory')\n",
        "\n",
        "writer.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW-eBr1cSd0e",
        "outputId": "f51a8101-aaef-452d-c35a-559402c49869"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Batch 312/5000 | Current Batch Loss: 0.3374\n",
            "Epoch 1/10 | Batch 624/5000 | Current Batch Loss: 0.3822\n",
            "Epoch 1/10 | Batch 936/5000 | Current Batch Loss: 0.2426\n",
            "Epoch 1/10 | Batch 1248/5000 | Current Batch Loss: 0.2798\n",
            "Epoch 1/10 | Batch 1560/5000 | Current Batch Loss: 0.2762\n",
            "Epoch 1/10 | Batch 1872/5000 | Current Batch Loss: 0.2230\n",
            "Epoch 1/10 | Batch 2184/5000 | Current Batch Loss: 0.3615\n",
            "Epoch 1/10 | Batch 2496/5000 | Current Batch Loss: 0.3129\n",
            "Epoch 1/10 | Batch 2808/5000 | Current Batch Loss: 0.2861\n",
            "Epoch 1/10 | Batch 3120/5000 | Current Batch Loss: 0.2992\n",
            "Epoch 1/10 | Batch 3432/5000 | Current Batch Loss: 0.2731\n",
            "Epoch 1/10 | Batch 3744/5000 | Current Batch Loss: 0.2611\n",
            "Epoch 1/10 | Batch 4056/5000 | Current Batch Loss: 0.2103\n",
            "Epoch 1/10 | Batch 4368/5000 | Current Batch Loss: 0.1356\n",
            "Epoch 1/10 | Batch 4680/5000 | Current Batch Loss: 0.1441\n",
            "Epoch 1/10 | Batch 4992/5000 | Current Batch Loss: 0.2796\n",
            "Validation Loss after Epoch 1/10: 0.2315\n",
            "Epoch 1/10 | Avg Loss: 0.3157 | Time Taken: 1109.67 seconds\n",
            "Epoch 2/10 | Batch 312/5000 | Current Batch Loss: 0.1952\n",
            "Epoch 2/10 | Batch 624/5000 | Current Batch Loss: 0.2277\n",
            "Epoch 2/10 | Batch 936/5000 | Current Batch Loss: 0.3305\n",
            "Epoch 2/10 | Batch 1248/5000 | Current Batch Loss: 0.2167\n",
            "Epoch 2/10 | Batch 1560/5000 | Current Batch Loss: 0.1789\n",
            "Epoch 2/10 | Batch 1872/5000 | Current Batch Loss: 0.2057\n",
            "Epoch 2/10 | Batch 2184/5000 | Current Batch Loss: 0.2201\n",
            "Epoch 2/10 | Batch 2496/5000 | Current Batch Loss: 0.2388\n",
            "Epoch 2/10 | Batch 2808/5000 | Current Batch Loss: 0.2669\n",
            "Epoch 2/10 | Batch 3120/5000 | Current Batch Loss: 0.1174\n",
            "Epoch 2/10 | Batch 3432/5000 | Current Batch Loss: 0.1641\n",
            "Epoch 2/10 | Batch 3744/5000 | Current Batch Loss: 0.1558\n",
            "Epoch 2/10 | Batch 4056/5000 | Current Batch Loss: 0.1476\n",
            "Epoch 2/10 | Batch 4368/5000 | Current Batch Loss: 0.2490\n",
            "Epoch 2/10 | Batch 4680/5000 | Current Batch Loss: 0.1564\n",
            "Epoch 2/10 | Batch 4992/5000 | Current Batch Loss: 0.1895\n",
            "Validation Loss after Epoch 2/10: 0.2102\n",
            "Epoch 2/10 | Avg Loss: 0.2138 | Time Taken: 1100.01 seconds\n",
            "Epoch 3/10 | Batch 312/5000 | Current Batch Loss: 0.1272\n",
            "Epoch 3/10 | Batch 624/5000 | Current Batch Loss: 0.1604\n",
            "Epoch 3/10 | Batch 936/5000 | Current Batch Loss: 0.1910\n",
            "Epoch 3/10 | Batch 1248/5000 | Current Batch Loss: 0.1411\n",
            "Epoch 3/10 | Batch 1560/5000 | Current Batch Loss: 0.1698\n",
            "Epoch 3/10 | Batch 1872/5000 | Current Batch Loss: 0.2541\n",
            "Epoch 3/10 | Batch 2184/5000 | Current Batch Loss: 0.1824\n",
            "Epoch 3/10 | Batch 2496/5000 | Current Batch Loss: 0.2073\n",
            "Epoch 3/10 | Batch 2808/5000 | Current Batch Loss: 0.1771\n",
            "Epoch 3/10 | Batch 3120/5000 | Current Batch Loss: 0.2080\n",
            "Epoch 3/10 | Batch 3432/5000 | Current Batch Loss: 0.1904\n",
            "Epoch 3/10 | Batch 3744/5000 | Current Batch Loss: 0.1481\n",
            "Epoch 3/10 | Batch 4056/5000 | Current Batch Loss: 0.2289\n",
            "Epoch 3/10 | Batch 4368/5000 | Current Batch Loss: 0.1638\n",
            "Epoch 3/10 | Batch 4680/5000 | Current Batch Loss: 0.1225\n",
            "Epoch 3/10 | Batch 4992/5000 | Current Batch Loss: 0.1565\n",
            "Validation Loss after Epoch 3/10: 0.2024\n",
            "Epoch 3/10 | Avg Loss: 0.1892 | Time Taken: 1099.79 seconds\n",
            "Epoch 4/10 | Batch 312/5000 | Current Batch Loss: 0.1727\n",
            "Epoch 4/10 | Batch 624/5000 | Current Batch Loss: 0.1808\n",
            "Epoch 4/10 | Batch 936/5000 | Current Batch Loss: 0.2012\n",
            "Epoch 4/10 | Batch 1248/5000 | Current Batch Loss: 0.1149\n",
            "Epoch 4/10 | Batch 1560/5000 | Current Batch Loss: 0.1464\n",
            "Epoch 4/10 | Batch 1872/5000 | Current Batch Loss: 0.1413\n",
            "Epoch 4/10 | Batch 2184/5000 | Current Batch Loss: 0.1405\n",
            "Epoch 4/10 | Batch 2496/5000 | Current Batch Loss: 0.1037\n",
            "Epoch 4/10 | Batch 2808/5000 | Current Batch Loss: 0.1614\n",
            "Epoch 4/10 | Batch 3120/5000 | Current Batch Loss: 0.2432\n",
            "Epoch 4/10 | Batch 3432/5000 | Current Batch Loss: 0.1301\n",
            "Epoch 4/10 | Batch 3744/5000 | Current Batch Loss: 0.1272\n",
            "Epoch 4/10 | Batch 4056/5000 | Current Batch Loss: 0.1932\n",
            "Epoch 4/10 | Batch 4368/5000 | Current Batch Loss: 0.1429\n",
            "Epoch 4/10 | Batch 4680/5000 | Current Batch Loss: 0.1192\n",
            "Epoch 4/10 | Batch 4992/5000 | Current Batch Loss: 0.1443\n",
            "Validation Loss after Epoch 4/10: 0.1995\n",
            "Epoch 4/10 | Avg Loss: 0.1732 | Time Taken: 1101.27 seconds\n",
            "Epoch 5/10 | Batch 312/5000 | Current Batch Loss: 0.1084\n",
            "Epoch 5/10 | Batch 624/5000 | Current Batch Loss: 0.2011\n",
            "Epoch 5/10 | Batch 936/5000 | Current Batch Loss: 0.3194\n",
            "Epoch 5/10 | Batch 1248/5000 | Current Batch Loss: 0.1463\n",
            "Epoch 5/10 | Batch 1560/5000 | Current Batch Loss: 0.1393\n",
            "Epoch 5/10 | Batch 1872/5000 | Current Batch Loss: 0.1503\n",
            "Epoch 5/10 | Batch 2184/5000 | Current Batch Loss: 0.2764\n",
            "Epoch 5/10 | Batch 2496/5000 | Current Batch Loss: 0.1393\n",
            "Epoch 5/10 | Batch 2808/5000 | Current Batch Loss: 0.0904\n",
            "Epoch 5/10 | Batch 3120/5000 | Current Batch Loss: 0.2621\n",
            "Epoch 5/10 | Batch 3432/5000 | Current Batch Loss: 0.1204\n",
            "Epoch 5/10 | Batch 3744/5000 | Current Batch Loss: 0.1031\n",
            "Epoch 5/10 | Batch 4056/5000 | Current Batch Loss: 0.1960\n",
            "Epoch 5/10 | Batch 4368/5000 | Current Batch Loss: 0.1294\n",
            "Epoch 5/10 | Batch 4680/5000 | Current Batch Loss: 0.1754\n",
            "Epoch 5/10 | Batch 4992/5000 | Current Batch Loss: 0.1509\n",
            "Validation Loss after Epoch 5/10: 0.1987\n",
            "Epoch 5/10 | Avg Loss: 0.1618 | Time Taken: 1100.45 seconds\n",
            "Epoch 6/10 | Batch 312/5000 | Current Batch Loss: 0.1184\n",
            "Epoch 6/10 | Batch 624/5000 | Current Batch Loss: 0.2084\n",
            "Epoch 6/10 | Batch 936/5000 | Current Batch Loss: 0.1556\n",
            "Epoch 6/10 | Batch 1248/5000 | Current Batch Loss: 0.2068\n",
            "Epoch 6/10 | Batch 1560/5000 | Current Batch Loss: 0.1268\n",
            "Epoch 6/10 | Batch 1872/5000 | Current Batch Loss: 0.1369\n",
            "Epoch 6/10 | Batch 2184/5000 | Current Batch Loss: 0.1052\n",
            "Epoch 6/10 | Batch 2496/5000 | Current Batch Loss: 0.1666\n",
            "Epoch 6/10 | Batch 2808/5000 | Current Batch Loss: 0.1656\n",
            "Epoch 6/10 | Batch 3120/5000 | Current Batch Loss: 0.1813\n",
            "Epoch 6/10 | Batch 3432/5000 | Current Batch Loss: 0.1159\n",
            "Epoch 6/10 | Batch 3744/5000 | Current Batch Loss: 0.1314\n",
            "Epoch 6/10 | Batch 4056/5000 | Current Batch Loss: 0.2632\n",
            "Epoch 6/10 | Batch 4368/5000 | Current Batch Loss: 0.1686\n",
            "Epoch 6/10 | Batch 4680/5000 | Current Batch Loss: 0.1531\n",
            "Epoch 6/10 | Batch 4992/5000 | Current Batch Loss: 0.1654\n",
            "Validation Loss after Epoch 6/10: 0.1995\n",
            "Epoch 6/10 | Avg Loss: 0.1531 | Time Taken: 1096.21 seconds\n",
            "Epoch 7/10 | Batch 312/5000 | Current Batch Loss: 0.1558\n",
            "Epoch 7/10 | Batch 624/5000 | Current Batch Loss: 0.1692\n",
            "Epoch 7/10 | Batch 936/5000 | Current Batch Loss: 0.1446\n",
            "Epoch 7/10 | Batch 1248/5000 | Current Batch Loss: 0.1693\n",
            "Epoch 7/10 | Batch 1560/5000 | Current Batch Loss: 0.1333\n",
            "Epoch 7/10 | Batch 1872/5000 | Current Batch Loss: 0.1033\n",
            "Epoch 7/10 | Batch 2184/5000 | Current Batch Loss: 0.0803\n",
            "Epoch 7/10 | Batch 2496/5000 | Current Batch Loss: 0.1892\n",
            "Epoch 7/10 | Batch 2808/5000 | Current Batch Loss: 0.1301\n",
            "Epoch 7/10 | Batch 3120/5000 | Current Batch Loss: 0.2139\n",
            "Epoch 7/10 | Batch 3432/5000 | Current Batch Loss: 0.1401\n",
            "Epoch 7/10 | Batch 3744/5000 | Current Batch Loss: 0.1961\n",
            "Epoch 7/10 | Batch 4056/5000 | Current Batch Loss: 0.1143\n",
            "Epoch 7/10 | Batch 4368/5000 | Current Batch Loss: 0.1532\n",
            "Epoch 7/10 | Batch 4680/5000 | Current Batch Loss: 0.1439\n",
            "Epoch 7/10 | Batch 4992/5000 | Current Batch Loss: 0.1998\n",
            "Validation Loss after Epoch 7/10: 0.2001\n",
            "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch 7/10 | Avg Loss: 0.1462 | Time Taken: 1102.20 seconds\n",
            "Epoch 8/10 | Batch 312/5000 | Current Batch Loss: 0.0928\n",
            "Epoch 8/10 | Batch 624/5000 | Current Batch Loss: 0.0776\n",
            "Epoch 8/10 | Batch 936/5000 | Current Batch Loss: 0.1232\n",
            "Epoch 8/10 | Batch 1248/5000 | Current Batch Loss: 0.1717\n",
            "Epoch 8/10 | Batch 1560/5000 | Current Batch Loss: 0.1175\n",
            "Epoch 8/10 | Batch 1872/5000 | Current Batch Loss: 0.1500\n",
            "Epoch 8/10 | Batch 2184/5000 | Current Batch Loss: 0.0854\n",
            "Epoch 8/10 | Batch 2496/5000 | Current Batch Loss: 0.1498\n",
            "Epoch 8/10 | Batch 2808/5000 | Current Batch Loss: 0.1140\n",
            "Epoch 8/10 | Batch 3120/5000 | Current Batch Loss: 0.1028\n",
            "Epoch 8/10 | Batch 3432/5000 | Current Batch Loss: 0.1517\n",
            "Epoch 8/10 | Batch 3744/5000 | Current Batch Loss: 0.1067\n",
            "Epoch 8/10 | Batch 4056/5000 | Current Batch Loss: 0.1580\n",
            "Epoch 8/10 | Batch 4368/5000 | Current Batch Loss: 0.0646\n",
            "Epoch 8/10 | Batch 4680/5000 | Current Batch Loss: 0.1599\n",
            "Epoch 8/10 | Batch 4992/5000 | Current Batch Loss: 0.1006\n",
            "Validation Loss after Epoch 8/10: 0.1992\n",
            "Early stopping due to no improvement in validation loss!\n",
            "CPU times: user 2h 31min 57s, sys: 25.8 s, total: 2h 32min 23s\n",
            "Wall time: 2h 34min 58s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "while True:\n",
        "    print(\"\\n------\")\n",
        "    !nvidia-smi\n",
        "    print(\"------\\n\")\n",
        "    time.sleep(60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JbyjgUVe0zwP",
        "outputId": "b14f7fad-0a84-4e3d-e8d2-df26c9a92667"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------\n",
            "Fri Sep  8 17:25:29 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "------\n",
            "\n",
            "\n",
            "------\n",
            "Fri Sep  8 17:26:30 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "------\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a8be9b4d2fea>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nvidia-smi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(train_losses, label=\"Training\")\n",
        "plt.plot(val_losses, label=\"Validation\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "soTo3VfNsWRE",
        "outputId": "394e9e97-6797-46b7-e69c-36f0d6d3348a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, unload the TensorBoard extension to make sure it's fully stopped.\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "id": "G1yZLE40wBkl",
        "outputId": "976b7c59-349b-4652-b037-16d25bdbd781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 7418), started 0:02:43 ago. (Use '!kill 7418' to kill it.)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Tensorboard files"
      ],
      "metadata": {
        "id": "BvqNY2z-pkD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/runs.zip /content/runs/\n"
      ],
      "metadata": {
        "id": "MLutpXL7v_NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/runs.zip')\n"
      ],
      "metadata": {
        "id": "qrawMhjwwoIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "v1oNh9tgBtO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tokenizers"
      ],
      "metadata": {
        "id": "nAIRbhxORvh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "\n",
        "# Paths where tokenizers were saved\n",
        "tokenizer_en_path = f\"{tokenizer_en_save_path}/final_tokenizer_en_directory\"\n",
        "tokenizer_fr_path = f\"{tokenizer_fr_save_path}/final_tokenizer_fr_directory\"\n",
        "\n",
        "# Load the tokenizers\n",
        "tokenizer_en = Tokenizer.from_file(tokenizer_en_path)\n",
        "tokenizer_fr = Tokenizer.from_file(tokenizer_fr_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W6_IvcVAQmvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Hyperparameters\n",
        "\n",
        "MAX_SEQ_LEN = 234\n",
        "\n",
        "SRC_VOCAB_SIZE = tokenizer_en.get_vocab_size()\n",
        "TGT_VOCAB_SIZE = tokenizer_fr.get_vocab_size()\n",
        "EMBED_SIZE = 128\n",
        "N = 1 # Number of encoder and decoder blocks\n",
        "H = 4 # Number of attention heads\n",
        "DROPOUT = 0.1\n",
        "D_FF = 2048\n",
        "\n",
        "# Initialize the transformer model\n",
        "model = build_transformer(SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, MAX_SEQ_LEN, EMBED_SIZE, N, H, DROPOUT, D_FF)\n",
        "\n",
        "# Check if CUDA is available and move the model to GPU if it is\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(f\"SRC_VOCAB_SIZE:\", SRC_VOCAB_SIZE)\n",
        "print(f\"TGT_VOCAB_SIZE:\", TGT_VOCAB_SIZE)\n",
        "print(f\"MAX_SEQ_LEN:\", MAX_SEQ_LEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6W6omKGRkEP",
        "outputId": "77ee4e57-841a-4def-c581-f98225c22338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SRC_VOCAB_SIZE: 8534\n",
            "TGT_VOCAB_SIZE: 10873\n",
            "MAX_SEQ_LEN: 234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = f\"{model_save_path}/final_model_weights.pth\"\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KbaitrFPp6B",
        "outputId": "5b4b8684-20ba-4ab9-9fbd-f1c5d747b669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "def translate_sentence(sentence, model, tokenizer_en, tokenizer_fr, max_seq_len, device):\n",
        "    # Tokenize the source text\n",
        "    encoding = tokenizer_en.encode(sentence, add_special_tokens=True)\n",
        "    tokens = encoding.ids\n",
        "\n",
        "    # Handle truncation manually if sequence length exceeds max_seq_len\n",
        "    if len(tokens) > max_seq_len:\n",
        "        tokens = tokens[:max_seq_len-1]  # Leave space for [EOS]\n",
        "    tokens = tokens + [tokenizer_en.token_to_id(\"[EOS]\")]\n",
        "\n",
        "    tokens_tensor = torch.tensor(tokens).unsqueeze(0).to(device)\n",
        "\n",
        "    print(\"tokens_tensor: \", tokens_tensor)\n",
        "\n",
        "    # Create a source mask\n",
        "    src_mask = (tokens_tensor != tokenizer_en.token_to_id(\"[PAD]\")).unsqueeze(-2).to(device)\n",
        "\n",
        "    print(\"src_mask:\", src_mask)\n",
        "\n",
        "    # Start decoding with the [SOS] token\n",
        "    decoder_input = torch.tensor([tokenizer_fr.token_to_id(\"[SOS]\")]).unsqueeze(0).to(device)\n",
        "    output_tokens = []\n",
        "\n",
        "    for _ in range(max_seq_len):\n",
        "        with torch.no_grad():\n",
        "            logits = model(tokens_tensor, decoder_input, src_mask, None)  # No target mask in inference\n",
        "        next_token = logits.argmax(2)[:, -1].unsqueeze(1)\n",
        "        output_tokens.append(next_token.item())\n",
        "        decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
        "\n",
        "        # Stop decoding once the [EOS] token is generated\n",
        "        if next_token.item() == tokenizer_fr.token_to_id(\"[EOS]\"):\n",
        "            break\n",
        "\n",
        "    # Decode the outputs to get the translated sentence\n",
        "    decoded = tokenizer_fr.decode(output_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return decoded\n",
        "\n",
        "\n",
        "source_sentence = \"The best day\"\n",
        "translated = translate_sentence(source_sentence, model, tokenizer_en, tokenizer_fr, MAX_SEQ_LEN, device)\n",
        "print(translated)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfNqjIO1Buus",
        "outputId": "ac3c0128-fa9a-4d25-bb6e-2d118f65c0d1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens_tensor:  tensor([[ 49, 537, 125,   3]], device='cuda:0')\n",
            "src_mask: tensor([[[True, True, True, True]]], device='cuda:0')\n",
            "Le meilleur jour .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate model on test set"
      ],
      "metadata": {
        "id": "iqn-LQter_Eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "def evaluate_on_test(model, test_dataloader, loss_fn, device):\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            # Extract data from the batch\n",
        "            src = batch[\"encoder_input\"].to(device)\n",
        "            tgt = batch[\"decoder_input\"].to(device)\n",
        "            src_mask = batch[\"encoder_mask\"].to(device)\n",
        "            tgt_mask = batch[\"decoder_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(src, tgt, src_mask, tgt_mask)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_test_loss = total_loss / len(test_dataloader)\n",
        "    return avg_test_loss\n",
        "\n",
        "test_loss = evaluate_on_test(model, test_dataloader, loss_fn, device)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bKzrZSBsBTG",
        "outputId": "1f925d3e-c04b-4a88-a326-0ecca08c6d3c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.1987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Translations"
      ],
      "metadata": {
        "id": "6c8kwLYgzYWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "# Choose a random batch from the test dataloader\n",
        "batch = next(iter(test_dataloader))\n",
        "\n",
        "# Extract a few samples from the batch\n",
        "num_samples = 5\n",
        "indices = random.sample(range(batch['encoder_input'].shape[0]), num_samples)\n",
        "\n",
        "for idx in indices:\n",
        "    source_sentence = tokenizer_en.decode(batch['encoder_input'][idx].tolist(), skip_special_tokens=True)\n",
        "    actual_translation = tokenizer_fr.decode(batch['label'][idx].tolist(), skip_special_tokens=True)\n",
        "\n",
        "    # Model's translation\n",
        "    translated_sentence = translate_sentence(source_sentence, model, tokenizer_en, tokenizer_fr, MAX_SEQ_LEN, device)\n",
        "\n",
        "    print(f\"Source Sentence: {source_sentence}\")\n",
        "    print(f\"Model's Translation: {translated_sentence}\")\n",
        "    print(f\"Actual Translation: {actual_translation}\")\n",
        "    print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5QC7WN9zcQA",
        "outputId": "c253c277-9e70-4859-a514-024625b7223f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens_tensor:  tensor([[  225,   590,     4,   349,     4,    58,    50,   151,  3727,   365,\n",
            "            23,  1094,     9,   744,    45,     5,  5347,  1713,     7,    10,\n",
            "         18719,  3185,    12,     5,  2559,    16,    17,    14,     8,  1324,\n",
            "             5,   319,  1296,     4,     9,  3681,    14,     5,     6,     3]],\n",
            "       device='cuda:0')\n",
            "src_mask: tensor([[[True, True, True, True, True, True, True, True, True, True, True,\n",
            "          True, True, True, True, True, True, True, True, True, True, True,\n",
            "          True, True, True, True, True, True, True, True, True, True, True,\n",
            "          True, True, True, True, True, True, True]]], device='cuda:0')\n",
            "Source Sentence: At present , indeed , they were well supplied both with news and happiness by the recent arrival of a militia regiment in the neighbourhood ; it was to remain the whole winter , and Meryton was the .\n",
            "Model's Translation: Au but , ils furent bien servis à souhait d ’ un bonheur et de Lydia , par la nouvelle demeure d ’ un régiment fut définitivement installée dans le régiment , et le régiment fut définitivement installée a Meryton .\n",
            "Actual Translation: Si peu fertile que fut le pays en événements extraordinaires , elles arrivaient toujours a quelques nouvelles chez leur tante .\n",
            "================================================================================\n",
            "tokens_tensor:  tensor([[   13,  4078,     5,   554,     7, 19894,     4,  2172,    54,     5,\n",
            "           389,   572,    31,     4,    13,     9,   309,    12,     5,  4963,\n",
            "            15, 11008,     5,  6253,  9758,     7, 17634, 15901,     9, 22943,\n",
            "         15901,     6,     3]], device='cuda:0')\n",
            "src_mask: tensor([[[True, True, True, True, True, True, True, True, True, True, True,\n",
            "          True, True, True, True, True, True, True, True, True, True, True,\n",
            "          True, True, True, True, True, True, True, True, True, True, True]]],\n",
            "       device='cuda:0')\n",
            "Source Sentence: \" Near the island of Santorini , professor ,\" the captain answered me , \" and right in the channel that separates the volcanic islets of Nea Kameni and Palea Kameni .\n",
            "Model's Translation: « L ' île , monsieur le professeur , me répondit le capitaine Nemo , et voici que Néa - Kamenni et le canal de Paléa - Kamenni .\n",
            "Actual Translation: -- Près de l ' île Santorin , monsieur le professeur , me répondit le capitaine , et précisément dans ce canal qui sépare Néa - Kamenni de Paléa - Kamenni .\n",
            "================================================================================\n",
            "tokens_tensor:  tensor([[ 225,  178,    4,   75,   18,   21, 2733,   20, 4280,    4,  231,   34,\n",
            "           20, 1583,    4, 2070,   20, 3070,    4,    9, 5882,   20, 4215,    4,\n",
            "           18, 5047, 8195,  130,   34,   20, 1342,    6,    3]],\n",
            "       device='cuda:0')\n",
            "src_mask: tensor([[[True, True, True, True, True, True, True, True, True, True, True,\n",
            "          True, True, True, True, True, True, True, True, True, True, True,\n",
            "          True, True, True, True, True, True, True, True, True, True, True]]],\n",
            "       device='cuda:0')\n",
            "Source Sentence: At last , when he had eaten his soup , put on his cloak , lighted his pipe , and grasped his whip , he calmly installed himself on his seat .\n",
            "Model's Translation: Enfin , quand il avait mangé son col , il mit sa pipe , et prit son manteau , et , le cocher , suivi de cravache à son siège , il se mit à sourire , le siège .\n",
            "Actual Translation: Enfin , lorsqu ’ il avait mangé sa soupe , endossé sa , allumé sa pipe et son fouet , il s ’ installait tranquillement sur le siège .\n",
            "================================================================================\n",
            "tokens_tensor:  tensor([[1256,    4,  349,    4, 8375,  979,   15,  963,   14, 5800,   19,   42,\n",
            "          170,    6,    3]], device='cuda:0')\n",
            "src_mask: tensor([[[True, True, True, True, True, True, True, True, True, True, True,\n",
            "          True, True, True, True]]], device='cuda:0')\n",
            "Source Sentence: Perhaps , indeed , Marowsko believed that Jean was Marechal ' s son .\n",
            "Model's Translation: Peut - être Marowsko , Marowsko était - Jean , fils de Maréchal .\n",
            "Actual Translation: Peut - être même croyait - il que Jean était le fils de Maréchal .\n",
            "================================================================================\n",
            "tokens_tensor:  tensor([[  19,  490,   89,    4,   24,  367,   18,   46,    4,   10,  100,  672,\n",
            "            4, 1075,    9, 7018,   20,  839,    4,   19,  284,   77,    7,   17,\n",
            "          367,    3]], device='cuda:0')\n",
            "src_mask: tensor([[[True, True, True, True, True, True, True, True, True, True, True,\n",
            "          True, True, True, True, True, True, True, True, True, True, True,\n",
            "          True, True, True, True]]], device='cuda:0')\n",
            "Source Sentence: ' Now then , you !' he said , a little pale , rising and shutting his book , ' get out of it !'\n",
            "Model's Translation: – Alors , dit - il en levant , un petit livre pâle et en se levant , en se levant .\n",
            "Actual Translation: – Toi , dit - il , en se redressant et en fermant son livre , un peu pâle , tu vas commencer par sortir d ’ ici !\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate BLEU scores"
      ],
      "metadata": {
        "id": "TGEIQEcicI2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence, model, tokenizer_en, tokenizer_fr, max_seq_len, device):\n",
        "    # Tokenize the source text\n",
        "    encoding = tokenizer_en.encode(sentence, add_special_tokens=True)\n",
        "    tokens = encoding.ids\n",
        "\n",
        "    # Handle truncation manually if sequence length exceeds max_seq_len\n",
        "    if len(tokens) > max_seq_len:\n",
        "        tokens = tokens[:max_seq_len-1]  # Leave space for [EOS]\n",
        "    tokens = tokens + [tokenizer_en.token_to_id(\"[EOS]\")]\n",
        "\n",
        "    tokens_tensor = torch.tensor(tokens).unsqueeze(0).to(device)\n",
        "\n",
        "    # Create a source mask\n",
        "    src_mask = (tokens_tensor != tokenizer_en.token_to_id(\"[PAD]\")).unsqueeze(-2).to(device)\n",
        "\n",
        "    # Start decoding with the [SOS] token\n",
        "    decoder_input = torch.tensor([tokenizer_fr.token_to_id(\"[SOS]\")]).unsqueeze(0).to(device)\n",
        "    output_tokens = []\n",
        "\n",
        "    for _ in range(max_seq_len):\n",
        "        with torch.no_grad():\n",
        "            logits = model(tokens_tensor, decoder_input, src_mask, None)  # No target mask in inference\n",
        "        next_token = logits.argmax(2)[:, -1].unsqueeze(1)\n",
        "        output_tokens.append(next_token.item())\n",
        "        decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
        "\n",
        "        # Stop decoding once the [EOS] token is generated\n",
        "        if next_token.item() == tokenizer_fr.token_to_id(\"[EOS]\"):\n",
        "            break\n",
        "\n",
        "    # Decode the outputs to get the translated sentence\n",
        "    decoded = tokenizer_fr.decode(output_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return decoded"
      ],
      "metadata": {
        "id": "L9m7-XWkepSC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def calculate_bleu(data_loader, model, tokenizer_en, tokenizer_fr, device):\n",
        "    model.eval()\n",
        "\n",
        "    references = []  # Actual target sentences\n",
        "    hypotheses = []  # Model's translations\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            # Extract source and target sentences from the batch\n",
        "            src = batch[\"encoder_input\"].to(device)\n",
        "            tgt = batch[\"label\"].to(device)\n",
        "\n",
        "            # Translate the source sentences\n",
        "            translated = [translate_sentence(tokenizer_en.decode(sentence.tolist(), skip_special_tokens=True),\n",
        "                                              model, tokenizer_en, tokenizer_fr, MAX_SEQ_LEN, device)\n",
        "                          for sentence in src]\n",
        "\n",
        "            tgt = [tokenizer_fr.decode(t.tolist(), skip_special_tokens=True) for t in tgt]\n",
        "\n",
        "            hypotheses.extend(translated)\n",
        "            for t in tgt:\n",
        "                references.append([t])  # Note the list inside a list\n",
        "\n",
        "    # Compute BLEU score\n",
        "    bleu_score = corpus_bleu(references, hypotheses)\n",
        "    return bleu_score\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GtKdBLGldTtk"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test first on small dataset\n",
        "\n",
        "# 1. Create a subset of the test dataset\n",
        "small_test_subset = torch.utils.data.Subset(test_dataset, indices=range(50))\n",
        "\n",
        "# 2. Create a DataLoader for this subset\n",
        "small_test_dataloader = torch.utils.data.DataLoader(small_test_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# 3. Compute the BLEU score using the smaller DataLoader\n",
        "bleu_score_small = calculate_bleu(small_test_dataloader, model, tokenizer_en, tokenizer_fr, device)\n",
        "print(f\"Small Test BLEU Score: {bleu_score_small:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s21jBga9fZM5",
        "outputId": "e7766d99-e12b-40ad-dbea-c7b51fb0f94d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small Test BLEU Score: 0.1525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After computing the test loss:\n",
        "bleu_score = calculate_bleu(test_dataloader, model, tokenizer_en, tokenizer_fr, device)\n",
        "print(f\"Test BLEU Score: {bleu_score:.4f}\")"
      ],
      "metadata": {
        "id": "ujqXaGitf_XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter Count"
      ],
      "metadata": {
        "id": "8JTvgbc2KupD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params = count_parameters(model)\n",
        "print(f\"Total number of trainable parameters: {total_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSepcZLN8Miz",
        "outputId": "ab137dd3-5bbb-4bff-f169-70dbbc9a1402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of trainable parameters: 11517928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# manual parameter count\n",
        "\n",
        "source_embeddings = SRC_VOCAB_SIZE * EMBED_SIZE\n",
        "target_embeddings = TGT_VOCAB_SIZE * EMBED_SIZE\n",
        "embeddings = source_embeddings + target_embeddings\n",
        "\n",
        "encoder_block = (4*EMBED_SIZE*EMBED_SIZE)+(2*EMBED_SIZE*D_FF)\n",
        "decoder_block = (8*EMBED_SIZE*EMBED_SIZE)+(2*EMBED_SIZE*D_FF)\n",
        "\n",
        "output = EMBED_SIZE * TGT_VOCAB_SIZE\n",
        "\n",
        "total_paramter_count = embeddings + N*encoder_block + N*decoder_block + output\n",
        "print(\"number of parameters: \", total_paramter_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMl7otWcGq_q",
        "outputId": "d0539141-da72-4825-e225-20593706b81b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters:  48300032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# manual parameter count with biases\n",
        "\n",
        "# Embeddings\n",
        "source_embeddings = SRC_VOCAB_SIZE * EMBED_SIZE\n",
        "target_embeddings = TGT_VOCAB_SIZE * EMBED_SIZE\n",
        "embeddings = source_embeddings + target_embeddings\n",
        "\n",
        "# Encoder\n",
        "encoder_self_attention = 4 * EMBED_SIZE * EMBED_SIZE + 4 * EMBED_SIZE  # Including biases\n",
        "encoder_ffn = 2 * EMBED_SIZE * D_FF + EMBED_SIZE + D_FF  # Including biases\n",
        "encoder_layer_norm = 2 * 2 * EMBED_SIZE  # 2 (gamma and beta) for 2 layer norms\n",
        "encoder_block = encoder_self_attention + encoder_ffn + encoder_layer_norm\n",
        "\n",
        "# Decoder\n",
        "decoder_self_attention = 4 * EMBED_SIZE * EMBED_SIZE + 4 * EMBED_SIZE  # Including biases\n",
        "decoder_cross_attention = 4 * EMBED_SIZE * EMBED_SIZE + 4 * EMBED_SIZE  # Including biases\n",
        "decoder_ffn = 2 * EMBED_SIZE * D_FF + EMBED_SIZE + D_FF  # Including biases\n",
        "decoder_layer_norm = 3 * 2 * EMBED_SIZE  # 2 (gamma and beta) for 3 layer norms\n",
        "decoder_block = decoder_self_attention + decoder_cross_attention + decoder_ffn + decoder_layer_norm\n",
        "\n",
        "# Output Layer\n",
        "output = EMBED_SIZE * TGT_VOCAB_SIZE\n",
        "\n",
        "total_parameter_count = embeddings + N * encoder_block + N * decoder_block + output\n",
        "print(\"Adjusted number of parameters: \", total_parameter_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ral6a7XsJjIE",
        "outputId": "6797f654-fc32-4a03-e54c-4a0df0c284d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusted number of parameters:  48316416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "\n",
        "for _ in tqdm(range(10)):\n",
        "    time.sleep(0.5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "fc5a9a7a9a4840dda0f8178b6f3e922d",
            "c778529a8f5942c4a017a03e96d2f043",
            "a6a719441673488c8ff1893db8c51c0c",
            "e768eb65f9434d97a0212908afa2745a",
            "e275122cc01343f6b92cd9d8d4eb6231",
            "3d5e3096bcbf46e1ac5dae94e924e8c4",
            "904691aaff4841bbbb2be97be4a552c3",
            "4df9e5294fc942839b5dfe17a292b470",
            "6b1a88e600d043de8f3c0af49c594c07",
            "1e24d789f52d4fd89f96d0ead6d48029",
            "5ae1cba063544721a07672d8d649eb46"
          ]
        },
        "id": "V2BTYP4uKN0i",
        "outputId": "325d638b-a636-4ab4-c200-9860c079deaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc5a9a7a9a4840dda0f8178b6f3e922d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Notes"
      ],
      "metadata": {
        "id": "AUBxJ6TQyCrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Position Encoding code explanation\n",
        "\n",
        "1. **Initialization**:\n",
        "```python\n",
        "pe = torch.zeros(max_len, embed_size)\n",
        "```\n",
        "This initializes a matrix of size `(max_len, embed_size)`, where `max_len` is the maximum sequence length and `embed_size` is the embedding size. Each row of this matrix corresponds to a position in a sequence, and each column corresponds to a dimension in the embedding space.\n",
        "\n",
        "2. **Position Vector Creation**:\n",
        "```python\n",
        "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "```\n",
        "This creates a vector of size `(max_len, 1)`. Each element of this vector is essentially the index of the position in a sequence.\n",
        "\n",
        "3. **Division Term Computation**:\n",
        "```python\n",
        "div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(base) / embed_size))\n",
        "```\n",
        "This computes the `div_term` used in the positional encoding formula. This tensor is of size `(embed_size//2,)`, which means it has half the size of the embedding size. We use `embed_size, 2` in `arange` to skip every other value because the positional encoding formula applies the sine function to even indices and the cosine function to odd indices.\n",
        "\n",
        "4. **Applying the Positional Encoding Formula**:\n",
        "```python\n",
        "pe[:, 0::2] = torch.sin(position * div_term)  # sine applied to even indices\n",
        "pe[:, 1::2] = torch.cos(position * div_term)  # cosine applied to odd indices\n",
        "```\n",
        "For the sine function:\n",
        "- `position * div_term` results in broadcasting, so its size becomes `(max_len, embed_size//2)`.\n",
        "- `pe[:, 0::2]` selects all rows and even columns of the `pe` matrix. Thus, the size is `(max_len, embed_size//2)`.\n",
        "  \n",
        "For the cosine function:\n",
        "- The size remains `(max_len, embed_size//2)` since it's the same operation but applied to the odd columns of the `pe` matrix.\n",
        "\n",
        "5. **Add Batch Dimension**:\n",
        "```python\n",
        "pe = pe.unsqueeze(0)  # Add a batch dimension for broadcasting\n",
        "```\n",
        "The `unsqueeze(0)` function adds a new dimension at the beginning of the tensor, making its size `(1, max_len, embed_size)`. This allows for broadcasting the positional encodings to any batch size when adding them to the embeddings.\n",
        "\n",
        "In summary, the code generates a positional encoding matrix that, when added to embeddings of sequences, provides the model with information about the position of each token in the sequence.\n",
        "\n",
        "---\n",
        "\n",
        "The line `x = x + self.pe[:, :x.size(1)]` is adding the positional encoding to the input embeddings `x`.\n",
        "\n",
        "Let's break it down:\n",
        "\n",
        "1. `x`: This is a tensor representing the embeddings of a batch of sequences. Its shape is typically `(batch_size, seq_len, embed_size)`.\n",
        "    - `batch_size` is the number of sequences in the batch.\n",
        "    - `seq_len` is the length of each sequence in the batch (after padding if any).\n",
        "    - `embed_size` is the size of the embedding for each token in the sequence.\n",
        "\n",
        "2. `self.pe`: This is the tensor that contains the pre-computed positional encodings for all possible positions up to `max_len`. Its shape is `(1, max_len, embed_size)`, as defined in the PositionalEncoding class you provided earlier.\n",
        "\n",
        "3. `self.pe[:, :x.size(1)]`: This slices the `self.pe` tensor to match the sequence length of the current batch `x`.\n",
        "    - `x.size(1)` gets the sequence length of the current batch `x`.\n",
        "    - `self.pe[:, :x.size(1)]` takes all positional encodings up to the current sequence length. The resulting shape of this sliced tensor is `(1, seq_len, embed_size)`.\n",
        "\n",
        "4. `x = x + self.pe[:, :x.size(1)]`: This operation adds the positional encodings to the embeddings.\n",
        "    - Since the first dimension of `self.pe[:, :x.size(1)]` is 1 (batch dimension), it broadcasts to the batch size of `x`.\n",
        "    - The result is that each sequence in the batch `x` gets the same positional encodings added to it. The updated `x` still has the shape `(batch_size, seq_len, embed_size)`.\n",
        "\n",
        "In essence, this line ensures that the model is aware of the position of each token within a sequence by adding positional information to the embeddings of the tokens."
      ],
      "metadata": {
        "id": "K7I_-KsCyEJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "position = torch.arange(0, 10, dtype=torch.float)\n",
        "print(position.shape)\n",
        "print(position)"
      ],
      "metadata": {
        "id": "TugPAWiuyquM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "position = torch.arange(0, 10, dtype=torch.float).unsqueeze(1)\n",
        "print(position.shape)\n",
        "print(position)\n"
      ],
      "metadata": {
        "id": "ySny67RvyX4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Slicing example\n",
        "\n",
        "Imagine a box of chocolates with 3 layers, where each layer has a set of chocolates arranged in rows and columns. Here's how we can represent this box:\n",
        "\n",
        "$$\n",
        "\\text{chocolates} =\n",
        "\\begin{bmatrix}\n",
        "    \\text{layer 1 (bottom layer)} \\\\\n",
        "    \\text{layer 2 (middle layer)} \\\\\n",
        "    \\text{layer 3 (top layer)}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Each layer has chocolates arranged in rows and columns:\n",
        "\n",
        "$$\n",
        "\\text{layer} =\n",
        "\\begin{bmatrix}\n",
        "    \\text{a} & \\text{b} & \\text{c} & \\text{d} \\\\\n",
        "    \\text{e} & \\text{f} & \\text{g} & \\text{h} \\\\\n",
        "    \\text{i} & \\text{j} & \\text{k} & \\text{l} \\\\\n",
        "    \\text{m} & \\text{n} & \\text{o} & \\text{p}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Now, let's understand the slicing:\n",
        "\n",
        "1. **First Dimension (Layers)**:\n",
        "   - `:` means we want all layers, so we're taking the entire box of chocolates.\n",
        "\n",
        "2. **Second Dimension (Rows)**:\n",
        "   - `:3` means we want only the first 3 rows from each layer. So, from each layer, we'll exclude the last row.\n",
        "\n",
        "3. **Third Dimension (Columns)**:\n",
        "   - `:` means we want all columns from the selected rows.\n",
        "\n",
        "Using the slicing `[:, :3, :]` on our box of chocolates:\n",
        "\n",
        "$$\n",
        "\\text{selected\\_chocolates} =\n",
        "\\begin{bmatrix}\n",
        "    \\begin{bmatrix}\n",
        "        \\text{a} & \\text{b} & \\text{c} & \\text{d} \\\\\n",
        "        \\text{e} & \\text{f} & \\text{g} & \\text{h} \\\\\n",
        "        \\text{i} & \\text{j} & \\text{k} & \\text{l} \\\\\n",
        "    \\end{bmatrix} \\\\\n",
        "    \\begin{bmatrix}\n",
        "        \\text{a} & \\text{b} & \\text{c} & \\text{d} \\\\\n",
        "        \\text{e} & \\text{f} & \\text{g} & \\text{h} \\\\\n",
        "        \\text{i} & \\text{j} & \\text{k} & \\text{l} \\\\\n",
        "    \\end{bmatrix} \\\\\n",
        "    \\begin{bmatrix}\n",
        "        \\text{a} & \\text{b} & \\text{c} & \\text{d} \\\\\n",
        "        \\text{e} & \\text{f} & \\text{g} & \\text{h} \\\\\n",
        "        \\text{i} & \\text{j} & \\text{k} & \\text{l} \\\\\n",
        "    \\end{bmatrix}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "So, using `[:, :3, :]` on our 3D tensor gives us all layers, but only the first 3 rows of chocolates from each layer, and all columns from those rows."
      ],
      "metadata": {
        "id": "_5Q34Rz17fWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer Normalization\n",
        "\n",
        "Layer Normalization (LayerNorm) is a normalization technique similar to Batch Normalization (BatchNorm), but with a key difference in the dimensions over which the normalization is computed.\n",
        "\n",
        "#### What is getting normalized?\n",
        "\n",
        "In LayerNorm, normalization is applied over the feature dimensions (often referred to as the hidden dimensions or channels). It normalizes the features of each individual item in a batch independently. In contrast, BatchNorm normalizes across the batch dimension.\n",
        "\n",
        "Given an input tensor of shape \\([8, 32, 10]\\):\n",
        "- The first dimension (8) is the batch size.\n",
        "- The second dimension (32) is the sequence length or time steps.\n",
        "- The third dimension (10) is the feature size or hidden dimension.\n",
        "\n",
        "For LayerNorm, normalization is applied over the last dimension (feature size). This means that for each item in the batch and for each time step in the sequence, the features (of size 10 in this example) are normalized such that they have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "#### Example:\n",
        "\n",
        "Let's consider a single data point from the batch (ignoring the batch dimension for simplicity), and a single time step from the sequence (ignoring the sequence dimension as well). This gives us a vector of size 10 (our feature size).\n",
        "\n",
        "Original feature vector (size 10): \\([2, 5, 1, 6, 7, 4, 9, 3, 0, 8]\\)\n",
        "\n",
        "Let's say the computed mean (μ) of this vector is 4.5 and the standard deviation (σ) is 2.9.\n",
        "\n",
        "Normalized feature vector (size 10) will be:\n",
        "$$\n",
        "\\left( \\frac{[2, 5, 1, 6, 7, 4, 9, 3, 0, 8] - μ}{σ} \\right)\n",
        "$$\n",
        "\n",
        "This operation will be done for each time step for each item in the batch. After normalization, a learnable scale and shift will be applied to the features. This allows the network to decide the best distribution for the features.\n",
        "\n",
        "### Benefits:\n",
        "\n",
        "1. **Stability**: LayerNorm can stabilize the hidden state dynamics in recurrent networks, making training more predictable and stable.\n",
        "2. **Independence**: LayerNorm normalizes each item independently, making it batch size agnostic. This means the statistics computed for normalization are not dependent on the batch size.\n",
        "3. **Improved Training**: Like other normalization techniques, LayerNorm often leads to faster convergence and can allow for higher learning rates.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "To sum up, in the given tensor of size \\([8, 32, 10]\\), LayerNorm will ensure that for each of the 8 items in the batch, for each of the 32 time steps, the 10 features are normalized individually based on their own mean and standard deviation."
      ],
      "metadata": {
        "id": "k4iLcDXEBLHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `nn.LayerNorm`\n",
        "\n",
        "Using `nn.LayerNorm(embed_size)` from PyTorch's built-in module is indeed a recommended approach for most use cases. The PyTorch team has optimized it for performance, and it abstracts away the internal workings, making it simpler and less prone to mistakes.\n",
        "\n",
        "Here are some reasons to consider using the built-in `nn.LayerNorm`:\n",
        "\n",
        "1. **Optimization**: PyTorch's built-in layers are optimized for performance. They often have CUDA implementations that make them faster on GPUs.\n",
        "2. **Maintainability**: Using built-in layers can make your codebase smaller, more readable, and easier to maintain.\n",
        "3. **Robustness**: The built-in layers are widely used and tested by the community. They are less likely to have bugs compared to custom implementations.\n",
        "4. **Compatibility**: Future updates or changes in the PyTorch framework are more likely to remain compatible with their built-in layers.\n",
        "\n",
        "However, there are scenarios where you might want to implement LayerNorm (or other layers) from scratch:\n",
        "\n",
        "1. **Educational Purposes**: Implementing layers from scratch is a great way to understand their inner workings.\n",
        "2. **Custom Modifications**: If you have a variant of LayerNorm that's not captured by the built-in module, you might need a custom implementation.\n",
        "3. **Research**: When trying out new normalization techniques or building upon LayerNorm, a custom implementation might be necessary.\n",
        "\n",
        "But for most standard use cases, especially in production models, leveraging the built-in `nn.LayerNorm(embed_size)` is a good choice. If you don't have any specific requirements that necessitate a custom implementation, it's advisable to use the built-in one."
      ],
      "metadata": {
        "id": "pD9VSMzJCqeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `contiguous()`\n",
        "\n",
        "The use of `.contiguous()` followed by `.view()` is often a defensive coding practice to ensure that the reshaping operations with `.view()` do not result in errors. Here's why:\n",
        "\n",
        "1. **Tensor Memory Layout**: In PyTorch, tensors have their data laid out in memory. When you perform operations like transposing, the logical structure of the tensor changes, but the underlying data might not be rearranged in memory. Instead, PyTorch uses `strides` to index into the same chunk of memory. This can mean the tensor is no longer \"contiguous\" in memory.\n",
        "\n",
        "2. **The `.view()` operation**: When you use `.view()`, it returns a new tensor with the desired shape. However, it expects the data to be contiguous in memory. If it's not, you'll get a runtime error.\n",
        "\n",
        "3. **Using `.contiguous()`**: When you call `.contiguous()`, it physically rearranges the tensor data in memory to make it contiguous. After calling `.contiguous()`, you can safely use `.view()` to reshape the tensor without any errors.\n",
        "\n",
        "In many cases, especially when the tensor is already contiguous, calling `.contiguous()` is a no-op (i.e., it doesn't do anything). However, after operations that might change the memory layout (like `.transpose()`), it's a good practice to ensure contiguity before reshaping with `.view()`.\n",
        "\n",
        "In the given code:\n",
        "\n",
        "```python\n",
        "x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_size)\n",
        "```\n",
        "\n",
        "The tensor `x` is transposed first. The `.contiguous()` ensures that the memory layout is contiguous, and then the tensor is reshaped with `.view()`. This ensures that the code won't break due to non-contiguous memory issues."
      ],
      "metadata": {
        "id": "i0y8skuco5Ro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `nn.Dropout`\n",
        "\n",
        "Dropout is a regularization technique for reducing overfitting in neural networks. It works by randomly setting a fraction of input units to 0 at each update during training time, which helps to prevent overfitting. This fraction is the dropout rate.\n",
        "\n",
        "### How Does `nn.Dropout` Work?\n",
        "\n",
        "In PyTorch, `nn.Dropout` works in the following manner:\n",
        "\n",
        "1. During training:\n",
        "    - A random set of activations is set to zero.\n",
        "    - The remaining activations are scaled up by \\( \\frac{1}{1 - \\text{dropout rate}} \\) to keep the sum of all activations unchanged.\n",
        "    \n",
        "2. During evaluation/testing:\n",
        "    - No activations are set to zero.\n",
        "    - No scaling is applied. The module simply returns the input as is.\n",
        "\n",
        "### Purpose of Multiple Dropout Mechanisms:\n",
        "\n",
        "Every class has its own dropout mechanism. This approach offers several benefits:\n",
        "\n",
        "1. **Flexibility**: Different parts of the network might require different dropout rates. For instance, you might want a higher dropout rate in the feed-forward layers and a lower dropout rate in the attention mechanisms.\n",
        "  \n",
        "2. **Regularization at Multiple Levels**: By applying dropout at multiple stages or levels in the model, you're introducing regularization throughout the model. This can help in preventing overfitting at different stages of the network.\n",
        "\n",
        "3. **Diverse Representations**: Dropout can encourage the model to learn more robust and diverse internal representations. By randomly dropping out certain activations, the network is less likely to rely heavily on any single neuron and is encouraged to learn redundant representations.\n",
        "\n",
        "4. **Avoid Co-adaptation**: Co-adaptation is when two or more neurons detect the same feature. Dropout helps in avoiding this by ensuring that no set of neurons is consistently used together.\n",
        "\n",
        "### Desired Effect:\n",
        "\n",
        "The idea behind using dropout in different parts of the Transformer is to ensure that the model doesn't overfit and can generalize well to unseen data. For instance:\n",
        "\n",
        "1. **Dropout in Attention Mechanisms**: Helps in preventing overfitting in the self-attention calculations.\n",
        "2. **Dropout in Feed-Forward Layers**: Helps in preventing overfitting in the feed-forward neural networks inside each transformer block.\n",
        "3. **Dropout in Positional Encodings**: Helps to ensure the model doesn't over-rely on positional information.\n",
        "\n",
        "Overall, the combination of these individual dropout mechanisms aims to create a model that is robust and can generalize well across various sequences and tasks."
      ],
      "metadata": {
        "id": "iqfrXj2csW8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## the `forward` method of the `EncoderBlock`\n",
        "\n",
        "```python\n",
        "def forward(self, src: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
        "    src = self.residual_connections[0](src, lambda src: self.self_attention_block(src, src, src, src_mask))\n",
        "    src = self.residual_connections[1](src, self.feed_forward_block)\n",
        "    return src\n",
        "```\n",
        "\n",
        "1. **Input Parameters**:\n",
        "    - `src`: This is the source sequence tensor with a shape typically of `(batch_size, seq_len, embed_size)`.\n",
        "    - `src_mask`: This is a mask tensor used to ensure that the self-attention mechanism doesn't attend to padding tokens. It has a shape of `(batch_size, 1, 1, seq_len)`.\n",
        "\n",
        "2. **First Residual Connection**:\n",
        "   ```python\n",
        "   src = self.residual_connections[0](src, lambda src: self.self_attention_block(src, src, src, src_mask))\n",
        "   ```\n",
        "   - This line applies the first `ResidualConnection`.\n",
        "   - Inside the residual connection, the input `src` goes through layer normalization first.\n",
        "   - Then, the normalized `src` is passed to the function provided as the second argument. This function is a lambda function which applies the multi-head self-attention block to the input. In the context of self-attention, the query, key, and value are all the same and are equal to the normalized `src`.\n",
        "   - The output of the self-attention block is added to the original (unnormalized) `src`, achieving the residual connection.\n",
        "   - This summed output becomes the new value of `src`.\n",
        "\n",
        "3. **Second Residual Connection**:\n",
        "   ```python\n",
        "   src = self.residual_connections[1](src, self.feed_forward_block)\n",
        "   ```\n",
        "   - This line applies the second `ResidualConnection`.\n",
        "   - Just like before, the input `src` goes through layer normalization first.\n",
        "   - After normalization, the `feed_forward_block` is applied to the normalized `src`.\n",
        "   - The output of the feed-forward block is added to the original (unnormalized) `src`, achieving the second residual connection.\n",
        "   - This summed output becomes the new value of `src`.\n",
        "\n",
        "4. **Return Value**:\n",
        "   - Finally, the modified `src` is returned. This will be the input to the next encoder block (if there is one) or will be the final output of the encoder if it's the last block.\n",
        "\n",
        "In essence, the forward pass of the `EncoderBlock` can be thought of as two main steps: a multi-head self-attention mechanism followed by a pointwise feed-forward network. Each of these steps is wrapped in a residual connection, which means the output of each step is added to its input."
      ],
      "metadata": {
        "id": "erX_R7I11OUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `encode` and `decode` methods in the `Transformer` class\n",
        "\n",
        "The `encode` and `decode` methods in the `Transformer` class are particularly useful during the inference phase. Let's discuss their roles and utility:\n",
        "\n",
        "1. **Inference Phase in Sequence-to-Sequence Models**:\n",
        "    - In sequence-to-sequence tasks like machine translation, the typical inference process involves encoding the source sequence and then decoding it into the target sequence, often one token at a time.\n",
        "    \n",
        "2. **`encode` method**:\n",
        "    - The `encode` method takes the source sequence and its corresponding mask as inputs. It returns the encoder's output.\n",
        "    - This encoder's output is then used as context for the decoder. The important point to note is that, once encoded, this context does not change during the decoding of the entire target sequence. It's a static representation of the source sequence for the decoder.\n",
        "\n",
        "3. **`decode` method**:\n",
        "    - The `decode` method is used to generate the target sequence.\n",
        "    - During inference, especially when generating sequences token-by-token (like in beam search or greedy decoding), the `decode` method can be called iteratively, each time with an extended target sequence (with the newly predicted token appended) until a termination condition is met (e.g., a special end-of-sequence token is produced or a maximum sequence length is reached).\n",
        "\n",
        "4. **Benefits of Separate Methods**:\n",
        "    - **Modularity**: Having separate methods for encoding and decoding allows for a more modular approach. For example, in tasks like text summarization, after encoding an article, you might want to decode multiple summaries (e.g., a one-sentence summary, a paragraph-length summary).\n",
        "    - **Efficiency**: Especially during beam search decoding, where multiple candidate sequences are explored, the source sequence remains the same. By encoding it just once and reusing the context, you save computational resources.\n",
        "    - **Flexibility**: In some advanced applications, you might want to manipulate the encoded context before decoding (e.g., in style transfer tasks). Separate methods make such interventions more straightforward.\n",
        "\n",
        "5. **Training vs. Inference**:\n",
        "    - During training, the entire target sequence is often available, and the model is trained to predict each token based on the previous ground-truth tokens (teacher forcing). So, you might not call `encode` and `decode` separately.\n",
        "    - During inference, however, the model predicts the target sequence iteratively, using its own previous predictions as input for subsequent tokens. This is where having separate `encode` and `decode` methods becomes particularly useful.\n",
        "\n",
        "In conclusion, while the `encode` and `decode` methods can be useful during both training and inference, their utility becomes especially apparent during the inference phase in sequence-to-sequence tasks."
      ],
      "metadata": {
        "id": "m-8u8BVYATzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torch.softmax` and `torch.log_softmax`\n",
        "\n",
        "Both `torch.softmax` and `torch.log_softmax` have their uses, and the best one to use often depends on the context.\n",
        "\n",
        "1. **Numerical Stability**: When training models, especially deep neural networks, the values can sometimes become very large or very small. When you take the logarithm of a very small number, you can end up with `-inf`, and when you exponentiate a very large number (as done in softmax), you can end up with `inf`. Combining the softmax and log operations into a single `log_softmax` function can help ensure that the computation is stable and avoids these extreme values.\n",
        "\n",
        "2. **Loss Function Compatibility**: If you're using the negative log likelihood loss (`nn.NLLLoss` in PyTorch), then the input to this loss function should be the log probabilities, which is what `log_softmax` provides. On the other hand, if you were using `nn.CrossEntropyLoss`, this loss function combines `log_softmax` and `nn.NLLLoss` into a single step, so you'd directly provide the raw outputs (logits) of your model.\n",
        "\n",
        "3. **Interpretability**: If you want the actual probabilities (e.g., for interpretation, analysis, or to provide a probability distribution to an end user), then you'd use `torch.softmax`.\n",
        "\n",
        "In many transformer-based models, especially when using the negative log likelihood as the loss function, `log_softmax` is commonly used for the reasons of numerical stability and loss compatibility.\n",
        "\n",
        "Given that the original code uses `log_softmax` for \"numerical stability\" and you're likely training the model using negative log likelihood, I would recommend sticking with `log_softmax`. When you're computing the loss during training, make sure to pair it with `nn.NLLLoss`. If at any point during inference or post-training analysis you need the actual probabilities, you can always apply the exponential function (`torch.exp`) to convert the log probabilities back into regular probabilities."
      ],
      "metadata": {
        "id": "KdfML3n38nwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuggingFace's `datasets`\n",
        "\n",
        "### 1. Loading the dataset using HuggingFace's `datasets` library:\n",
        "\n",
        "When you run:\n",
        "\n",
        "```python\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"opus_books\", \"en-fr\")\n",
        "```\n",
        "\n",
        "The `dataset` variable becomes an instance of the `DatasetDict` class. A `DatasetDict` is essentially a dictionary-like structure with keys corresponding to different splits of the dataset, like 'train', 'test', etc.\n",
        "\n",
        "### 2. Understanding the dataset type:\n",
        "\n",
        "You can check the type of `dataset` as follows:\n",
        "\n",
        "```python\n",
        "print(type(dataset))\n",
        "```\n",
        "\n",
        "This will likely print: `<class 'datasets.dataset_dict.DatasetDict'>`\n",
        "\n",
        "### 3. Accessing the individual splits:\n",
        "\n",
        "To access the training split, you'd use:\n",
        "\n",
        "```python\n",
        "train_dataset = dataset['train']\n",
        "```\n",
        "\n",
        "Similarly, if the dataset contains other splits like 'test' or 'validation', you can access them with `dataset['test']` and `dataset['validation']` respectively.\n",
        "\n",
        "### 4. Structure of the splits:\n",
        "\n",
        "Each split (like `train_dataset` in the above example) is an instance of the `Dataset` class. This acts like a list of dictionaries. Each element of the list corresponds to a data example, and the dictionary keys correspond to the different columns or features of the dataset.\n",
        "\n",
        "For instance, to access the first example's English text, you'd use:\n",
        "\n",
        "```python\n",
        "english_text = train_dataset[0]['translation']['en']\n",
        "```\n",
        "\n",
        "### 5. Preparing the data:\n",
        "\n",
        "For training models, especially with PyTorch, you'll typically want your data in the form of tensors. However, the HuggingFace `datasets` library usually loads data in other formats (like lists of dictionaries).\n",
        "\n",
        "To convert this data into a more suitable format for training, you'll often do the following:\n",
        "\n",
        "- Tokenize the text data to convert it into numerical format.\n",
        "- Convert the tokenized data into tensors.\n",
        "- Use PyTorch's `DataLoader` to batch and shuffle the data.\n",
        "\n",
        "HuggingFace's `transformers` library provides tokenizers that can help with the tokenization step, and they can directly output PyTorch tensors.\n",
        "\n",
        "### Example:\n",
        "\n",
        "Here's a simplified process using a tokenizer from HuggingFace's `transformers`:\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('model_name_or_path')  # Replace 'model_name_or_path' with a suitable model/tokenizer name\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_data = dataset.map(lambda examples: tokenizer(examples['translation']['en'], examples['translation']['fr'], truncation=True, padding='max_length', max_length=100), batched=True)\n",
        "\n",
        "# Set the format to PyTorch tensors\n",
        "tokenized_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "```\n",
        "\n",
        "Now, `tokenized_data` is ready to be used with PyTorch's `DataLoader` for training.\n"
      ],
      "metadata": {
        "id": "853kBZoISu9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torch.utils.data.DataLoader` and `torch.utils.data.TensorDataset`.\n",
        "\n",
        "### 1. `torch.utils.data.TensorDataset`\n",
        "\n",
        "**Purpose**:\n",
        "- `TensorDataset` wraps tensors into a dataset. It allows us to have a multi-input dataset without having to manually handle each item's indexing. For example, if we want to train a model using source sentences and target sentences as inputs, `TensorDataset` can handle this pairing.\n",
        "\n",
        "**How it works**:\n",
        "- Given tensors `a` of size `(n, m)` and `b` of size `(n, p)`, it creates a dataset of `n` samples where each sample is a tuple containing a slice of `a` and `b`.\n",
        "\n",
        "**Usage**:\n",
        "```python\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "# Example tensors\n",
        "tensor_a = torch.tensor([[1,2], [3,4], [5,6]])\n",
        "tensor_b = torch.tensor([[10], [20], [30]])\n",
        "\n",
        "# Create a TensorDataset\n",
        "dataset = TensorDataset(tensor_a, tensor_b)\n",
        "\n",
        "# Accessing an item\n",
        "item = dataset[1]  # Returns (tensor([3,4]), tensor([20]))\n",
        "```\n",
        "\n",
        "### 2. `torch.utils.data.DataLoader`\n",
        "\n",
        "**Purpose**:\n",
        "- `DataLoader` helps with batching, shuffling, and loading the data in parallel. It provides an iterable over a dataset (like `TensorDataset`).\n",
        "- It is especially useful because it abstracts away the manual handling of batching and provides a simple interface to loop through data in batches.\n",
        "\n",
        "**Key Parameters**:\n",
        "- `dataset`: The dataset from which to load the data (could be a `TensorDataset`, among others).\n",
        "- `batch_size`: How many samples per batch to load.\n",
        "- `shuffle`: Whether to shuffle the dataset before each epoch.\n",
        "- `num_workers`: How many subprocesses to use for data loading. The default, 0, means that the data will be loaded in the main process (which can slow down training on GPUs).\n",
        "\n",
        "**Usage**:\n",
        "```python\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Using the previously defined TensorDataset 'dataset'\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Looping through the DataLoader\n",
        "for batch in dataloader:\n",
        "    batch_a, batch_b = batch\n",
        "    # Now, 'batch_a' and 'batch_b' are batches from 'tensor_a' and 'tensor_b'\n",
        "```\n",
        "\n",
        "In the context of our Transformer model, `DataLoader` will be used to create batches of source sentences, target sentences, and their corresponding masks. This makes it easier to train the model in mini-batches rather than using the entire dataset at once (which might not fit in memory)."
      ],
      "metadata": {
        "id": "0WiIm5RiapiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torch.utils.data.DataLoader`.\n",
        "\n",
        "### What is `torch.utils.data.DataLoader`?\n",
        "\n",
        "The `torch.utils.data.DataLoader` is a utility class provided by PyTorch that offers an efficient way to iterate over datasets. It provides functionality to automatically batch, shuffle, and load data in parallel using multiprocessing. It's designed to work seamlessly with instances of the `torch.utils.data.Dataset` class.\n",
        "\n",
        "### Main Features of `torch.utils.data.DataLoader`:\n",
        "\n",
        "1. **Batching**: It automatically groups multiple data points into batches, making it easier to process large datasets in chunks.\n",
        "2. **Shuffling**: For training datasets, you often want to access your data in a random order. DataLoader provides an easy way to shuffle data at the beginning of each epoch.\n",
        "3. **Parallel Loading with Multiprocessing**: `DataLoader` can use multiple subprocesses to load data in parallel, which can lead to significant speedups, especially when the data loading or preprocessing is CPU-intensive.\n",
        "4. **Custom Sampling**: It supports custom sampling strategies through the `Sampler` argument.\n",
        "5. **Automatic Collation**: It automatically collates individual data points into batches, but if your data points are more complex (e.g., different shapes, types), you can provide a custom `collate_fn` to specify how this should be done.\n",
        "6. **Lazy Loading**: Works well with datasets that support lazy loading, ensuring that only the data that's needed for the current batch gets loaded and processed.\n",
        "\n",
        "### Main Uses:\n",
        "\n",
        "1. **Efficient Iteration**: It gives you an iterator that yields batches of data, making it easier to loop through a dataset in a for-loop during training or evaluation.\n",
        "2. **Training Loop Simplification**: Instead of manually handling batching, shuffling, and data loading, you can rely on DataLoader to handle these details.\n",
        "3. **Performance Optimization**: By using multiple processes, DataLoader can significantly speed up data loading, which can be a bottleneck in training deep learning models, especially when the preprocessing is involved.\n",
        "\n",
        "### Step-by-Step Explanation:\n",
        "\n",
        "1. **Initialization**: When creating a `DataLoader`, you provide it with a `Dataset` and various other arguments that control its behavior (batch size, shuffling, number of workers, etc.).\n",
        "   \n",
        "2. **Batching**: One of the primary roles of the `DataLoader` is to take individual data points from the provided `Dataset` and group them into batches. This is controlled by the `batch_size` argument.\n",
        "   \n",
        "3. **Shuffling**: If the `shuffle` argument is set to `True`, the `DataLoader` will shuffle the data at the beginning of each epoch. This is crucial for training deep learning models to ensure they don't see data in the same order each time.\n",
        "   \n",
        "4. **Parallel Loading**: The `num_workers` argument controls how many subprocesses are used for data loading. If it's greater than 0, data loading will happen in parallel using the specified number of processes, improving loading times.\n",
        "   \n",
        "5. **Sampling**: DataLoader can be combined with various `Sampler` classes to define a custom strategy for drawing samples from the dataset. This is useful for imbalanced datasets or other scenarios where specific sampling techniques are beneficial.\n",
        "   \n",
        "6. **Collation**: Once individual data points are batched together, they need to be collated (merged) into a single tensor. DataLoader handles this automatically for simple cases but can be customized with the `collate_fn` argument for more complex data structures.\n",
        "   \n",
        "7. **Iteration**: Once initialized, the `DataLoader` acts as an iterator. During training or evaluation, you typically loop through the DataLoader to get batches of data.\n",
        "   \n",
        "8. **Lazy Loading**: When combined with a `Dataset` that supports lazy loading, the `DataLoader` ensures that only the necessary data is loaded and processed, conserving memory and computational resources.\n",
        "\n",
        "In essence, `torch.utils.data.DataLoader` is an essential tool in PyTorch that streamlines the data loading process, ensuring that the training loop is efficient, flexible, and easier to manage."
      ],
      "metadata": {
        "id": "6LVN-veY5ioK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torch.utils.data.Dataset`\n",
        "\n",
        "### What is `torch.utils.data.Dataset`?\n",
        "\n",
        "The `torch.utils.data.Dataset` is an abstract class provided by PyTorch that represents a dataset. It allows you to build custom datasets by overriding specific methods. This class is especially beneficial when you're working with large datasets that don't fit into memory or when you need to apply some custom transformations on-the-fly during data loading.\n",
        "\n",
        "### Main Features of `torch.utils.data.Dataset`:\n",
        "\n",
        "1. **Flexibility**: It's an abstract class, which means you have to provide implementations for some of its methods, allowing you to define exactly how your data is loaded and processed.\n",
        "2. **Integration with DataLoader**: It's designed to seamlessly integrate with `torch.utils.data.DataLoader`, which provides multi-process data loading, batching, shuffling, and other utilities.\n",
        "3. **Support for Map-style and Iterable-style datasets**: You can create datasets that are map-like (i.e., they implement the `__getitem__` and `__len__` methods) or iterable-like (i.e., they implement the `__iter__` method).\n",
        "\n",
        "### Main Uses:\n",
        "\n",
        "1. **Custom Data Loading**: It provides a consistent interface to load custom data. This is particularly useful when working with domain-specific datasets that aren't catered to by standard libraries.\n",
        "2. **On-the-fly Data Augmentation/Transformation**: By overriding the `__getitem__` method, you can apply transformations to data points as they're loaded, allowing for dynamic data augmentation.\n",
        "3. **Lazy Loading**: You don't have to load the entire dataset into memory. Instead, data points can be loaded and processed as needed, which is essential for large datasets.\n",
        "4. **Custom Sampling**: While not directly a feature of `Dataset`, in combination with `DataLoader` and custom `Sampler` classes, you can define custom sampling strategies for your data.\n",
        "\n",
        "### Step-by-Step Explanation:\n",
        "\n",
        "1. **Definition**: At its core, the `Dataset` class is a way of representing data. It's an abstract class, meaning you can't use it directly. Instead, you have to define a subclass and provide implementations for certain methods.\n",
        "   \n",
        "2. **Mandatory Methods**:\n",
        "   - For map-style datasets:\n",
        "     - `__getitem__(self, index)`: This returns a data point (a sample) given an index. This is where you'd typically apply any on-the-fly transformations or data augmentations.\n",
        "     - `__len__(self)`: This returns the number of data points in the dataset.\n",
        "   - For iterable-style datasets:\n",
        "     - `__iter__(self)`: This should yield samples from the dataset.\n",
        "   \n",
        "3. **Usage with DataLoader**: Once you've defined a `Dataset`, you can wrap it with a `DataLoader`. This gives you an iterator that yields batches of data, and it can handle things like shuffling and multi-process data loading for you.\n",
        "   \n",
        "4. **Custom Transformations**: One common use-case for custom datasets is applying transformations to the data. By integrating with libraries like `torchvision.transforms`, you can apply a series of transformations every time a data point is fetched.\n",
        "   \n",
        "5. **Integration with Other Tools**: Since the `Dataset` class provides a consistent interface for data loading, it can be integrated with other tools or libraries that expect data in a certain format. For instance, tools for visualization, augmentation, or even other machine learning frameworks can work with your data as long as it's represented as a `Dataset`.\n",
        "\n",
        "6. **Combination with Samplers**: For more advanced sampling techniques (e.g., stratified sampling, weighted sampling), you can use custom `Sampler` classes with the `DataLoader` to control the order in which data points are fetched.\n",
        "\n",
        "In summary, `torch.utils.data.Dataset` offers a flexible and standardized way to represent and load data in PyTorch, making it easier to work with various types and sizes of datasets in a consistent manner."
      ],
      "metadata": {
        "id": "UzPPRONu4540"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mask dimensions\n",
        "\n",
        "There seems to be a discrepancy in dimensions between the `mask_fr` and the `tgt_mask`.\n",
        "\n",
        "1. `mask_fr` is a 2D tensor of shape `(batch_size, sequence_length)`.\n",
        "2. `tgt_mask` is a 3D tensor of shape `(batch_size, sequence_length, sequence_length)`.\n",
        "\n",
        "The difference in dimensions is because:\n",
        "\n",
        "- `mask_fr` (and `mask_en`) is an **attention mask**, which is used to specify which tokens should be attended to (usually `True` for real tokens and `False` for padding tokens).\n",
        "- `tgt_mask` is a **combined mask** for the target sequences. It combines the attention mask (to ignore padding tokens) and the look-ahead mask (to ensure that a token in position `i` can't attend to future tokens in positions `j > i`).\n",
        "\n",
        "When you are using these masks in the Transformer model:\n",
        "\n",
        "- The source mask (`mask_en`) and target attention mask (`mask_fr`) are used in both the encoder and the decoder to mask out padding tokens.\n",
        "- The `tgt_mask` (look-ahead mask) is specifically used in the decoder's self-attention mechanism to prevent future tokens from being attended to.\n",
        "\n",
        "So, while it might seem unusual to have this difference in dimensions, it's actually expected given the use-case of each mask. However, it's essential to ensure that these masks are used correctly in the Transformer model."
      ],
      "metadata": {
        "id": "_iT-UUTKdOlf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## formula for the Transformer model's parameters\n",
        "\n",
        "Below is a breakdown formula for the Transformer model's parameters:\n",
        "\n",
        "1. **Embeddings**:\n",
        "    - Source embeddings: \\( \\text{SRC\\_VOCAB\\_SIZE} \\times \\text{EMBED\\_SIZE} \\)\n",
        "    - Target embeddings: \\( \\text{TGT\\_VOCAB\\_SIZE} \\times \\text{EMBED\\_SIZE} \\)\n",
        "\n",
        "2. **Positional Encodings**:\n",
        "    - They do not have trainable parameters.\n",
        "\n",
        "3. **Encoder**:\n",
        "    Each Encoder Block has:\n",
        "    - Self Attention:\n",
        "        - Query weights: \\( \\text{EMBED\\_SIZE} \\times \\text{EMBED\\_SIZE} \\)\n",
        "        - Key weights: \\( \\text{EMBED\\_SIZE} \\times \\text{EMBED\\_SIZE} \\)\n",
        "        - Value weights: \\( \\text{EMBED\\_SIZE} \\times \\text{EMBED\\_SIZE} \\)\n",
        "        - Output projection weights: \\( \\text{EMBED\\_SIZE} \\times \\text{EMBED\\_SIZE} \\)\n",
        "    - Feed Forward Network:\n",
        "        - Input layer: \\( \\text{EMBED\\_SIZE} \\times \\text{D\\_FF} \\)\n",
        "        - Output layer: \\( \\text{D\\_FF} \\times \\text{EMBED\\_SIZE} \\)\n",
        "    \n",
        "    Total for each Encoder Block: \\( 4 \\times \\text{EMBED\\_SIZE} \\times \\text{EMBED\\_SIZE} + 2 \\times \\text{EMBED\\_SIZE} \\times \\text{D\\_FF} \\)\n",
        "\n",
        "4. **Decoder**:\n",
        "    Each Decoder Block has:\n",
        "    - Self Attention (same as in Encoder)\n",
        "    - Cross Attention (same structure as Self Attention)\n",
        "    - Feed Forward Network (same as in Encoder)\n",
        "    \n",
        "    Total for each Decoder Block: \\( 8 \\times \\text{EMBED\\_SIZE} \\times \\text{EMBED\\_SIZE} + 2 \\times \\text{EMBED\\_SIZE} \\times \\text{D\\_FF} \\)\n",
        "\n",
        "5. **Output Layer**:\n",
        "    - \\( \\text{EMBED\\_SIZE} \\times \\text{TGT\\_VOCAB\\_SIZE} \\)\n",
        "\n",
        "To get the total number of parameters, you can multiply the parameters for each block by the number of blocks \\( N \\) and then sum up the parameters from the embeddings, all encoder blocks, all decoder blocks, and the output layer.\n",
        "\n",
        "You can use the above formulas in your Colab to compute the total number of parameters."
      ],
      "metadata": {
        "id": "715bOMv8GaV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The embedding size, d_model\n",
        "\n",
        "The embedding size, d_model, in the Transformer architecture significantly affects the model's capacity and, therefore, its performance. The choice of embedding size is a trade-off between computational cost (both in terms of memory and processing time) and model performance.\n",
        "\n",
        "When considering the embedding size, here are some points to consider:\n",
        "\n",
        "1. **Model Capacity**: A larger embedding size increases the model's capacity, allowing it to potentially capture more intricate patterns in the data. This could be beneficial for complex tasks or large datasets. However, a model with a larger capacity can also overfit more easily if not properly regularized or if trained on a small dataset.\n",
        "\n",
        "2. **Computational Cost**: As you've noticed, the embedding size significantly affects the total number of parameters in the model. This has implications for training time, memory usage, and deployment considerations.\n",
        "\n",
        "3. **Task Complexity**: For simpler tasks or smaller datasets, a large embedding size might be overkill and could lead to overfitting. For such tasks, reducing the embedding size (and potentially the number of layers) might be appropriate.\n",
        "\n",
        "4. **Empirical Results**: Often, the best way to determine the right embedding size is through empirical testing. By training models with different embedding sizes and evaluating their performance on a validation set, you can select the size that works best for your specific task and dataset.\n",
        "\n",
        "5. **Literature and Pre-trained Models**: The choice of 512 for the embedding size in the original \"Attention Is All You Need\" paper was based on their experiments and the scale of the tasks they were addressing (like machine translation on the WMT dataset). However, different tasks or datasets might benefit from different sizes. It's always a good idea to check recent literature or related works for your specific task to see if there are recommended embedding sizes.\n",
        "\n",
        "6. **Transfer Learning**: If you plan to leverage pre-trained models and fine-tune them on your task, you'd typically want to use the same embedding size as the pre-trained model to benefit from the learned representations.\n",
        "\n",
        "For a small project, if computational resources are limited, or if you're dealing with a less complex dataset, you might consider reducing the embedding size. Values like 256 or even 128 could be a starting point. However, the best approach would be to empirically test a few different sizes to see which one offers the best trade-off between performance and computational efficiency for your specific use case."
      ],
      "metadata": {
        "id": "kY2lBVxOLHFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `tqdm`\n",
        "\n",
        "`tqdm` is a Python library that provides a fast, extensible progress bar for loops and other computations. The name \"tqdm\" stands for the Arabic word \"taqaddum\" (تقدّم) which means \"progress.\"\n",
        "\n",
        "Here's what `tqdm` offers:\n",
        "\n",
        "1. **Instant Feedback**: It provides instant feedback by showing a progress bar that estimates how much time is left for the loop or computation to complete.\n",
        "  \n",
        "2. **Versatility**: It works on any iterable, including lists, dictionaries, and even files.\n",
        "  \n",
        "3. **Extensibility**: It can be used in a variety of environments, such as the command line, Jupyter notebooks, and graphical interfaces.\n",
        "  \n",
        "4. **Customizability**: The appearance and information displayed by the progress bar can be customized.\n",
        "\n",
        "5. **Additional Utilities**: Beyond the basic progress bar, `tqdm` also provides utilities for tracking download progress, parallel processing, and more.\n",
        "\n",
        "### Basic Usage:\n",
        "\n",
        "Here's a simple example of how to use `tqdm`:\n",
        "\n",
        "```python\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "for _ in tqdm(range(10)):\n",
        "    time.sleep(0.5)\n",
        "```\n",
        "\n",
        "When you run the above code, you'll see a progress bar in the console that updates in real-time, showing the number of iterations completed, the total number of iterations, the percentage completed, the time elapsed, and an estimate of the time remaining.\n",
        "\n",
        "In Jupyter notebooks, you can use `tqdm.notebook.tqdm` for a better-looking, notebook-friendly progress bar:\n",
        "\n",
        "```python\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "for _ in tqdm(range(10)):\n",
        "    time.sleep(0.5)\n",
        "```\n",
        "\n",
        "Overall, `tqdm` is an invaluable tool for long computations where you want to keep track of progress and estimated completion time."
      ],
      "metadata": {
        "id": "Ot5Cga4NKa9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `tensor.masked_fill_`\n",
        "\n",
        "The `masked_fill_` method is a very useful in-place operation provided by PyTorch for tensors. Let's break down its functionality:\n",
        "\n",
        "### `masked_fill_`\n",
        "\n",
        "**Purpose**:\n",
        "To update specific elements of a tensor based on a boolean mask.\n",
        "\n",
        "**Syntax**:\n",
        "```python\n",
        "tensor.masked_fill_(mask, value)\n",
        "```\n",
        "\n",
        "**Parameters**:\n",
        "\n",
        "- `mask` (BoolTensor): A boolean tensor with the same shape as the original tensor (`tensor`). The mask specifies which elements in the tensor should be updated. A `True` value in the mask means the corresponding element in the tensor will be updated, while a `False` value means it will be left unchanged.\n",
        "  \n",
        "- `value`: The scalar value used to update the tensor's elements specified by the mask.\n",
        "\n",
        "**In-place**:\n",
        "The underscore at the end of `masked_fill_` means it's an in-place operation. This means that the tensor is modified directly and nothing is returned. There's also a non-in-place version called `masked_fill` which returns a new tensor.\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "import torch\n",
        "\n",
        "x = torch.tensor([[1, 2], [3, 4]])\n",
        "mask = torch.tensor([[True, False], [False, True]])\n",
        "x.masked_fill_(mask, -1)\n",
        "print(x)\n",
        "```\n",
        "Output:\n",
        "```\n",
        "tensor([[-1,  2],\n",
        "        [ 3, -1]])\n",
        "```\n",
        "\n",
        "In this example, only the positions in `x` where `mask` has a `True` value are replaced with `-1`.\n",
        "\n",
        "### Usage in the Transformer Model\n",
        "\n",
        "In the context of the Transformer model, `masked_fill_` is particularly useful for applying masks to the attention scores. For instance, in attention mechanisms, certain positions (like padding tokens) should not be attended to. By setting the scores for these positions to negative infinity (`float('-inf')`), and then applying a softmax, these positions will have an attention weight of nearly zero. This ensures that the model doesn't pay attention to these positions.\n",
        "\n"
      ],
      "metadata": {
        "id": "2Op9nJ9qn31h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## masks\n",
        "\n",
        "In the context of Transformers, masks are used for two primary reasons:\n",
        "\n",
        "1. **Padding Masks**: To ensure the model doesn't consider padding tokens.\n",
        "2. **Causal or Look-ahead Masks**: To ensure that while predicting a token in the decoder, only the previous tokens are considered and not the future ones. This mask is used in the decoder side of transformers during training for sequence-to-sequence tasks, like language translation.\n",
        "\n",
        "Given the batch size as \\( B \\), sequence length as \\( S \\), and the number of attention heads as \\( H \\), here's the typical shape and structure of these masks:\n",
        "\n",
        "1. **Padding Masks**:\n",
        "   - Shape: \\( (B, 1, 1, S) \\) or \\( (B, S) \\)\n",
        "   - The mask has a 1 for real tokens and 0 for padding tokens. It's usually broadcasted across the attention heads and the sequence length when used.\n",
        "  \n",
        "2. **Causal or Look-ahead Masks**:\n",
        "   - Shape: \\( (B, 1, S, S) \\) or \\( (1, 1, S, S) \\) if shared across the batch.\n",
        "   - This is a lower triangular matrix where the entries below the diagonal are 1 (indicating allowed positions) and 0 above the diagonal (indicating masked positions).\n",
        "   \n",
        "In the context of multi-head attention in Transformers, the attention scores typically have a shape of \\( (B, H, S, S) \\). Masks should be broadcastable to this shape to be applied correctly.\n",
        "\n",
        "In some implementations, the padding mask might be incorporated with the look-ahead mask in the decoder to form a combined mask. This combined mask would then have the same shape as the look-ahead mask and would account for both padding tokens and future tokens."
      ],
      "metadata": {
        "id": "WFhmqZb5wamO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation Loop\n",
        "\n",
        "### Value of a Validation Loop:\n",
        "\n",
        "1. **Model Generalization**: By evaluating the model on a dataset that it hasn't seen during training (validation set), you can gauge how well the model is generalizing to new data.\n",
        "  \n",
        "2. **Avoid Overfitting**: Monitoring the performance on the validation set can help in identifying if the model starts to overfit to the training data. If the training loss continues to decrease but the validation loss starts increasing, it's a sign of overfitting.\n",
        "  \n",
        "3. **Model Selection**: If you're experimenting with various architectures, hyperparameters, or other training settings, the validation performance can guide which model or setup is best.\n",
        "  \n",
        "4. **Early Stopping**: By tracking validation loss, you can implement early stopping to halt training if the model's performance on the validation set doesn't improve for a certain number of epochs.\n",
        "  \n",
        "5. **Hyperparameter Tuning**: The validation set can be used to tune hyperparameters. By observing the performance on the validation set, you can adjust hyperparameters like learning rate, batch size, etc.\n",
        "\n",
        "### Implementing a Validation Loop:\n",
        "\n",
        "For a typical translation transformer, the validation loop will be similar to the training loop but without backpropagation and weight updates. Here's how you can implement it:\n",
        "\n",
        "1. Set the model to evaluation mode using `model.eval()`.\n",
        "2. Loop over the validation data.\n",
        "3. Forward pass the data through the model.\n",
        "4. Compute the loss (and other metrics if needed).\n",
        "5. Accumulate the loss for reporting.\n",
        "6. Set the model back to training mode using `model.train()` after validation.\n",
        "\n",
        "### Code for Validation Loop:\n",
        "\n",
        "```python\n",
        "# Assuming val_dataloader is the DataLoader for validation data\n",
        "\n",
        "def validate(model, val_dataloader, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            src = batch[\"encoder_input\"].to(device)\n",
        "            tgt = batch[\"decoder_input\"].to(device)\n",
        "            src_mask = batch[\"encoder_mask\"].to(device)\n",
        "            tgt_mask = batch[\"decoder_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            logits = model(src, tgt, src_mask, tgt_mask)\n",
        "            loss = loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "    model.train()\n",
        "    return avg_val_loss\n",
        "\n",
        "# Inside your main training loop, after each epoch:\n",
        "val_loss = validate(model, val_dataloader, loss_fn, device)\n",
        "print(f\"Validation Loss after Epoch {epoch + 1}/{EPOCHS}: {val_loss:.4f}\")\n",
        "```\n",
        "\n",
        "Make sure to integrate this code into your existing training loop, and remember to define `val_dataloader` which holds the validation data. This will give you an idea of how your model is performing on unseen data after each epoch, helping you make more informed decisions during training."
      ],
      "metadata": {
        "id": "zAkZUHn20Hhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Weights and Tokenizers\n",
        "\n",
        "1. **Saving Model Weights**: You can save the state dictionary (state_dict) of the model using PyTorch's `torch.save()` function. The state dictionary includes the model parameters (weights and biases) and is suitable for model checkpointing.\n",
        "  \n",
        "    ```python\n",
        "    torch.save(model.state_dict(), 'model_weights.pth')\n",
        "    ```\n",
        "\n",
        "2. **Saving Tokenizers**: For tokenizers like those from the HuggingFace's `transformers` library, you can save them using the `save_pretrained()` method:\n",
        "\n",
        "    ```python\n",
        "    tokenizer_en.save_pretrained('./tokenizer_en_directory/')\n",
        "    tokenizer_fr.save_pretrained('./tokenizer_fr_directory/')\n",
        "    ```\n",
        "\n",
        "**Saving Directly to Google Drive:**\n",
        "\n",
        "To save files directly to Google Drive from a Colab notebook:\n",
        "\n",
        "1. Mount your Google Drive to the Colab environment.\n",
        "2. Define the path in your Google Drive where you want to save the weights/tokenizers.\n",
        "3. Use the appropriate save functions.\n",
        "\n",
        "Here's a strategy to implement this:\n",
        "\n",
        "1. **Mount Google Drive**:\n",
        "\n",
        "    ```python\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    ```\n",
        "\n",
        "    This will prompt you to authorize Colab to access your Google Drive. Once authorized, you can access your Drive as if it's a local file system.\n",
        "\n",
        "2. **Define Paths**:\n",
        "\n",
        "    ```python\n",
        "    # Define paths\n",
        "    model_save_path = '/content/drive/MyDrive/Colab Notebooks/model_weights.pth'\n",
        "    tokenizer_en_save_path = '/content/drive/MyDrive/Colab Notebooks/tokenizer_en_directory'\n",
        "    tokenizer_fr_save_path = '/content/drive/MyDrive/Colab Notebooks/tokenizer_fr_directory'\n",
        "    ```\n",
        "\n",
        "    Make sure the directories (`Colab Notebooks`, `tokenizer_en_directory`, `tokenizer_fr_directory`) exist or create them before saving.\n",
        "\n",
        "3. **Save Model and Tokenizers**:\n",
        "\n",
        "    ```python\n",
        "    # Save model weights\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "    # Save tokenizers\n",
        "    tokenizer_en.save_pretrained(tokenizer_en_save_path)\n",
        "    tokenizer_fr.save_pretrained(tokenizer_fr_save_path)\n",
        "    ```\n",
        "\n",
        "With this strategy, you can save your model weights and tokenizers directly to your Google Drive, making it easier to access them later or share with others."
      ],
      "metadata": {
        "id": "KloX0ZAv7MHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference explanation\n",
        "\n",
        "Let's break down the `translate_sentence` function step by step, using the example sentence \"A new chapter\".\n",
        "\n",
        "### 1. Tokenization:\n",
        "```python\n",
        "encoding = tokenizer_en.encode(sentence, add_special_tokens=True)\n",
        "tokens = encoding.ids\n",
        "```\n",
        "The input sentence \"A new chapter\" is tokenized using the English tokenizer, resulting in a list of token IDs. For the sake of illustration, let's assume the tokens are represented as `[5, 8, 12]`.\n",
        "\n",
        "### 2. Truncate and Add [EOS]:\n",
        "```python\n",
        "if len(tokens) > max_seq_len:\n",
        "    tokens = tokens[:max_seq_len-1]\n",
        "tokens = tokens + [tokenizer_en.token_to_id(\"[EOS]\")]\n",
        "```\n",
        "If the tokenized sequence exceeds the maximum sequence length, it's truncated to make space for the [EOS] token. Then, the [EOS] token is appended to the end. The tokens might look something like: `[5, 8, 12, 3]`, where `3` represents the ID for the [EOS] token.\n",
        "\n",
        "### 3. Convert to Tensor and Move to Device:\n",
        "```python\n",
        "tokens_tensor = torch.tensor(tokens).unsqueeze(0).to(device)\n",
        "```\n",
        "The token list is converted into a PyTorch tensor and is reshaped to have an extra dimension (for batch processing). It's then moved to the device (typically a GPU).\n",
        "\n",
        "### 4. Create a Source Mask:\n",
        "```python\n",
        "src_mask = (tokens_tensor != tokenizer_en.token_to_id(\"[PAD]\")).unsqueeze(-2).to(device)\n",
        "```\n",
        "This mask is used to inform the model which tokens are actual words versus which are padding. It ensures that the model doesn't pay attention to padding tokens.\n",
        "\n",
        "### 5. Initialize the Decoder Input:\n",
        "```python\n",
        "decoder_input = torch.tensor([tokenizer_fr.token_to_id(\"[SOS]\")]).unsqueeze(0).to(device)\n",
        "```\n",
        "Translation starts with the [SOS] token. This initializes the decoding process.\n",
        "\n",
        "### 6. Decoding Loop (Greedy Decoding):\n",
        "```python\n",
        "for _ in range(max_seq_len):\n",
        "    with torch.no_grad():\n",
        "        logits = model(tokens_tensor, decoder_input, src_mask, None)\n",
        "    next_token = logits.argmax(2)[:, -1].unsqueeze(1)\n",
        "    output_tokens.append(next_token.item())\n",
        "    decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
        "    if next_token.item() == tokenizer_fr.token_to_id(\"[EOS]\"):\n",
        "        break\n",
        "```\n",
        "\n",
        "- The loop runs for a maximum number of times equal to `max_seq_len`.\n",
        "- At each step, the model predicts the next word/token in the sequence.\n",
        "- The `logits` represent the raw prediction scores for each token in the vocabulary.\n",
        "- The line `next_token = logits.argmax(2)[:, -1].unsqueeze(1)` gets the token with the highest score from `logits`. This is where **greedy decoding** comes into play: at each step, we're greedily selecting the word with the highest probability as our next word in the sequence.\n",
        "- The selected token is then appended to `decoder_input`, and the loop continues until either the maximum sequence length is reached or an [EOS] token is produced.\n",
        "\n",
        "### 7. Convert Token IDs back to Words:\n",
        "```python\n",
        "decoded = tokenizer_fr.decode(output_tokens, skip_special_tokens=True)\n",
        "```\n",
        "The list of output token IDs is converted back into a human-readable string using the French tokenizer.\n",
        "\n",
        "---\n",
        "\n",
        "**Greedy Decoding**:\n",
        "Greedy decoding is a simple and intuitive method for generating sequences. At each step in the sequence generation, it picks the word/token with the highest probability (from the model's perspective) and moves on to the next step. While it's computationally efficient, it doesn't always produce the best possible sequences because it doesn't consider future steps. It's \"greedy\" because it makes the locally optimal choice at each step. In some cases, particularly with more complex models or tasks, a locally optimal choice might not lead to a globally optimal sequence."
      ],
      "metadata": {
        "id": "neVrb1rUOY18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `ReduceLROnPlateau`\n",
        "\n",
        "### What is `ReduceLROnPlateau`?\n",
        "\n",
        "`ReduceLROnPlateau` is a learning rate scheduler provided by PyTorch. Its primary job is to reduce the learning rate when a certain metric (usually validation loss) stops improving. Unlike other learning rate schedulers that adjust the learning rate at specific intervals or after a fixed number of epochs, `ReduceLROnPlateau` monitors a specific metric and adjusts the learning rate based on the performance of that metric.\n",
        "\n",
        "### How does it work?\n",
        "\n",
        "1. **Monitor a Metric**: At the end of each epoch, the scheduler checks the provided metric (usually the validation loss).\n",
        "2. **Check for \"Improvement\"**: It determines if the metric has improved beyond a certain threshold. \"Improvement\" is defined by the `mode` argument:\n",
        "   - If `mode` is set to `'min'`, then an improvement is defined as a decrease in the metric.\n",
        "   - If `mode` is set to `'max'`, then an improvement is defined as an increase in the metric.\n",
        "3. **Adjust Learning Rate on Plateau**: If the metric hasn’t improved for a specified number of epochs (`patience` parameter), the learning rate is reduced by multiplying it with a factor (usually less than 1, e.g., 0.1).\n",
        "\n",
        "### What value do we get from using it?\n",
        "\n",
        "1. **Avoid Overfitting**: By reducing the learning rate when the validation performance plateaus, the model makes smaller updates, which can prevent it from overfitting to the training data.\n",
        "2. **Stable Convergence**: A smaller learning rate in the later stages of training can help the model to converge more stably and avoid oscillations.\n",
        "3. **Automatic Adjustment**: Instead of manually tweaking the learning rate or having a predetermined schedule, `ReduceLROnPlateau` automatically adjusts the rate based on the model's performance.\n",
        "4. **Potential to Reach Better Minima**: By allowing the model to train with a smaller learning rate upon plateaus, it might navigate the loss landscape more finely and potentially reach a better (lower) local minimum.\n",
        "\n",
        "In summary, `ReduceLROnPlateau` provides an adaptive way to adjust the learning rate based on the actual performance of the model, potentially leading to better generalization and more stable training dynamics."
      ],
      "metadata": {
        "id": "R8dvr678W9xN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is BLEU?\n",
        "\n",
        "BLEU (Bilingual Evaluation Understudy) is a metric used to measure the quality of machine-generated translations, primarily in the context of machine translation but also in tasks like image captioning. It evaluates the generated translations (candidates) against one or more human references.\n",
        "\n",
        "### How does BLEU work?\n",
        "\n",
        "BLEU considers the precision of n-grams in the generated translation with respect to the reference translation(s). Here's a step-by-step breakdown:\n",
        "\n",
        "1. **N-gram Precision**: For each n-gram size (e.g., 1-gram, 2-gram, 3-gram, 4-gram), calculate the precision. This is the ratio of the number of n-grams in the candidate translation that appear in the reference translation to the total number of n-grams in the candidate.\n",
        "\n",
        "2. **Clipping**: If an n-gram appears in the candidate more times than in the reference, its count is clipped to the maximum number of times it appears in the reference.\n",
        "\n",
        "3. **Weighted Average**: The n-gram precisions are then combined using a weighted geometric mean. The weights are typically set to ensure that each n-gram precision has an equal contribution to the overall score.\n",
        "\n",
        "4. **Brevity Penalty**: If the length of the candidate translation is shorter than the length of the reference, a brevity penalty is applied to penalize shorter translations. This ensures that a system isn't rewarded for simply generating very short translations that naturally have higher precision.\n",
        "\n",
        "   \\$$\n",
        "   \\text{BP} =\n",
        "   \\begin{cases}\n",
        "   1 & \\text{if } c > r \\\\\n",
        "   \\exp(1 - \\frac{r}{c}) & \\text{if } c \\leq r\n",
        "   \\end{cases}\n",
        "   $$\n",
        "   \n",
        "   Where \\( c \\) is the length of the candidate translation and \\( r \\) is the effective reference length.\n",
        "\n",
        "5. **Overall BLEU Score**: The final BLEU score is computed as:\n",
        "\n",
        "   $$\n",
        "   \\text{BLEU} = \\text{BP} \\times \\exp\\left( \\sum_{n=1}^{N} w_n \\log p_n \\right)\n",
        "   $$\n",
        "\n",
        "   Where \\( p_n \\) is the n-gram precision, \\( w_n \\) are the weights (usually set to \\($ \\frac{1}{N}$ \\) for equal weights), and \\( N \\) is the maximum order of n-grams considered.\n",
        "\n",
        "### Why is BLEU Useful?\n",
        "\n",
        "1. **Automatic Evaluation**: BLEU provides an automatic and quick way to assess translation quality without the need for human evaluators.\n",
        "2. **Correlation with Human Judgment**: BLEU scores have been shown to correlate well with human evaluations, making it a useful benchmark for translation tasks.\n",
        "3. **Consistency**: It gives consistent scores across different evaluations, enabling reliable comparisons between different models or approaches.\n",
        "\n",
        "### Limitations:\n",
        "\n",
        "1. **Doesn't Capture Semantics**: BLEU mainly focuses on the surface-level word matching and doesn't capture the semantics or meaning of translations.\n",
        "2. **Variability with Multiple References**: Having multiple reference translations can lead to variability in BLEU scores.\n",
        "3. **Not Always Perfect**: A high BLEU score doesn't always guarantee a perfect translation, and a low BLEU score doesn't necessarily mean a poor translation.\n",
        "\n",
        "In summary, while BLEU is a widely used metric in machine translation and provides valuable insights into translation quality, it should be used alongside other evaluation metrics and human judgment for a comprehensive evaluation."
      ],
      "metadata": {
        "id": "FdrqQmnuX-OO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLEU Score: The Simple Explanation\n",
        "\n",
        "Imagine you're teaching a robot to make sentences. After teaching it, you test the robot by asking it to describe a picture. The robot says, \"The cat sat on the mat.\"\n",
        "\n",
        "Now, you ask four of your friends to describe the same picture. They say:\n",
        "1. \"The cat is on the mat.\"\n",
        "2. \"On the mat is a cat.\"\n",
        "3. \"A cat is sitting on the mat.\"\n",
        "4. \"The mat has a cat sitting on it.\"\n",
        "\n",
        "You now want to see how well the robot did. So, you compare the robot's sentence to your friends' sentences. You notice:\n",
        "- All sentences talk about a \"cat\" and a \"mat\".\n",
        "- Three sentences mention the cat \"on\" the mat.\n",
        "- One of your friends also used the word \"sat\" like the robot.\n",
        "\n",
        "The BLEU score helps you do this comparison mathematically. It checks:\n",
        "1. How many words (or groups of words) in the robot's sentence match any of your friends' sentences.\n",
        "2. If the robot's sentence is too short or too long.\n",
        "\n",
        "In the end, the BLEU score gives a number between 0 and 1. Closer to 1 means the robot did great; closer to 0 means not so great. If the robot's sentence matched words and ideas in your friends' sentences often, the score will be higher!\n",
        "\n",
        "In short, the BLEU score is like a game score that tells you how close the robot's sentence is to human-made sentences."
      ],
      "metadata": {
        "id": "EXe_3aXGYJV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing the BLEU score\n",
        "\n",
        "Implementing the BLEU score requires comparing your model's translated outputs to reference translations. While BLEU is typically used for evaluation rather than during training, you can use it in both scenarios:\n",
        "\n",
        "1. **During Evaluation (Preferred)**: After each epoch, or at the end of all epochs, use the trained model to translate the validation (or test) dataset sentences. Then, compare these translations with the actual reference translations using BLEU. This gives you a measure of the model's performance on unseen data.\n",
        "  \n",
        "2. **During Training (Less Common)**: This is less conventional because computing BLEU can be time-consuming, and optimizing directly for BLEU during training is not always straightforward. However, you can compute and track the BLEU score on a subset of the training data or validation data during training, just to monitor how it evolves.\n",
        "\n",
        "Here's how you can implement BLEU score evaluation using the `nltk` library:\n",
        "\n",
        "```python\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def calculate_bleu(data_loader, model, tokenizer_en, tokenizer_fr, device):\n",
        "    model.eval()\n",
        "    \n",
        "    references = []  # Actual target sentences\n",
        "    hypotheses = []  # Model's translations\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            # Extract source and target sentences from the batch\n",
        "            src = batch[\"encoder_input\"].to(device)\n",
        "            tgt = batch[\"label\"].to(device)\n",
        "\n",
        "            # Translate the source sentences\n",
        "            translated = [translate_sentence(sentence, model, tokenizer_en, tokenizer_fr, MAX_SEQ_LEN, device)\n",
        "                          for sentence in src]\n",
        "\n",
        "            hypotheses.extend(translated)\n",
        "            references.extend(tgt)\n",
        "\n",
        "    # Compute BLEU score\n",
        "    bleu_score = corpus_bleu(references, hypotheses)\n",
        "    \n",
        "    return bleu_score\n",
        "```\n",
        "\n",
        "In your training loop, after each epoch (or every few epochs), you can call this function:\n",
        "\n",
        "```python\n",
        "bleu = calculate_bleu(val_dataloader, model, tokenizer_en, tokenizer_fr, device)\n",
        "print(f\"Bleu Score = {bleu*100:.2f}\")\n",
        "```\n",
        "\n",
        "Remember:\n",
        "- The BLEU score can be a good indicator of the general quality of translations, but it's not the only metric you should rely on.\n",
        "- BLEU looks at the presence of specific words and sequences of words in the translation. It does not always capture fluency or meaning accurately.\n",
        "- Especially when training on smaller datasets or for fewer epochs, you might find that the BLEU score can be quite volatile. It's more stable and meaningful with larger, diverse datasets and more extensive training."
      ],
      "metadata": {
        "id": "edZvWoj4ZB9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing model checkpoints based on BLEU\n",
        "\n",
        "To implement model checkpoints based on BLEU, you'd want to save the model whenever you achieve a higher BLEU score than any previous epochs. This ensures that you're always retaining the model version that has performed the best in terms of translation quality.\n",
        "\n",
        "Here's a step-by-step guide on how to implement this:\n",
        "\n",
        "1. **Initialize a variable to keep track of the best BLEU score**:\n",
        "   ```python\n",
        "   best_bleu = 0.0\n",
        "   ```\n",
        "\n",
        "2. **After each epoch, calculate the BLEU score** on the validation set:\n",
        "   ```python\n",
        "   bleu = calculate_bleu(val_dataloader, model, tokenizer_en, tokenizer_fr, device)\n",
        "   print(f\"Epoch {epoch + 1}/{EPOCHS} | BLEU Score: {bleu*100:.2f}\")\n",
        "   ```\n",
        "\n",
        "3. **Compare the current BLEU score to the best BLEU score**. If the current BLEU score is better, update the best BLEU score and save the model:\n",
        "   ```python\n",
        "   if bleu > best_bleu:\n",
        "       best_bleu = bleu\n",
        "       torch.save(model.state_dict(), f'{model_save_path}/best_bleu_model_weights.pth')\n",
        "       print(f\"New best BLEU! Model saved to {model_save_path}/best_bleu_model_weights.pth\")\n",
        "   ```\n",
        "\n",
        "Here's how this can be integrated into your existing training loop:\n",
        "\n",
        "```python\n",
        "best_val_loss = float('inf')\n",
        "best_bleu = 0.0\n",
        "early_stop_counter = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # ... [Training code]\n",
        "\n",
        "    # Validation Loss\n",
        "    val_loss = validate(model, val_dataloader, loss_fn, device)\n",
        "    print(f\"Validation Loss after Epoch {epoch + 1}/{EPOCHS}: {val_loss:.4f}\")\n",
        "    \n",
        "    # Calculate BLEU score\n",
        "    bleu = calculate_bleu(val_dataloader, model, tokenizer_en, tokenizer_fr, device)\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS} | BLEU Score: {bleu*100:.2f}\")\n",
        "\n",
        "    # Checkpoint model based on BLEU score\n",
        "    if bleu > best_bleu:\n",
        "        best_bleu = bleu\n",
        "        torch.save(model.state_dict(), f'{model_save_path}/best_bleu_model_weights.pth')\n",
        "        print(f\"New best BLEU! Model saved to {model_save_path}/best_bleu_model_weights.pth\")\n",
        "\n",
        "    # Early Stopping based on Validation Loss (you can also include BLEU in this logic if you prefer)\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        early_stop_counter = 0\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        if early_stop_counter >= patience:\n",
        "            print(\"Early stopping due to no improvement in validation loss!\")\n",
        "            break\n",
        "```\n",
        "\n",
        "Note: This approach saves two models: one based on the best validation loss and another based on the best BLEU score. It's entirely possible that these two models are different, especially if the loss does not perfectly correlate with translation quality as measured by BLEU."
      ],
      "metadata": {
        "id": "VDYGQUfladDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate the BLEU score on a trained model\n",
        "\n",
        "To calculate the BLEU score on a trained model, follow these steps:\n",
        "\n",
        "1. **Install Necessary Libraries**:\n",
        "   Make sure you have the `sacrebleu` library, which provides a standardized implementation of the BLEU metric.\n",
        "   ```python\n",
        "   !pip install sacrebleu\n",
        "   ```\n",
        "\n",
        "2. **Translate Test Data**:\n",
        "   Using your trained model, translate the source sentences in your test dataset to get the predicted translations.\n",
        "\n",
        "3. **Calculate BLEU Score**:\n",
        "   Compare the predicted translations with the actual target translations using the BLEU metric.\n",
        "\n",
        "Here's a Python function that demonstrates this process:\n",
        "\n",
        "```python\n",
        "import sacrebleu\n",
        "\n",
        "def calculate_bleu(model, test_dataloader, device):\n",
        "    model.eval()\n",
        "    translations = []\n",
        "    references = []\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "        src = batch[\"encoder_input\"].to(device)\n",
        "        tgt_text = batch[\"tgt_text\"]\n",
        "        # Note: Depending on your implementation, you might need to adjust the below function\n",
        "        predicted = translate(src, model, device)  # Implement the translate function that returns the model's translations\n",
        "        translations.extend(predicted)\n",
        "        references.extend(tgt_text)\n",
        "\n",
        "    # SacreBLEU expects a list of references, where each reference is a list (supporting multiple reference translations, which we don't have here)\n",
        "    bleu_score = sacrebleu.corpus_bleu(translations, [references])\n",
        "    return bleu_score.score\n",
        "\n",
        "# Usage\n",
        "bleu = calculate_bleu(model, test_dataloader, device)\n",
        "print(f\"BLEU Score: {bleu:.2f}\")\n",
        "```\n",
        "\n",
        "Few things to note:\n",
        "\n",
        "- The function `translate` is a placeholder for whatever function you use to generate translations from your model. This function should take a batch of source sentences, pass them through your model, and return the translated sentences.\n",
        "  \n",
        "- BLEU is a comparison between your model's output and a reference translation. Higher BLEU scores indicate that the model's output is closer to the reference, with a score of 100 indicating a perfect match.\n",
        "\n",
        "- `sacrebleu.corpus_bleu` computes the BLEU score over the entire corpus, which is more meaningful than computing the average of BLEU scores for individual sentences."
      ],
      "metadata": {
        "id": "hEema_-vZ-TA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List of metrics used to evaluate machine translation models\n",
        "\n",
        "While the BLEU score is a widely used metric to evaluate machine translation models, there are other metrics and approaches that can be used to assess the quality of translations. Here's a brief overview of some of them:\n",
        "\n",
        "1. **METEOR Score**: The METEOR (Metric for Evaluation of Translation with Explicit ORdering) score is another metric that considers precision and recall of the unigrams between the translated and reference sentences. It also takes into account synonyms, stemming, and word order.\n",
        "\n",
        "2. **ROUGE Score**: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics that measures the overlap of n-grams between the generated text and a reference text. Originally developed for evaluating automatic summarization, it's sometimes used for translation as well.\n",
        "\n",
        "3. **TER (Translation Edit Rate)**: TER computes the number of edits required to change a system output into one of the references.\n",
        "\n",
        "4. **ChrF Score**: This metric evaluates the quality of translations by considering character n-grams, which can be more sensitive to differences in morphologically rich languages.\n",
        "\n",
        "5. **BERTScore**: BERTScore leverages the pre-trained contextual embeddings from BERT and matches words in candidate and reference sentences by cosine similarity. It has been shown to correlate well with human judgment.\n",
        "\n",
        "6. **Human Evaluation**: One of the most reliable ways to evaluate a translation model is to have human evaluators rate the quality of translations. While this method is resource-intensive, it's often used in combination with automated metrics to ensure high-quality translations.\n",
        "\n",
        "7. **BLEURT**: A learned metric that is trained on human evaluations. It's designed to work well with neural machine translations and is built on top of BERT.\n",
        "\n",
        "8. **Consistency Test**: Translate a sentence from language A to language B and then back-translate it from language B to language A. This should ideally produce a sentence very close to the original sentence in language A.\n",
        "\n",
        "9. **Domain-Specific Evaluation**: Depending on the use-case (e.g., medical translations, legal translations), it might be useful to develop custom evaluation metrics or have domain experts evaluate the quality of translations.\n",
        "\n",
        "10. **Diversity**: For some applications, it's important not just to get a correct translation, but a diverse set of possible translations. You can evaluate a model based on how many different, yet correct, translations it can produce.\n",
        "\n",
        "When evaluating a translation model, it's often recommended to use a combination of several metrics to get a holistic view of the model's performance. Additionally, always consider the specific domain and requirements of your application when choosing and interpreting evaluation metrics."
      ],
      "metadata": {
        "id": "By0MZtb8o8ZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## METEOR and ROUGE\n",
        "\n",
        "Both METEOR and ROUGE are common metrics used in natural language processing tasks, and there are existing implementations that make it straightforward to compute these scores. Below, I'll guide you on how to compute both:\n",
        "\n",
        "### 1. METEOR Score:\n",
        "\n",
        "The NLTK library provides an implementation for the METEOR score. Here's a basic guide to compute it:\n",
        "\n",
        "``` python\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "# Assuming you have a list of reference translations and a list of model's translations:\n",
        "references = [\"This is a reference translation.\", \"This is another one.\"]\n",
        "hypothesis = \"This is the model's translation.\"\n",
        "\n",
        "score = meteor_score(references, hypothesis)\n",
        "print(f\"METEOR Score: {score:.4f}\")\n",
        "```\n",
        "\n",
        "For multiple sentences, you can loop through each hypothesis and its corresponding references.\n",
        "\n",
        "### 2. ROUGE Score:\n",
        "\n",
        "The `rouge` library provides an implementation to compute the ROUGE score. Here's how you can use it:\n",
        "\n",
        "First, you might need to install the library:\n",
        "```\n",
        "pip install rouge\n",
        "```\n",
        "\n",
        "Then, you can compute the score as follows:\n",
        "\n",
        "``` python\n",
        "from rouge import Rouge\n",
        "\n",
        "rouge = Rouge()\n",
        "\n",
        "# Assuming you have a single reference translation and a model's translation:\n",
        "reference = \"This is a reference translation.\"\n",
        "hypothesis = \"This is the model's translation.\"\n",
        "\n",
        "scores = rouge.get_scores(hypothesis, reference, avg=True)\n",
        "\n",
        "print(f\"ROUGE-1: {scores['rouge-1']}\")\n",
        "print(f\"ROUGE-2: {scores['rouge-2']}\")\n",
        "print(f\"ROUGE-L: {scores['rouge-l']}\")\n",
        "```\n",
        "\n",
        "For `rouge.get_scores()`, the first argument is the predicted sequence, and the second argument is the ground truth.\n",
        "\n",
        "Remember, both METEOR and ROUGE scores have their own strengths and weaknesses. METEOR, for instance, takes into account synonyms and stemming, while ROUGE is based on n-gram overlaps. Depending on your application and the specifics of your dataset, one might be more appropriate than the other, or you might consider using both to get a more comprehensive view of your model's performance."
      ],
      "metadata": {
        "id": "yrkTmDHsrizm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Clipping\n",
        "\n",
        "Gradient clipping is a technique used to counter the \"exploding gradients\" problem, where large gradients result in very large updates to neural network model weights during training. At an extreme, the values of weights can become too large for the model to process, and the model can fail to fit the training data (NaN loss values).\n",
        "\n",
        "When gradients are large, the model is effectively \"learning too much\" from the current batch, which can lead to instability. By clipping the gradients, you ensure they never exceed a certain value and thus prevent large updates to the model's weights.\n",
        "\n",
        "There are different ways to perform gradient clipping, but the most common method is to normalize the gradients of the model's parameters to ensure they have a maximum norm.\n",
        "\n",
        "**How to Implement Gradient Clipping**:\n",
        "\n",
        "1. **Compute Gradients**:\n",
        "   After the backward pass, but before the optimizer's update step, gradients for all model parameters are computed.\n",
        "\n",
        "2. **Clip Gradients**:\n",
        "   The gradients are then clipped to prevent them from exceeding a threshold.\n",
        "\n",
        "3. **Update Model Parameters**:\n",
        "   The optimizer's update step is then called, using the clipped gradients.\n",
        "\n",
        "Here's how you can integrate gradient clipping into your training loop using PyTorch:\n",
        "\n",
        "```python\n",
        "GRAD_CLIP = 1.0  # Set the maximum gradient norm\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for idx, batch in enumerate(train_dataloader):\n",
        "        # ... [Your existing training code]\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "\n",
        "        # Update the model parameters\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "```\n",
        "\n",
        "The function `torch.nn.utils.clip_grad_norm_` takes in the model parameters and a maximum norm (in this case, `GRAD_CLIP`). It will clip the gradients of the model in-place.\n",
        "\n",
        "It's worth noting that the optimal value of `GRAD_CLIP` might need some tuning for different datasets and model architectures."
      ],
      "metadata": {
        "id": "yxOaOxkkbcLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Size\n",
        "\n",
        "Selecting the right batch size is a combination of empirical experimentation and understanding the trade-offs involved. In the context of training a model on Google Colab (or any environment, for that matter), here are the factors to consider:\n",
        "\n",
        "1. **Memory Constraints**:\n",
        "   - **Larger Batch Size**: Consumes more GPU memory. If the batch size is too large, you may run out of GPU memory, leading to out-of-memory (OOM) errors.\n",
        "   - **Smaller Batch Size**: Consumes less GPU memory, allowing for more complex models or larger input sizes.\n",
        "   \n",
        "2. **Training Dynamics**:\n",
        "   - **Larger Batch Size**: May lead to more stable training dynamics because the gradient estimate is based on more data, and is therefore less noisy. This can lead to faster convergence.\n",
        "   - **Smaller Batch Size**: Can introduce more noise in the gradient estimate. Sometimes, this noise can be beneficial as it acts as a form of implicit regularization, possibly helping to escape local minima. However, it can also make training less stable.\n",
        "\n",
        "3. **Generalization**:\n",
        "   - Some research suggests that smaller batch sizes might lead to models that generalize better, at the cost of longer training times (because more updates are required).\n",
        "\n",
        "4. **Training Speed**:\n",
        "   - **Larger Batch Size**: Processes more data at once, which can lead to faster epochs. However, the benefit tends to plateau after a certain point due to hardware limitations.\n",
        "   - **Smaller Batch Size**: Processes less data at once, leading to slower epochs. However, it might require fewer epochs to converge.\n",
        "\n",
        "5. **Hardware Optimization**:\n",
        "   - Modern GPUs are optimized for parallel processing. Using larger batch sizes can better utilize the GPU's parallel processing capabilities, leading to faster training times. However, this is up to a saturation point, after which increasing the batch size might not yield performance gains.\n",
        "\n",
        "6. **Frequency of Model Updates**:\n",
        "   - **Larger Batch Size**: Model weights are updated less frequently (fewer updates per epoch).\n",
        "   - **Smaller Batch Size**: More frequent updates can lead to faster convergence, but each update might be of lower quality due to the noisy gradient.\n",
        "\n",
        "**Recommendation for Google Colab**:\n",
        "\n",
        "Google Colab typically offers a Tesla K80, T4, or P100 for its free tier, with around 12-16GB of GPU memory. Considering this:\n",
        "\n",
        "1. Start with a small batch size (e.g., 32 or 64) to ensure everything runs without memory issues.\n",
        "2. Gradually increase the batch size while monitoring GPU memory usage (you can use the `nvidia-smi` command in a Colab cell) until you start to approach the memory limit or notice diminishing returns in speed.\n",
        "3. Remember to adjust the learning rate when changing the batch size, as they are often correlated. As a heuristic, if you double the batch size, you might want to double the learning rate, but this isn't a strict rule and requires experimentation.\n",
        "4. During experimentation, use a subset of your data to quickly gauge the effects of different batch sizes on training dynamics and speed.\n",
        "\n",
        "In conclusion, the best batch size often depends on the specific problem, dataset, and model architecture. It's usually found through a combination of understanding the trade-offs and empirical testing."
      ],
      "metadata": {
        "id": "xxwR9xGXco88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference output\n",
        "\n",
        "The output you're seeing is a common issue with models that haven't been trained to a sufficient level of accuracy or that are not set up optimally for the decoding process. It's repeating phrases or producing overly verbose translations.\n",
        "\n",
        "A few possible reasons for this:\n",
        "\n",
        "1. **Model Training**: The model might not have been trained sufficiently or might not have learned an optimal representation.\n",
        "  \n",
        "2. **Greedy Decoding**: The decoding method we used is called greedy decoding, which takes the highest probability word at each time step. This can sometimes lead to repetitive sequences.\n",
        "\n",
        "3. **Lack of Beam Search**: A more sophisticated method than greedy decoding is beam search, which considers multiple sequences at once and can produce more coherent outputs. Implementing beam search is more involved but can help in generating better translations.\n",
        "\n",
        "4. **Model Architecture**: If the model is too small or not deep enough, it might not capture the complexities of the translation task. Adjusting the model's hyperparameters might help.\n",
        "\n",
        "5. **Dataset Limitations**: If the training data doesn't have varied examples, the model might not generalize well to unseen sentences.\n",
        "\n",
        "For immediate improvement, you could try:\n",
        "\n",
        "1. Implementing **beam search**.\n",
        "2. Training the model for more epochs or until the validation loss doesn't improve.\n",
        "3. Using a **larger model** with more layers or increasing the embedding size.\n",
        "4. If possible, using a **pre-trained model** or incorporating more training data.\n",
        "\n",
        "Regarding the current implementation, one quick check would be to ensure that after the `[EOS]` token is appended, the decoding process stops. It seems to be the case from the code you provided, so the repetitive nature likely stems from the model's current training state or the greedy decoding method.\n"
      ],
      "metadata": {
        "id": "d0maq8FPFkQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monitoring training and validation curves\n",
        "\n",
        "Monitoring training and validation curves is essential for understanding how well your model is learning and to diagnose potential issues like overfitting or underfitting. The process involves logging and visualizing the loss (and possibly other metrics) for both the training and validation datasets over each epoch.\n",
        "\n",
        "Here's a step-by-step guide on how to implement this using the popular `matplotlib` library:\n",
        "\n",
        "1. **Initialize Logging Variables**: Before the training loop, initialize lists to store training and validation losses.\n",
        "\n",
        "2. **Log Losses During Training**: Inside the training loop, append the training and validation losses to their respective lists.\n",
        "\n",
        "3. **Visualize Losses After Training**: After the training loop, use `matplotlib` to plot the training and validation curves.\n",
        "\n",
        "Let's add this to your existing code:\n",
        "\n",
        "1. Initialize logging variables:\n",
        "\n",
        "```python\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "```\n",
        "\n",
        "2. Log losses during training:\n",
        "\n",
        "Inside your training loop, after computing the average training loss and validation loss for an epoch:\n",
        "\n",
        "```python\n",
        "avg_train_loss = total_loss / len(train_dataloader)\n",
        "train_losses.append(avg_train_loss)\n",
        "\n",
        "avg_val_loss = validate(model, val_dataloader, loss_fn, device)\n",
        "val_losses.append(avg_val_loss)\n",
        "```\n",
        "\n",
        "3. Visualize losses after training:\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.plot(train_losses, label=\"Training\")\n",
        "plt.plot(val_losses, label=\"Validation\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "By examining the plotted curves:\n",
        "\n",
        "- If your **training loss keeps decreasing but validation loss starts increasing**, this may indicate your model is overfitting. The model is memorizing the training data and performs poorly on unseen data.\n",
        "  \n",
        "- If both **training and validation loss decrease slowly** or plateau, your model might be underfitting. It's not complex enough to capture the underlying data patterns.\n",
        "\n",
        "- Ideally, you want both the **training and validation losses to decrease** to a point of stability with a minimal gap between them.\n",
        "\n",
        "Monitoring curves is a simple yet powerful technique. For a more advanced setup, you might want to consider using tools like TensorBoard (which comes with PyTorch's `torch.utils.tensorboard`), which allows for real-time monitoring of training/validation metrics, model architectures, and more."
      ],
      "metadata": {
        "id": "0JszlEEQrd9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the model on the test set\n",
        "\n",
        "Evaluating the model on the test set is similar to the validation process, but without any model updates. Essentially, you'll be feeding the test data into your trained model and computing the loss (and other metrics, if applicable).\n",
        "\n",
        "Here's how you can evaluate your model on the test set:\n",
        "\n",
        "1. **Ensure Model is in Evaluation Mode**: Before evaluating, always set your model to evaluation mode. This is crucial because certain layers like dropout or batch normalization work differently during training and evaluation.\n",
        "\n",
        "    ```python\n",
        "    model.eval()\n",
        "    ```\n",
        "\n",
        "2. **Compute the Loss on the Test Set**:\n",
        "\n",
        "    ```python\n",
        "    def evaluate_on_test(model, test_dataloader, loss_fn, device):\n",
        "        total_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in test_dataloader:\n",
        "                # Extract data from the batch\n",
        "                src = batch[\"encoder_input\"].to(device)\n",
        "                tgt = batch[\"decoder_input\"].to(device)\n",
        "                src_mask = batch[\"encoder_mask\"].to(device)\n",
        "                tgt_mask = batch[\"decoder_mask\"].to(device)\n",
        "                labels = batch[\"label\"].to(device)\n",
        "                \n",
        "                # Forward pass\n",
        "                logits = model(src, tgt, src_mask, tgt_mask)\n",
        "\n",
        "                # Compute the loss\n",
        "                loss = loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        avg_test_loss = total_loss / len(test_dataloader)\n",
        "        return avg_test_loss\n",
        "    ```\n",
        "\n",
        "3. **Call the Evaluation Function**:\n",
        "\n",
        "    ```python\n",
        "    test_loss = evaluate_on_test(model, test_dataloader, loss_fn, device)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    ```\n",
        "\n",
        "4. **(Optional) Compute Other Metrics**:\n",
        "\n",
        "    If you want to compute additional metrics like BLEU score for translation tasks, you'd need to modify the `evaluate_on_test` function to also generate model outputs, then compare these outputs to the ground truth to compute the BLEU score.\n",
        "\n",
        "    Remember, while the loss is a good metric to track during training, it might not always be the most representative of the model's real-world performance. Metrics like BLEU, ROUGE, etc., for translation tasks, or accuracy, precision, recall for classification tasks, can provide a clearer picture of the model's performance.\n",
        "\n",
        "Now, by following the above steps, you can evaluate your model's performance on the test set after training. This will give you an unbiased estimate of how well your model might perform in a real-world setting, as the test set represents previously unseen data."
      ],
      "metadata": {
        "id": "9bFn4OnIr5mq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overfitting\n",
        "\n",
        "When the training loss continues to decrease, but the validation loss remains roughly level or even starts increasing, this phenomenon is typically indicative of overfitting. Let's break down what might be happening:\n",
        "\n",
        "1. **Overfitting**: This is the most common reason for such a pattern. Overfitting occurs when a model begins to \"memorize\" the training data rather than \"generalizing\" from it. As a result, the model performs very well on the training data (hence the decreasing training loss) but fails to generalize well to new, unseen data (leading to stagnant or increasing validation loss).\n",
        "\n",
        "2. **Gap Between Training and Validation Loss**: The larger the gap between the training and validation loss, the more the model is likely overfitting. A small gap can be acceptable, but a significant divergence should be a cause for concern.\n",
        "\n",
        "3. **Capacity of the Model**: A model with a large number of parameters (i.e., a very \"deep\" neural network) has a higher capacity to fit the training data. If not regularized or controlled, such models can easily overfit.\n",
        "\n",
        "4. **Insufficient Regularization**: Techniques like dropout, weight decay (L2 regularization), or even early stopping can help in preventing overfitting. If the model is overfitting, consider increasing the regularization.\n",
        "\n",
        "5. **Data Augmentation**: Especially for tasks like image classification, data augmentation can help in improving generalization. If you're not already using it, consider adding data augmentation techniques suitable for your data type.\n",
        "\n",
        "6. **Dataset Size**: Overfitting is more prevalent when the amount of training data available is small. In such cases, even a model of moderate complexity can overfit.\n",
        "\n",
        "### Remedial Steps:\n",
        "\n",
        "1. **Early Stopping**: You can stop the training process once the validation loss starts to plateau or increase. This is a form of regularization and can prevent the model from overfitting.\n",
        "\n",
        "2. **Increase Regularization**: If using techniques like dropout, consider increasing the dropout rate. Alternatively, you could also increase the weight decay coefficient if using L2 regularization.\n",
        "\n",
        "3. **Reduce Model Complexity**: Consider using a simpler model architecture with fewer parameters.\n",
        "\n",
        "4. **Use a Learning Rate Schedule**: Sometimes, reducing the learning rate during training can help in achieving a better fit.\n",
        "\n",
        "5. **Data Augmentation**: If applicable, use data augmentation techniques to artificially increase the size and diversity of your training set.\n",
        "\n",
        "6. **Cross-Validation**: Using cross-validation can give a better estimate of the model's performance on unseen data and can help in diagnosing overfitting.\n",
        "\n",
        "To summarize, when the training loss consistently decreases but the validation loss plateaus or even increases, it's a sign that the model might be overfitting to the training data. Various techniques and strategies can be employed to address this and improve the model's generalization capability."
      ],
      "metadata": {
        "id": "Bx0CR8ottbe7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Qualitative evaluation\n",
        "\n",
        "Qualitative evaluation involves manually inspecting the outputs of your model to gauge its performance. For translation tasks, this means looking at some source sentences, the model's translations of those sentences, and comparing them to the actual (ground truth) translations.\n",
        "\n",
        "Here's how you can do a qualitative evaluation:\n",
        "\n",
        "1. **Randomly Select Samples**: Choose a few random samples from your test dataset.\n",
        "\n",
        "2. **Translate Using Model**: Pass these samples through your trained model to get the translated outputs.\n",
        "\n",
        "3. **Compare with Ground Truth**: Display the source sentence, the model's translation, and the actual translation side by side. This will give you a sense of how well the model is translating.\n",
        "\n",
        "Here's a simple code to help you with this:\n",
        "\n",
        "```python\n",
        "import random\n",
        "\n",
        "# Choose a random batch from the test dataloader\n",
        "batch = next(iter(test_dataloader))\n",
        "\n",
        "# Extract a few samples from the batch\n",
        "num_samples = 5\n",
        "indices = random.sample(range(batch['encoder_input'].shape[0]), num_samples)\n",
        "\n",
        "for idx in indices:\n",
        "    source_sentence = tokenizer_en.decode(batch['encoder_input'][idx].tolist(), skip_special_tokens=True)\n",
        "    actual_translation = tokenizer_fr.decode(batch['label'][idx].tolist(), skip_special_tokens=True)\n",
        "    \n",
        "    # Model's translation\n",
        "    translated_sentence = translate_sentence(source_sentence, model, tokenizer_en, tokenizer_fr, MAX_SEQ_LEN, device)\n",
        "    \n",
        "    print(f\"Source Sentence: {source_sentence}\")\n",
        "    print(f\"Model's Translation: {translated_sentence}\")\n",
        "    print(f\"Actual Translation: {actual_translation}\")\n",
        "    print(\"=\"*80)\n",
        "```\n",
        "\n",
        "By visually inspecting these translations, you can:\n",
        "\n",
        "- **Identify Common Mistakes**: Does your model consistently make certain types of errors? For instance, is it often missing out on translating certain words or mistranslating them?\n",
        "  \n",
        "- **Assess Fluency**: Are the translations grammatically correct and fluent, or do they read awkwardly?\n",
        "  \n",
        "- **Check for Overfitting**: If your model is merely memorizing the training data, it might produce translations that are too literal or not contextually appropriate.\n",
        "  \n",
        "- **Get Insights for Further Improvement**: Based on the mistakes, you can think of ways to improve the model, whether it's by changing the architecture, using more data, or employing other regularization techniques."
      ],
      "metadata": {
        "id": "umC_nksuzyw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `nvidia-smi`\n",
        "\n",
        "`nvidia-smi` (NVIDIA System Management Interface) is a command-line utility, shipped with NVIDIA GPU drivers, that provides monitoring and management capabilities for NVIDIA GPUs. It's a helpful tool to get a snapshot of your GPU's current state, usage, temperatures, and more.\n",
        "\n",
        "Here are some of the basic usages and explanations:\n",
        "\n",
        "1. **Basic Usage**:\n",
        "   Simply run `nvidia-smi` in your terminal. This gives a snapshot of the current GPU state, including:\n",
        "   - GPU name and driver version.\n",
        "   - Current GPU temperature.\n",
        "   - Current GPU utilization.\n",
        "   - Memory usage (used memory / total memory).\n",
        "   - List of processes currently using the GPU.\n",
        "\n",
        "2. **Update Continuously Every X Seconds**:\n",
        "   ```\n",
        "   nvidia-smi -l X\n",
        "   ```\n",
        "   Replace `X` with the number of seconds you'd like the information to be refreshed.\n",
        "\n",
        "3. **Query Specific Information**:\n",
        "   `nvidia-smi` allows you to query specific attributes. For example, to only get GPU utilization and memory usage, use:\n",
        "   ```\n",
        "   nvidia-smi --query-gpu=utilization.gpu,utilization.memory --format=csv\n",
        "   ```\n",
        "\n",
        "4. **Display More Detailed Information**:\n",
        "   ```\n",
        "   nvidia-smi -q\n",
        "   ```\n",
        "\n",
        "5. **Display Information About Installed NVIDIA Driver**:\n",
        "   ```\n",
        "   nvidia-smi -a\n",
        "   ```\n",
        "\n",
        "6. **Set GPU Configuration**:\n",
        "   While `nvidia-smi` is mostly used for monitoring, it also has some management capabilities. For instance, you can set the GPU's persistence mode or compute mode. However, be careful when changing these settings; do so only if you understand the implications.\n",
        "\n",
        "7. **List Processes Using GPU**:\n",
        "   ```\n",
        "   nvidia-smi pmon\n",
        "   ```\n",
        "   This shows a real-time monitor of processes using the GPU.\n",
        "\n",
        "8. **Kill a Process Using GPU**:\n",
        "   If you identify a process (from the list of processes) that you want to kill, note its PID, and then use the `kill` command in the terminal:\n",
        "   ```\n",
        "   kill -9 PID\n",
        "   ```\n",
        "\n",
        "9. **Help**:\n",
        "   To get a list of all commands and options, simply use:\n",
        "   ```\n",
        "   nvidia-smi --help\n",
        "   ```\n",
        "\n",
        "**Note**: If you're on a system like Google Colab or another cloud-based Jupyter environment, you can run these commands directly from a notebook cell by prefixing them with an exclamation mark, e.g., `!nvidia-smi`.\n",
        "\n",
        "In a nutshell, `nvidia-smi` is a handy tool for anyone using NVIDIA GPUs, whether for deep learning, gaming, or other compute-heavy tasks. It provides a quick overview of the GPU's state and helps in diagnosing potential issues."
      ],
      "metadata": {
        "id": "2rIaumnC400N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `nvidia-smi` on a colab\n",
        "\n",
        "In a long-running job on Google Colab (or any Jupyter-like environment), you might want to monitor your GPU usage intermittently to check if everything is running as expected. Here's how you can use `nvidia-smi` within such an environment:\n",
        "\n",
        "1. **Basic Usage in a New Cell**:\n",
        "   Simply add a new cell in your Colab notebook and run:\n",
        "   ```python\n",
        "   !nvidia-smi\n",
        "   ```\n",
        "\n",
        "   This will give you a snapshot of the current GPU state. You can run this cell anytime you want to check the GPU status.\n",
        "\n",
        "2. **Monitor Continuously**:\n",
        "   If you want to monitor GPU usage continuously, you can run `nvidia-smi` in a loop. However, be aware that this will keep running until you manually stop it, so it might not be ideal for very long jobs. Here's a simple Python loop that calls `nvidia-smi` every 60 seconds:\n",
        "   \n",
        "   ```python\n",
        "   import time\n",
        "\n",
        "   while True:\n",
        "       print(\"\\n------\")\n",
        "       !nvidia-smi\n",
        "       print(\"------\\n\")\n",
        "       time.sleep(60)\n",
        "   ```\n",
        "\n",
        "   To stop the loop, you can simply interrupt the cell execution.\n",
        "\n",
        "3. **Automatically Logging GPU Usage**:\n",
        "   If you're interested in logging GPU statistics over time without manual intervention, you can write the output of `nvidia-smi` to a file at regular intervals:\n",
        "\n",
        "   ```python\n",
        "   import time\n",
        "\n",
        "   with open(\"gpu_log.txt\", \"w\") as log_file:\n",
        "       for _ in range(10):  # Log for 10 minutes, adjust as needed\n",
        "           log_file.write(\"\\n------\\n\")\n",
        "           log_file.write(!nvidia-smi)\n",
        "           log_file.write(\"------\\n\")\n",
        "           time.sleep(60)\n",
        "   ```\n",
        "\n",
        "   After the logging is done, you can download and inspect the `gpu_log.txt` file.\n",
        "\n",
        "4. **Visualization Tools**:\n",
        "   While `nvidia-smi` provides text-based statistics, there are tools like TensorBoard (which you've already looked into) and `wandb` that can offer graphical visualizations of system metrics, including GPU usage, during training.\n",
        "\n",
        "**Note**: In Google Colab, your session can timeout due to inactivity or if it exceeds the maximum allowed runtime. Therefore, even if your code is set up to log GPU usage over a long period, the Colab session might terminate before that. Always ensure you've saved your data and models to avoid losing progress."
      ],
      "metadata": {
        "id": "nxsVgnnu5OmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input to Cross Entrpy Loss\n",
        "\n",
        "1. **Forward Pass**:\n",
        "    - `logits = model(src, tgt, src_mask, tgt_mask)`:\n",
        "      This line is passing the input tensors (`src`, `tgt`, `src_mask`, and `tgt_mask`) through the model to get the output `logits`. The shape of `logits` would typically be `(batch_size, sequence_length, vocab_size)`.\n",
        "        - `batch_size`: Number of sequences in the current batch.\n",
        "        - `sequence_length`: Number of tokens in each sequence.\n",
        "        - `vocab_size`: Size of the target vocabulary, which represents the number of possible output tokens.\n",
        "\n",
        "2. **Compute the Loss**:\n",
        "    - The `loss_fn` expects the logits and labels to be 2D and 1D tensors, respectively. However, the output `logits` from the model is a 3D tensor, and `labels` is a 2D tensor. So, we need to reshape them before passing them to the `loss_fn`.\n",
        "\n",
        "    - `logits.view(-1, logits.shape[-1])`:\n",
        "        - This reshapes the `logits` tensor to a 2D tensor.\n",
        "        - The `-1` in the view function means that this dimension will be inferred from the length of the tensor so that the total number of elements remains constant.\n",
        "        - `logits.shape[-1]` ensures that the last dimension remains the same (which is `vocab_size`).\n",
        "        - The resulting shape will be `(batch_size * sequence_length, vocab_size)`.\n",
        "    \n",
        "    - `labels.view(-1)`:\n",
        "        - This reshapes the `labels` tensor to be a 1D tensor.\n",
        "        - The resulting shape will be `(batch_size * sequence_length)`, which is just a flattened version of the original labels.\n",
        "\n",
        "    - `loss = loss_fn(...)`:\n",
        "        - The loss function (`loss_fn`) computes the CrossEntropyLoss between the predicted logits and the actual labels. Given that the logits are now a 2D tensor where each row represents the logits for a single token, and the labels are a 1D tensor where each entry is the actual token ID, the `loss_fn` can now compute the loss for each token and then average it across all tokens.\n",
        "\n",
        "In summary, the reshaping operations ensure that the logits and labels are in the expected format for the loss function, allowing it to compute the loss for each token in the sequences and then average the loss over all tokens."
      ],
      "metadata": {
        "id": "vS8uff44xSsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Entropy Loss in the context of machine translation.\n",
        "\n",
        "1. **Cross Entropy Loss in NLP**:\n",
        "\n",
        "    Cross Entropy Loss measures the performance of a classification model whose output is a probability value between 0 and 1. In the context of neural networks used for NLP tasks like translation, the loss quantifies the difference between the predicted probability distribution and the true distribution.\n",
        "\n",
        "2. **Multi-class Classification**:\n",
        "\n",
        "    Yes, in machine translation, for each token in the output sequence, the model predicts a probability distribution over the entire vocabulary. So if the vocabulary has 10,000 words, the model will output a probability distribution of size 10,000 for each token in the sequence. The token with the highest probability is the model's prediction for that position in the sequence. This is a multi-class classification problem for each token in the sequence.\n",
        "\n",
        "3. **Simple Example**:\n",
        "\n",
        "    Let's consider a simple translation task where we're translating the English word \"hello\" to French. Our target vocabulary has only three words: `['bonjour', 'salut', 'chat']` (which translate to \"hello\", \"hi\", and \"cat\", respectively).\n",
        "\n",
        "    For the word \"hello\", our model might output the following probability distribution:\n",
        "    `{'bonjour': 0.7, 'salut': 0.2, 'chat': 0.1}`.\n",
        "\n",
        "    The true distribution, since we know \"hello\" should be translated to \"bonjour\" in this context, would be:\n",
        "    `{'bonjour': 1.0, 'salut': 0.0, 'chat': 0.0}`.\n",
        "\n",
        "    The Cross Entropy Loss would then measure the difference between these two distributions. In this case, the model's prediction is fairly good, but not perfect, so there would be some loss. The closer the predicted distribution is to the true distribution, the lower the Cross Entropy Loss.\n",
        "\n",
        "4. **Why is it Useful for Translation Tasks?**:\n",
        "\n",
        "    In translation, the target sentence can have multiple correct translations, and some words in the vocabulary are more likely to appear than others. The Cross Entropy Loss allows the model to be penalized more for being certain and wrong than for being uncertain.\n",
        "\n",
        "    For instance, if the true translation is a common word, but the model predicts a rare word with high confidence, the loss will be large. On the other hand, if the model is unsure and outputs a more uniform probability distribution, the loss will be smaller. This property encourages the model to make predictions that are in line with the frequency and likelihood of words and phrases in the target language, leading to more fluent and natural translations.\n",
        "\n",
        "In essence, Cross Entropy Loss helps ensure that the model's predictions align with the actual likelihoods of words and phrases in the target language, which is crucial for generating coherent and accurate translations."
      ],
      "metadata": {
        "id": "KTdy37yg-G0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the optimizer's state\n",
        "\n",
        "Saving the optimizer's state along with the model can be beneficial, especially if you plan to resume training later on. Here's why:\n",
        "\n",
        "1. **Learning Rate State**: If you're using learning rate schedules or learning rate decay, the optimizer retains this state. If you don't save it and then later resume training, you might inadvertently start with the original learning rate, potentially undoing some of the benefits of the decay or schedule you've set up.\n",
        "\n",
        "2. **Momentum**: Some optimizers, like SGD with momentum or Adam, accumulate values (momentums or moving averages of parameters) over time which are used to update the parameters. If you don't save these accumulated values and resume training later, the optimizer will start from scratch, and this might lead to more volatile or less optimal training.\n",
        "\n",
        "3. **Avoiding Stale Starts**: Even without learning rate schedules or momentum, starting again with a 'fresh' optimizer might lead to a stale or suboptimal start when resuming training.\n",
        "\n",
        "4. **Consistency**: It's a good practice to ensure that if you pause and then resume training, the process picks up as if it never stopped. Saving the optimizer's state is part of that consistency.\n",
        "\n",
        "However, there are some things to keep in mind:\n",
        "\n",
        "- **Storage**: Optimizers, especially for large models, can significantly increase the size of the checkpoint. If storage is a concern, this might be a consideration.\n",
        "  \n",
        "- **Deployment**: If you're saving for the purpose of deployment, you typically don't need the optimizer's state. In such cases, just saving the model's state_dict might be enough.\n",
        "\n",
        "If you decide to save the optimizer's state, here's how you can do it:\n",
        "\n",
        "```python\n",
        "# Saving\n",
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            ...\n",
        "            }, PATH)\n",
        "\n",
        "# Loading\n",
        "checkpoint = torch.load(PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "...\n",
        "model.train()  # or model.eval() depending on what you're doing next\n",
        "```\n",
        "\n",
        "Remember, if you're changing any hyperparameters or training conditions when resuming, it might make sense not to load the optimizer's state, as the previous state might not be as relevant under the new conditions."
      ],
      "metadata": {
        "id": "r_Igi7vNAWMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to create a subset of the test dataset\n",
        "\n",
        "It's a good practice to try it out on a smaller subset first to ensure that everything is working as expected. You can create a smaller DataLoader from your test or validation dataset.\n",
        "\n",
        "Here's how you can do this for 50 rows:\n",
        "\n",
        "1. Create a new subset of the test dataset.\n",
        "2. Create a DataLoader for this subset.\n",
        "3. Compute the BLEU score using this smaller DataLoader.\n",
        "\n",
        "Here's the code:\n",
        "\n",
        "```python\n",
        "# 1. Create a subset of the test dataset\n",
        "small_test_subset = torch.utils.data.Subset(test_dataset, indices=range(50))\n",
        "\n",
        "# 2. Create a DataLoader for this subset\n",
        "small_test_dataloader = torch.utils.data.DataLoader(small_test_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 3. Compute the BLEU score using the smaller DataLoader\n",
        "bleu_score_small = calculate_bleu(small_test_dataloader, model, tokenizer_en, tokenizer_fr, device)\n",
        "print(f\"Small Test BLEU Score: {bleu_score_small:.4f}\")\n",
        "```\n",
        "\n",
        "This approach will give you a quick check to see if there are any issues or errors in the BLEU calculation. If everything runs smoothly on the smaller subset, you can then confidently compute the BLEU score on the entire test set."
      ],
      "metadata": {
        "id": "06Pfrfp9fiKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reproducibility\n",
        "\n",
        "When working with machine learning models, reproducibility is crucial. To ensure reproducibility, especially when splitting datasets, you should:\n",
        "\n",
        "1. **Set Random Seeds**: Before performing any operations that involve randomness (like splitting a dataset), set random seeds. This includes seeds for native Python, NumPy, and PyTorch or any other deep learning framework you are using. Setting seeds ensures that the random operations produce the same results every time you run them.\n",
        "\n",
        "   ```python\n",
        "   import random\n",
        "   import numpy as np\n",
        "   import torch\n",
        "\n",
        "   SEED = 42\n",
        "\n",
        "   random.seed(SEED)\n",
        "   np.random.seed(SEED)\n",
        "   torch.manual_seed(SEED)\n",
        "   if torch.cuda.is_available():\n",
        "       torch.cuda.manual_seed(SEED)\n",
        "       torch.backends.cudnn.deterministic = True\n",
        "   ```\n",
        "\n",
        "2. **Save Train/Test/Validation Splits**: After splitting your data, save the indices used for each split. This way, even if you didn't set a seed or if you lose the session, you can always reload the exact same data splits.\n",
        "\n",
        "   For instance, if you're using PyTorch's `random_split`, it returns subsets of the original dataset. You can save the indices of these subsets:\n",
        "\n",
        "   ```python\n",
        "   train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "   # Save indices\n",
        "   with open('train_indices.txt', 'w') as f:\n",
        "       for idx in train_dataset.indices:\n",
        "           f.write(\"%s\\n\" % idx)\n",
        "\n",
        "   # Repeat for val_dataset and test_dataset\n",
        "   ```\n",
        "\n",
        "   To load the data splits in a future session, you can use these saved indices to subset your original dataset.\n",
        "\n",
        "3. **Version Control**: Use version control (like Git) for your code. This way, you can always track back to the exact code (and random seed) you used at a specific point in time.\n",
        "\n",
        "4. **Consistent Data Versioning**: Make sure you're always working with the same version of the dataset. Data versioning tools like [DVC](https://dvc.org/) can be helpful in tracking and managing different versions of datasets.\n",
        "\n",
        "5. **Document Everything**: Alongside code versioning, document the experiments, seeds, hyperparameters, and any relevant information in a systematic way. Tools like [MLflow](https://mlflow.org/) can be very handy for this purpose.\n",
        "\n",
        "By following the above best practices, you'll ensure that your experiments on platforms like Colab (or any other environment) are reproducible and can be reliably built upon in the future."
      ],
      "metadata": {
        "id": "qR7l0GgTiNSK"
      }
    }
  ]
}