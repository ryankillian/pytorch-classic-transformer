{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminary dataset analysis"
      ],
      "metadata": {
        "id": "sNgUYyDUCJ2w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XaliYSAyQIx",
        "outputId": "68a20e82-102a-4372-9771-08bc92ce5833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the opus_books dataset\n",
        "dataset = load_dataset(\"opus_books\", \"en-fr\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4TwAfOLjy-Th"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrGRtHsxz__6",
        "outputId": "258c536b-f8b3-4ad0-d60a-25178a4c00f2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': (127085, 2)}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few examples from the training set\n",
        "print(dataset[\"train\"].shape)\n",
        "print(dataset[\"train\"][:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx8O3rAgzr0o",
        "outputId": "7f4b1b7e-d4db-4dda-8878-2356f1b8b311"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(127085, 2)\n",
            "{'id': ['0', '1', '2', '3', '4', '5'], 'translation': [{'en': 'The Wanderer', 'fr': 'Le grand Meaulnes'}, {'en': 'Alain-Fournier', 'fr': 'Alain-Fournier'}, {'en': 'First Part', 'fr': 'PREMIÈRE PARTIE'}, {'en': 'I', 'fr': 'CHAPITRE PREMIER'}, {'en': 'THE BOARDER', 'fr': 'LE PENSIONNAIRE'}, {'en': 'He arrived at our home on a Sunday of November, 189-.', 'fr': 'Il arriva chez nous un dimanche de novembre 189-…'}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Length Analysis\n",
        "english_lengths = [len(example['translation']['en'].split()) for example in dataset[\"train\"]]\n",
        "french_lengths = [len(example['translation']['fr'].split()) for example in dataset[\"train\"]]\n",
        "\n",
        "print(f\"Average English sentence length: {sum(english_lengths) / len(english_lengths)}\")\n",
        "print(f\"Average French sentence length: {sum(french_lengths) / len(french_lengths)}\")\n",
        "print(f\"Maximum English sentence length: {max(english_lengths)}\")\n",
        "print(f\"Maximum French sentence length: {max(french_lengths)}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2URTeoz0RSY",
        "outputId": "cd6d5c74-79c8-44a7-8fe8-d36db5710b85"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average English sentence length: 21.364212928355037\n",
            "Average French sentence length: 20.79159617578786\n",
            "Maximum English sentence length: 372\n",
            "Maximum French sentence length: 324\n",
            "Maximum French sentence length: 324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique Tokens (basic count for now)\n",
        "english_tokens = set()\n",
        "french_tokens = set()\n",
        "\n",
        "for example in dataset[\"train\"]:\n",
        "    english_tokens.update(example['translation']['en'].split())\n",
        "    french_tokens.update(example['translation']['fr'].split())\n",
        "\n",
        "print(f\"Unique English tokens: {len(english_tokens)}\")\n",
        "print(f\"Unique French tokens: {len(french_tokens)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0tzzhMA-L4m",
        "outputId": "df3fa40e-2203-4b1e-ae33-6e9e562845ee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique English tokens: 146031\n",
            "Unique French tokens: 169205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length_en_item = max(dataset['train']['translation'], key=lambda x: len(x['en'].split()))\n",
        "print(len(max_length_en_item['en'].split()))\n",
        "print(max_length_en_item['en'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhSWFHL8k-v9",
        "outputId": "f0d159a2-9994-41f5-eea1-7dccba639448"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "372\n",
            "Upon the whole, I was by this time so fixed upon my design of going over with him to the continent that I told him we would go and make one as big as that, and he should go home in it. He answered not one word, but looked very grave and sad. I asked him what was the matter with him. He asked me again, “Why you angry mad with Friday?—what me done?” I asked him what he meant. I told him I was not angry with him at all. “No angry!” says he, repeating the words several times; “why send Friday home away to my nation?” “Why,” says I, “Friday, did not you say you wished you were there?” “Yes, yes,” says he, “wish we both there; no wish Friday there, no master there.” In a word, he would not think of going there without me. “I go there, Friday?” says I; “what shall I do there?” He turned very quick upon me at this. “You do great deal much good,” says he; “you teach wild mans be good, sober, tame mans; you tell them know God, pray God, and live new life.” “Alas, Friday!” says I, “thou knowest not what thou sayest; I am but an ignorant man myself.” “Yes, yes,” says he, “you teachee me good, you teachee them good.” “No, no, Friday,” says I, “you shall go without me; leave me here to live by myself, as I did before.” He looked confused again at that word; and running to one of the hatchets which he used to wear, he takes it up hastily, and gives it to me. “What must I do with this?” says I to him. “You take kill Friday,” says he. “What must kill you for?” said I again. He returns very quick—“What you send Friday away for? Take kill Friday, no send Friday away.” This he spoke so earnestly that I saw tears stand in his eyes. In a word, I so plainly discovered the utmost affection in him to me, and a firm resolution in him, that I told him then and often after, that I would never send him away from me if he was willing to stay with me.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Vocabulary"
      ],
      "metadata": {
        "id": "R0a5kZqnGOki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeE9sVr1gbbU",
        "outputId": "e1faa0ee-7621-4220-e5e3-5ea8f7992151"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "def get_all_sentences(ds, lang):\n",
        "    for item in ds['translation']:\n",
        "        yield item[lang]\n",
        "\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer_en = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "tokenizer_en.pre_tokenizer = Whitespace()\n",
        "\n",
        "dataset = dataset[\"train\"]\n",
        "# Train tokenizer\n",
        "trainer_en = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
        "tokenizer_en.train_from_iterator(get_all_sentences(dataset, 'en'), trainer=trainer_en)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mc922cVPe2sP"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer_fr = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "tokenizer_fr.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Train tokenizer\n",
        "trainer_fr = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
        "tokenizer_fr.train_from_iterator(get_all_sentences(dataset, 'fr'), trainer=trainer_fr)"
      ],
      "metadata": {
        "id": "oRJDfdY8022c"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer_en.get_vocab_size())\n",
        "print(tokenizer_fr.get_vocab_size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyUOeZFy_XrE",
        "outputId": "3489c36c-5c6d-486b-a030-2648c429ecab"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000\n",
            "30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize Dataset"
      ],
      "metadata": {
        "id": "x9emOFEgdkmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(dataset, tokenizer_en, tokenizer_fr):\n",
        "    tokenized_data_en = []\n",
        "    tokenized_data_fr = []\n",
        "    for example in dataset['translation']:\n",
        "        tokenized_data_en.append(tokenizer_en.encode(example['en']).ids)\n",
        "        tokenized_data_fr.append(tokenizer_fr.encode(example['fr']).ids)\n",
        "    return tokenized_data_en, tokenized_data_fr\n",
        "\n",
        "\n",
        "tokenized_data_en,  tokenized_data_fr = tokenize_data(dataset, tokenizer_en, tokenizer_fr)"
      ],
      "metadata": {
        "id": "D2X5jI3JeAaM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_data_en[:5])\n",
        "print(tokenized_data_fr[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Yq_E_qYeCqo",
        "outputId": "405b9aac-e355-49b5-97b0-52c2769b1be7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[46, 0], [0, 31, 0], [2293, 9371], [11], [904, 0]]\n",
            "[[82, 157, 774], [0, 14, 0], [29730, 14265], [1033, 17335], [2150, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length_item = max(tokenized_data_en, key=len)\n",
        "print(len(max_length_item))\n",
        "print(max_length_item)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7HIAWUqksia",
        "outputId": "01842642-5ed3-42c8-dd9b-e2b08777e655"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "471\n",
            "[2363, 5, 306, 4, 11, 14, 42, 43, 98, 55, 882, 97, 32, 2388, 7, 223, 122, 22, 38, 8, 5, 3536, 15, 11, 248, 38, 59, 54, 138, 9, 163, 56, 26, 763, 26, 15, 4, 9, 17, 117, 138, 454, 12, 18, 6, 66, 478, 27, 56, 307, 4, 45, 243, 84, 1284, 9, 1252, 6, 11, 222, 38, 81, 14, 5, 488, 22, 38, 6, 66, 222, 34, 131, 4, 836, 424, 23, 1444, 1169, 22, 1308, 19423, 81, 34, 312, 4496, 11, 222, 38, 81, 17, 1372, 6, 11, 248, 38, 11, 14, 27, 1444, 22, 38, 29, 50, 6, 836, 226, 1444, 6954, 465, 17, 4, 2650, 5, 326, 530, 484, 16, 836, 527, 953, 1308, 454, 171, 8, 32, 3677, 4496, 836, 424, 2483, 465, 11, 4, 836, 1308, 4, 101, 27, 23, 155, 23, 657, 23, 48, 70, 4496, 836, 247, 4, 813, 2483, 465, 17, 4, 836, 487, 59, 361, 70, 16, 64, 487, 1308, 70, 4, 64, 379, 70, 2411, 169, 10, 307, 4, 17, 54, 27, 202, 7, 223, 70, 127, 34, 6, 836, 11, 138, 70, 4, 1308, 4496, 465, 11, 16, 836, 81, 175, 11, 100, 70, 4496, 66, 334, 84, 1696, 97, 34, 29, 43, 6, 836, 128, 100, 141, 860, 142, 134, 2483, 465, 17, 16, 836, 23, 3397, 822, 11211, 39, 134, 4, 5808, 4, 6028, 11211, 16, 23, 265, 61, 130, 344, 4, 2479, 344, 4, 9, 575, 377, 219, 2411, 836, 1963, 4, 1308, 6954, 465, 11, 4, 836, 2704, 23324, 27, 81, 2704, 0, 16, 11, 136, 45, 58, 2037, 90, 214, 2411, 836, 247, 4, 813, 2483, 465, 17, 4, 836, 23, 28807, 34, 134, 4, 23, 28807, 61, 134, 2411, 836, 226, 4, 64, 4, 1308, 2483, 465, 11, 4, 836, 23, 175, 138, 127, 34, 16, 351, 34, 208, 8, 575, 42, 214, 4, 26, 11, 101, 115, 2411, 66, 243, 2086, 131, 29, 15, 307, 16, 9, 828, 8, 56, 7, 5, 9173, 33, 17, 584, 8, 3123, 4, 17, 2341, 18, 76, 2628, 4, 9, 2250, 18, 8, 34, 6, 836, 150, 125, 11, 100, 22, 43, 4496, 465, 11, 8, 38, 6, 836, 128, 199, 1148, 1308, 2483, 465, 17, 6, 836, 150, 125, 1148, 23, 24, 4496, 49, 11, 131, 6, 66, 6925, 84, 1696, 15115, 150, 23, 953, 1308, 171, 24, 79, 2153, 1148, 1308, 4, 64, 953, 1308, 171, 2411, 161, 17, 555, 55, 3849, 15, 11, 179, 673, 1078, 12, 19, 152, 6, 169, 10, 307, 4, 11, 55, 2125, 1069, 5, 2655, 1243, 12, 38, 8, 34, 4, 9, 10, 2371, 2464, 12, 38, 4, 15, 11, 248, 38, 86, 9, 493, 140, 4, 15, 11, 54, 144, 953, 38, 171, 47, 34, 75, 17, 14, 1994, 8, 1026, 22, 34, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding"
      ],
      "metadata": {
        "id": "kEzRHNoPiG79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# tokenized_data_en = [[46, 0], [0, 31, 0], [2293, 9371], [11], [904, 0]]\n",
        "# tokenized_data_fr = [[82, 157, 774], [0, 14, 0], [29730, 14265], [1033, 17335], [2150, 0]]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "tokenized_tensors_en = [torch.tensor(seq) for seq in tokenized_data_en]\n",
        "tokenized_tensors_fr = [torch.tensor(seq) for seq in tokenized_data_fr]\n",
        "\n",
        "# Pad sequences\n",
        "padded_en = pad_sequence(tokenized_tensors_en, batch_first=True, padding_value=1)\n",
        "padded_fr = pad_sequence(tokenized_tensors_fr, batch_first=True, padding_value=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "UxmqHeT4iI7q"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(padded_en[:5])\n",
        "print(len(padded_en[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYaU9BLxi8Vp",
        "outputId": "8af54cb9-3a50-452d-8b6d-3613d97279ef"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  46,    0,    1,  ...,    1,    1,    1],\n",
            "        [   0,   31,    0,  ...,    1,    1,    1],\n",
            "        [2293, 9371,    1,  ...,    1,    1,    1],\n",
            "        [  11,    1,    1,  ...,    1,    1,    1],\n",
            "        [ 904,    0,    1,  ...,    1,    1,    1]])\n",
            "471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trouble-shooting vocabulary issues"
      ],
      "metadata": {
        "id": "3d2CQ9CP093e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_sample = {\n",
        "    'id': ['0', '1', '2', '3', '4'],\n",
        "    'translation': [\n",
        "        {'en': 'The Wanderer', 'fr': 'Le grand Meaulnes'},\n",
        "        {'en': 'Alain-Fournier', 'fr': 'Alain-Fournier'},\n",
        "        {'en': 'First Part', 'fr': 'PREMIÈRE PARTIE'},\n",
        "        {'en': 'I', 'fr': 'CHAPITRE PREMIER'},\n",
        "        {'en': 'THE BOARDER', 'fr': 'LE PENSIONNAIRE'}\n",
        "    ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "cZC1erbXntSu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ID for [UNK]:\", tokenizer_en.token_to_id(\"[UNK]\"))\n",
        "print(\"ID for [PAD]:\", tokenizer_en.token_to_id(\"[PAD]\"))\n",
        "print(\"ID for [SOS]:\", tokenizer_en.token_to_id(\"[SOS]\"))\n",
        "print(\"ID for [EOS]:\", tokenizer_en.token_to_id(\"[EOS]\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpueQH7GhISn",
        "outputId": "9262043e-368a-44a3-ef7f-472d84237aa8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID for [UNK]: 0\n",
            "ID for [PAD]: 1\n",
            "ID for [SOS]: 2\n",
            "ID for [EOS]: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer_en.encode(\"whom I used to call\")\n",
        "print(encoded.ids)\n",
        "print(tokenizer_en.decode(encoded.ids))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHl7BLA7iDf3",
        "outputId": "b1096e61-d3e3-4314-fbb4-fc7939d20422"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[264, 11, 584, 8, 608]\n",
            "whom I used to call\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_en = tokenizer_en.get_vocab()\n",
        "print(\"Is 'Wanderer' in the vocabulary?\", 'Wanderer' in vocab_en)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oi25PBsJmJVO",
        "outputId": "d3638a94-3df4-4842-de5b-298a081961e4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is 'Wanderer' in the vocabulary? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = tokenizer_en.get_vocab()\n",
        "\n",
        "first_few = {k: vocab[k] for k in list(vocab)[:10]}\n",
        "print(first_few)\n",
        "print(len(vocab))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eijO-0iMifhA",
        "outputId": "2b8795c8-d884-4e57-db87-b30128a12a77"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prudemment': 28107, 'exasperate': 18383, 'occupés': 20843, 'rude': 4108, 'Chékina': 11446, 'Japanese': 5934, 'snored': 21237, 'Valley': 22064, 'Beauty': 29543, 'Malgré': 9705}\n",
            "30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = tokenizer_fr.get_vocab()\n",
        "\n",
        "first_few = {k: vocab[k] for k in list(vocab)[:10]}\n",
        "print(first_few)\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PqrorBQ1O01",
        "outputId": "2df37efa-538e-484b-a954-e14b0fba56b4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'vitre': 3120, 'vouée': 29051, 'considéré': 7364, 'idole': 16665, 'humiliante': 19522, 'jugeant': 16709, '_Il': 29925, 'affreusement': 9930, 'interroge': 18013, 'quais': 7881}\n",
            "30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wanderer_count = sum(1 for sentence in get_all_sentences(dataset[\"train\"], 'en') if 'Wanderer' in sentence)\n",
        "print(f\"'Wanderer' appears {wanderer_count} times.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGH0XlJWmNB-",
        "outputId": "3f5d9983-10d3-40d7-d45b-06fc4b559ae9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Wanderer' appears 2 times.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Glossary"
      ],
      "metadata": {
        "id": "HzOeFkilIaUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face offers two primary libraries for tokenization\n",
        "\n",
        "Hugging Face offers two primary libraries for tokenization, and it can be a bit confusing. Here's a breakdown:\n",
        "\n",
        "### 1. `tokenizers` Library:\n",
        "- **Purpose**: The `tokenizers` library is designed to be a standalone, fast, and efficient tokenization library. It's written in Rust with Python bindings to ensure performance, making it much faster than pure Python tokenization methods.\n",
        "- **Capabilities**: It supports a wide variety of tokenization methods, such as Byte-Pair Encoding (BPE), WordPiece, Unigram, and more.\n",
        "- **Flexibility**: This library provides a lot of flexibility to customize the tokenization process, such as adding special tokens, controlling tokenization at the character or word level, and building custom pre- and post-tokenization pipelines.\n",
        "- **Training**: The library allows you to train your tokenizers from scratch on custom datasets.\n",
        "\n",
        "### 2. `transformers` Library:\n",
        "- **Purpose**: The `transformers` library is primarily designed to provide pre-trained models and their associated tokenizers for various NLP tasks. While it does offer tokenization capabilities, it's deeply integrated with the models it provides.\n",
        "- **Capabilities**: It wraps around the tokenizers from the `tokenizers` library and provides easy-to-use methods that are tied to specific pre-trained models.\n",
        "- **Ease of Use**: If you're using a model from the `transformers` library, it's often easier to use the associated tokenizer from the same library because they're designed to work together seamlessly.\n",
        "- **Pre-trained Tokenizers**: The library provides pre-trained tokenizers for many popular models, ensuring that the tokenization process aligns perfectly with the pre-trained model you're using.\n",
        "\n",
        "### Why Both?\n",
        "You might wonder why Hugging Face provides two libraries that seemingly overlap in functionality. The reason is modularity and flexibility. The `tokenizers` library is designed for speed and versatility in tokenization without any assumptions about the downstream model. In contrast, the `transformers` library provides an end-to-end solution for using pre-trained models, including tokenization that's tailored for each model.\n",
        "\n",
        "### When to Use Which?\n",
        "- If you're building a custom tokenization pipeline or need high-speed tokenization without necessarily using a pre-trained model from the `transformers` library, you might prefer the `tokenizers` library.\n",
        "- If you're using a pre-trained model from the `transformers` library (like BERT, GPT-2, T5, etc.), it's generally easier and more straightforward to use the associated tokenizer from the `transformers` library.\n",
        "\n",
        "In essence, while there's overlap, the two libraries serve slightly different needs in the NLP ecosystem."
      ],
      "metadata": {
        "id": "eJ8iyOVBLXmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face's Transformers tokenizers\n",
        "\n",
        "Hugging Face's Transformers library provides a variety of tokenizers suitable for different models and use-cases. Here are some of the word-level tokenizers:\n",
        "\n",
        "1. **BertTokenizer**: Uses WordPiece tokenization. As seen in the example, it can break words into smaller subwords prefixed by `##`.\n",
        "\n",
        "2. **RobertaTokenizer**: Uses Byte-Pair Encoding (BPE) similar to GPT-2. It's another form of subword tokenization.\n",
        "\n",
        "3. **XLNetTokenizer**: Uses SentencePiece tokenization, which can also produce subwords.\n",
        "\n",
        "4. **OpenAIApiTokenizer**: Tokenizer for OpenAI models like GPT-3.\n",
        "\n",
        "5. **GPT2Tokenizer**: Uses Byte-Pair Encoding (BPE).\n",
        "\n",
        "6. **T5Tokenizer**: Tokenizer for the T5 models. It uses SentencePiece.\n",
        "\n",
        "7. **LongformerTokenizer**: Tokenizer for the Longformer model. It's similar to the BertTokenizer.\n",
        "\n",
        "8. **DistilBertTokenizer**: Tokenizer for the DistilBERT model, which is a distilled version of BERT. Uses WordPiece tokenization.\n",
        "\n",
        "9. **WhitespaceTokenizer**: Splits text on whitespaces. This is one of the simplest tokenization methods.\n",
        "\n",
        "10. **BasicTokenizer**: A very basic tokenizer that splits the text on whitespaces and punctuation.\n",
        "\n",
        "Out of these, the **WhitespaceTokenizer** and **BasicTokenizer** are the simplest. The **WhitespaceTokenizer** only splits on whitespace, making no other modifications, while the **BasicTokenizer** additionally splits on punctuation and can handle case and Unicode normalization.\n",
        "\n",
        "For most tasks, especially when starting, the **BasicTokenizer** provides a simple yet effective tokenization method. However, for advanced models like BERT and its variants, their respective tokenizers (like BertTokenizer) are recommended because they align with the pre-training data and techniques used for those models."
      ],
      "metadata": {
        "id": "wsWVOujyKawh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face's `tokenizers` library\n",
        "\n",
        "Hugging Face's `tokenizers` library  is a powerful and efficient tool for tokenizing text, which is a fundamental step in any NLP pipeline. Here's a high-level overview:\n",
        "\n",
        "### 1. **Purpose**:\n",
        "- The library is designed to provide an extremely fast and efficient tokenization, leveraging Rust's performance with Python bindings.\n",
        "\n",
        "### 2. **Tokenization Techniques**:\n",
        "- Supports a variety of tokenization methods:\n",
        "  - **Byte-Pair Encoding (BPE)**: A data compression algorithm that's been adapted for word segmentation in NLP tasks.\n",
        "  - **WordPiece**: A tokenization method often used with models like BERT.\n",
        "  - **Unigram**: A tokenization method that's particularly popular for models like SentencePiece.\n",
        "  - **SentencePiece**: While not directly a method, the library supports models trained with Google's SentencePiece.\n",
        "  \n",
        "### 3. **Key Features**:\n",
        "\n",
        "- **Custom Tokenization**: Beyond the standard methods, users can define custom tokenization processes.\n",
        "  \n",
        "- **Training from Scratch**: Users can train their tokenizers on custom datasets, allowing creation of domain-specific tokenizers.\n",
        "\n",
        "- **Pre-tokenizers & Post-processors**: These allow users to customize the tokenization process. For instance, a pre-tokenizer might split text into words or sentences before the main tokenization, and a post-processor might add special tokens (e.g., [CLS], [SEP]).\n",
        "\n",
        "- **Normalization**: The library supports various text normalization techniques, such as lowercasing, stripping accents, and more.\n",
        "\n",
        "- **Fast**: Being implemented in Rust, the library offers performance that's much faster than pure Python tokenization methods.\n",
        "\n",
        "### 4. **Decoding**:\n",
        "- Converts token IDs back to human-readable text, which is crucial for tasks like text generation.\n",
        "\n",
        "### 5. **Handles Special Tokens**:\n",
        "- Provides an easy way to deal with special tokens, like padding, start-of-sentence, and end-of-sentence.\n",
        "\n",
        "### 6. **Batching**:\n",
        "- Efficiently tokenizes batches of text, which is essential for processing large datasets or for real-time applications.\n",
        "\n",
        "### 7. **Files and Serialization**:\n",
        "- Tokenizers can be saved to and loaded from files, ensuring consistent tokenization across different stages and applications.\n",
        "\n",
        "### 8. **Alignment with Original Text**:\n",
        "- The library can keep track of alignments between tokens and their corresponding parts in the original text, which is useful for tasks like Named Entity Recognition (NER) where you need to map model predictions back to the original text.\n",
        "\n",
        "### 9. **Integration with `transformers` Library**:\n",
        "- Although the `tokenizers` library can be used standalone, it's also designed to work seamlessly with Hugging Face's `transformers` library. Many pre-trained models in `transformers` use tokenizers built with this library.\n",
        "\n",
        "### Conclusion:\n",
        "The `tokenizers` library provides a versatile and efficient solution for one of the foundational steps in NLP, ensuring that both researchers and practitioners can have fast and consistent tokenization across various tasks and models."
      ],
      "metadata": {
        "id": "YIiBUDiid8VU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \"Training\" the tokenizer\n",
        "\n",
        "\"Training\" the tokenizer refers to the process of learning the most optimal way to split text into smaller units (tokens) based on a given dataset. This concept can be a bit confusing because, in many contexts, \"training\" is typically associated with supervised learning models. However, for tokenizers, \"training\" is more about data-driven statistical analysis.\n",
        "\n",
        "Here's a deeper look:\n",
        "\n",
        "### 1. **Why Train a Tokenizer?**\n",
        "\n",
        "Tokenization is not always as straightforward as splitting text on spaces or punctuation. Different tasks or languages might require different tokenization strategies. By training a tokenizer, you're essentially letting it learn the most common patterns in your dataset, which can help in breaking down the text in the most meaningful and consistent way.\n",
        "\n",
        "### 2. **How Does It Work?**\n",
        "\n",
        "For methods like Byte-Pair Encoding (BPE) or WordPiece, the training process typically involves:\n",
        "\n",
        "- **Initialization**: Start by treating each word as a single token and each character as a token.\n",
        "  \n",
        "- **Iterative Merging**: Repeatedly merge the most common pair of consecutive tokens. For instance, if \"h\" and \"i\" appear next to each other more frequently than any other pair of characters or tokens, merge them to create the token \"hi\".\n",
        "\n",
        "- **Termination**: Stop when you've reached a predefined number of merges or when merges no longer improve the tokenization quality based on some criterion.\n",
        "\n",
        "### 3. **Vocabulary Building**:\n",
        "\n",
        "While training, the tokenizer also builds its vocabulary, which is a list of tokens that the tokenizer knows about. This vocabulary is then used to tokenize new unseen texts. Tokens in the unseen texts that are not in the vocabulary are further split until they match the known tokens or are represented as special tokens (like [UNK] for \"unknown\").\n",
        "\n",
        "### 4. **Special Tokens and Rules**:\n",
        "\n",
        "During training, the tokenizer can also learn or be provided with special rules or tokens. For instance, certain tokenization methods might decide to treat numbers or dates in a specific way.\n",
        "\n",
        "### 5. **Benefits**:\n",
        "\n",
        "- **Adaptability**: Training allows the tokenizer to adapt to the specifics of the language or domain in the dataset. For instance, a tokenizer trained on medical texts might tokenize medical jargon differently than one trained on general news articles.\n",
        "  \n",
        "- **Optimized Vocabulary**: By training on a large corpus, the tokenizer can build a vocabulary optimized for that corpus, leading to more efficient token representations.\n",
        "\n",
        "In summary, training a tokenizer is about letting it learn the best way to split texts from a specific dataset, ensuring that the resulting tokens are meaningful, consistent, and optimized for downstream tasks."
      ],
      "metadata": {
        "id": "RIpmQ-vVgGHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `tokenizer.tokenize()`\n",
        "\n",
        "The `tokenizer.tokenize()` method is part of Hugging Face's tokenization utilities. It's responsible for breaking down a given text into smaller pieces, typically words or subwords, depending on the tokenizer.\n",
        "\n",
        "In the context of the `BertTokenizer` that we've been using, it splits text into words or subwords that the BERT model was trained on. BERT uses WordPiece tokenization, which means it can split words into smaller pieces if those words are not in its vocabulary. This allows BERT to handle a wide variety of out-of-vocabulary words.\n",
        "\n",
        "Here's a simple example:\n",
        "\n",
        "```python\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenize a sample sentence\n",
        "sentence = \"ChatGPT is part of OpenAI's family of models.\"\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "\n",
        "print(tokens)\n",
        "```\n",
        "\n",
        "In the above example, the tokenizer will break down the sentence into individual words or subwords. Words like \"OpenAI's\" might get split into \"open\", \"ai\", and \"'s\" since \"OpenAI's\" might not be a single token in the BERT's vocabulary, but its constituent parts are.\n",
        "\n",
        "This approach helps the model generalize better to unseen or rare words in real-world data by representing them as a combination of seen subwords."
      ],
      "metadata": {
        "id": "ZRahcuOgJEz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the vocabulary using the tokenized sentences\n",
        "Now, we'll build the vocabulary using the tokenized sentences. We'll focus on words that appear more than once to filter out potential typos or very rare words. We'll also limit the vocabulary size for efficiency.\n",
        "\n",
        "### Rationale:\n",
        "\n",
        "- **Word-level Tokenization**: At this stage, we're breaking down sentences into individual words, which will serve as the basic units for our model.\n",
        "  \n",
        "- **Building Vocabulary**: The vocabulary represents the set of words our model will recognize. Words not in the vocabulary will be treated as unknown (`[UNK]`). By focusing on more common words (those appearing more than once) and setting a limit on vocabulary size, we aim to strike a balance between covering a broad range of words and computational efficiency.\n",
        "\n",
        "- **Special Tokens**: These tokens serve specific purposes in the tokenized sequences:\n",
        "  - `[PAD]`: Used for padding shorter sequences to a fixed length.\n",
        "  - `[START]` and `[END]`: Indicate the beginning and end of sequences.\n",
        "  - `[UNK]`: Represents any token that is not in the vocabulary.\n",
        "\n",
        "The steps outlined above should help in processing the data for use in the Transformer model. After tokenization and vocabulary creation, the next steps would involve converting words to their corresponding integer IDs from the vocabulary, padding sequences to a fixed length, and creating the necessary masks. But first, let's ensure the tokenization and vocabulary-building steps work correctly in your Colab environment."
      ],
      "metadata": {
        "id": "S2UfYk6ADuaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenize a sample sentence\n",
        "sentence = \"ChatGPT is part of OpenAI's family of models.\"\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMvLvYC2JNxS",
        "outputId": "a4599e4b-4d33-4234-8902-9bd91c5a201e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['chat', '##gp', '##t', 'is', 'part', 'of', 'open', '##ai', \"'\", 's', 'family', 'of', 'models', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Size of the vocabulary and computation\n",
        "\n",
        "The size of the vocabulary directly influences various computational aspects of a Transformer-based model, especially in the context of machine translation:\n",
        "\n",
        "1. **Embedding Layer**: Each word in the vocabulary has a corresponding embedding vector. A larger vocabulary means the embedding matrix (where each row is a word's embedding) will be larger. This increases the number of parameters and the memory required.\n",
        "\n",
        "2. **Output Layer**: In the case of machine translation, the output layer (or the projection layer) will have a size equivalent to the target vocabulary size. The logits for each word in the vocabulary are computed, and then a softmax is applied to get the probabilities. A larger vocabulary increases the computation required for this step.\n",
        "\n",
        "3. **Softmax Computation**: The softmax operation in the output layer, which normalizes logits into probabilities, is more computationally intensive with a larger vocabulary.\n",
        "\n",
        "4. **Training Time**: A larger vocabulary means more parameters to train (in the embedding and output layers), which can lead to longer training times.\n",
        "\n",
        "5. **Memory Footprint**: The larger embedding and output matrices require more memory, both in terms of model storage and during runtime (when matrices are loaded into memory/GPU).\n",
        "\n",
        "6. **Generalization**: A model with more parameters (due to a larger vocabulary) has a higher capacity. While this might allow it to fit the training data better, there's also a risk of overfitting, especially if the dataset isn't large enough.\n",
        "\n",
        "In summary, while the embedding and output layers are directly affected by vocabulary size, the repercussions are felt throughout the training and inference processes in terms of computational time, memory usage, and potential model generalization. However, it's also essential to strike a balance. A vocabulary that's too small can hinder the model's ability to generalize and perform well on unseen data, especially in tasks like translation where capturing nuances is crucial."
      ],
      "metadata": {
        "id": "pS56WPW8IcFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limiting the vocabulary size\n",
        "\n",
        "Limiting the vocabulary size to a certain number, such as 30,000, is a common practice in NLP, especially in machine translation tasks. This is done for several reasons:\n",
        "\n",
        "1. **Memory and Computational Efficiency**: Large vocabularies increase the size of embedding matrices, which directly impacts the memory requirements and computational cost. By limiting the vocabulary size, models become more manageable in terms of memory and speed.\n",
        "\n",
        "2. **Out-of-Vocabulary Handling**: No matter how large the vocabulary, there will always be words that are not included. Having a fixed-size vocabulary means there's a systematic way to handle out-of-vocabulary (OOV) words, often by using a special token like `[UNK]` (unknown).\n",
        "\n",
        "3. **Rare Words**: In any language, there are words that appear very infrequently. These rare words can be problematic for training because the model doesn't see them enough to learn their meaning reliably. By limiting the vocabulary size, many of these rare words are excluded, and the model focuses on more common and relevant words.\n",
        "\n",
        "4. **Regularization**: Limiting the vocabulary can act as a form of regularization, preventing the model from fitting too closely to the training data and potentially improving generalization to new, unseen data.\n",
        "\n",
        "5. **Consistency with Pretrained Models**: Many pretrained models, like those from BERT or GPT families, use a fixed vocabulary size (often around 30,000 to 50,000). When fine-tuning or adapting these models to specific tasks, it's beneficial to maintain consistency with their vocabulary size.\n",
        "\n",
        "6. **Subword Tokenization**: Modern tokenizers, like SentencePiece or the BPE (Byte-Pair Encoding) algorithm, split words into smaller units (subwords or even characters). This allows the model to handle OOV words by representing them as a sequence of known subwords. With subword tokenization, even a seemingly limited vocabulary can cover a vast majority of the language's words and phrases.\n",
        "\n",
        "Given the benefits, it's not surprising that the tokenizers you're examining have a vocabulary size of 30,000. However, the specific size is a hyperparameter and can be adjusted based on the needs of the specific task and the amount of available data."
      ],
      "metadata": {
        "id": "9X7bAMktATji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing the dataset.\n",
        "\n",
        "### Goal of Tokenizing the Dataset:\n",
        "\n",
        "The primary goal of tokenizing is to convert human-readable text (sentences) into a format that a machine learning model can understand, i.e., sequences of numbers. Each word or sub-word in the text gets mapped to a unique number (token ID) based on the vocabulary of the tokenizer.\n",
        "\n",
        "### Desired Output:\n",
        "\n",
        "For your dataset, after tokenization, we want to transform each English and French sentence into its corresponding sequence of token IDs.\n",
        "\n",
        "For instance, if the English sentence is \"The Wanderer\" and after tokenization it becomes `[46, 763]`, and the French sentence \"Le grand Meaulnes\" becomes `[35, 89, 123]`, our tokenized dataset should reflect these transformations.\n",
        "\n",
        "### Shape and Structure:\n",
        "\n",
        "Given your dataset structure, the tokenized dataset might look something like this:\n",
        "\n",
        "```python\n",
        "{\n",
        "    'en': [\n",
        "        [46, 763],\n",
        "        [24, 55],\n",
        "        ...\n",
        "    ],\n",
        "    'fr': [\n",
        "        [35, 89, 123],\n",
        "        [35, 55],\n",
        "        ...\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "Each list inside 'en' or 'fr' represents a tokenized sentence. So, the shape is essentially two lists of lists, where the outer list's length is the number of sentences in the dataset, and the inner list's length varies based on the sentence length.\n",
        "\n",
        "### Post-tokenization:\n",
        "\n",
        "Once the data is tokenized, most of the operations (like padding, batching, and passing data through the model) will be performed on the tokenized data. The raw text data won't be used for training the model. However, you will occasionally need the original or the detokenized data for tasks like evaluations, for instance, when calculating BLEU scores for translations, or when you want to convert model predictions back into human-readable format.\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. Use the trained tokenizers to convert each English and French sentence in the dataset to their corresponding token IDs.\n",
        "2. Store these tokenized sequences in a structured format (like the one shown above)."
      ],
      "metadata": {
        "id": "iR_LN5twezi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SOS and EOS tokens\n",
        "\n",
        "You'll typically add both SOS and EOS tokens to both the source and target sequences, but for slightly different reasons:\n",
        "\n",
        "1. **Source Sequence (English in your case)**:\n",
        "    - **SOS**: Helps the model know where the sequence starts, especially if you're batching multiple sequences together.\n",
        "    - **EOS**: Signals the end of the sequence. This is particularly useful when you're working with attention mechanisms, as it allows the model to know where the valid data for each sequence ends.\n",
        "\n",
        "2. **Target Sequence (French in your case)**:\n",
        "    - **SOS**: This is crucial for the decoding phase. During training, models like the Transformer are often trained with a technique called \"teacher forcing\", where the true previous token (rather than the predicted one) is fed as input for predicting the next token. The SOS token provides the starting point for this process.\n",
        "    - **EOS**: Signals the end of the target sequence. During the decoding/generation phase, the EOS token indicates to the model that it should stop generating further tokens.\n",
        "\n",
        "In summary, while you could technically get away with not using SOS or EOS for the source sequence (though it's still recommended to use them), they are pretty much essential for the target sequence due to the reasons stated above."
      ],
      "metadata": {
        "id": "st5R6hRITqia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torch.nn.utils.rnn.pad_sequence`\n",
        "\n",
        "`pad_sequence` is a utility function provided by PyTorch to pad a list of variable-length sequences with zeros (or any specified value) to make them all of the same length.\n",
        "\n",
        "#### Parameters:\n",
        "\n",
        "- **sequences**: A list of sequences, where each sequence is a 1D tensor of variable lengths.\n",
        "- **batch_first**: (Optional) If `True`, the returned tensor will have shape `(batch_size, max_sequence_length)`. If `False`, the returned tensor will have shape `(max_sequence_length, batch_size)`. Default is `False`.\n",
        "- **padding_value**: (Optional) The value used for padding. Default is `0`.\n",
        "\n",
        "#### Returns:\n",
        "\n",
        "- A 2D tensor where sequences are padded to equal length.\n",
        "\n",
        "#### Example:\n",
        "\n",
        "Suppose you have sequences of token IDs with different lengths:\n",
        "\n",
        "```python\n",
        "sequences = [\n",
        "    torch.tensor([1, 2, 3]),\n",
        "    torch.tensor([4, 5]),\n",
        "    torch.tensor([6])\n",
        "]\n",
        "```\n",
        "\n",
        "You can use `pad_sequence` to pad these sequences:\n",
        "\n",
        "```python\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "padded_sequences = pad_sequence(sequences, batch_first=True)\n",
        "```\n",
        "\n",
        "The `padded_sequences` tensor will look like:\n",
        "\n",
        "```\n",
        "tensor([[1, 2, 3],\n",
        "        [4, 5, 0],\n",
        "        [6, 0, 0]])\n",
        "```\n",
        "\n",
        "Here, `0` is used as the padding value, and the sequences are padded to the length of the longest sequence.\n",
        "\n",
        "#### When to use:\n",
        "\n",
        "`pad_sequence` is especially useful in natural language processing tasks where you're dealing with sequences of words/tokens of variable lengths, and you want to batch them together for processing with deep learning models. Padding is necessary because models typically require input data to have consistent dimensions.\n",
        "\n",
        "However, note that while padding allows sequences to be processed in batches, it might introduce computational inefficiencies (since the model would process padding tokens which don't carry meaningful information). To counteract this, masking is often used in conjunction to inform the model which tokens are real and which are padding."
      ],
      "metadata": {
        "id": "fwCSV7A-hiIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding\n",
        "\n",
        "1. **Goal of Padding**: When training neural networks (especially deep learning models), it's often necessary to process data in batches for computational efficiency. However, sequences (like sentences) can vary in length. Since deep learning models expect input data to have consistent dimensions, we pad shorter sequences to match the length of the longest sequence in a batch. This ensures that every sequence in the batch has the same length.\n",
        "\n",
        "2. **Output Format**: After padding, for every sequence in your dataset, you'll have a consistent list of integers of a given length (either a pre-defined fixed length or the length of the longest sequence in the batch). Sequences shorter than this length will have additional integers added (typically 0 or some other designated \"padding\" value) to bring them up to the required length.\n",
        "\n",
        "3. **Shape**: The shape will depend on the number of sequences you're processing in a batch and the length you've padded them to. For instance, if you're processing batches of 32 sequences and you've padded them all to a length of 20 tokens, then the shape of your input tensor for each batch would be \\([32, 20]\\).\n",
        "\n",
        "### How to go about padding:\n",
        "\n",
        "With PyTorch and HuggingFace, padding is quite straightforward.\n",
        "\n",
        "1. First, decide on a consistent length to which you want to pad your sequences. You can either:\n",
        "   - Use the length of the longest sequence in the entire dataset.\n",
        "   - Use the length of the longest sequence in each batch (this is dynamic padding and is more memory efficient).\n",
        "   - Define an arbitrary fixed length.\n",
        "\n",
        "2. Use PyTorch's `pad_sequence` function to pad sequences in your dataset to the desired length.\n",
        "\n",
        "Here's how you can do it:\n",
        "\n",
        "```python\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "# Assuming you have tokenized data like the one you've shown\n",
        "tokenized_data_en = [[46, 0], [0, 31, 0], [2293, 9371], [11], [904, 0]]\n",
        "tokenized_data_fr = [[82, 157, 774], [0, 14, 0], [29730, 14265], [1033, 17335], [2150, 0]]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "tokenized_tensors_en = [torch.tensor(seq) for seq in tokenized_data_en]\n",
        "tokenized_tensors_fr = [torch.tensor(seq) for seq in tokenized_data_fr]\n",
        "\n",
        "# Pad sequences\n",
        "padded_en = pad_sequence(tokenized_tensors_en, batch_first=True, padding_value=0)\n",
        "padded_fr = pad_sequence(tokenized_tensors_fr, batch_first=True, padding_value=0)\n",
        "\n",
        "print(padded_en)\n",
        "print(padded_fr)\n",
        "```\n",
        "\n",
        "3. When creating DataLoader batches, ensure that each batch has sequences of the same length, either by padding all sequences in the dataset to a fixed length or by dynamically padding each batch.\n"
      ],
      "metadata": {
        "id": "ELty5jzmj8bt"
      }
    }
  ]
}